---
title: "Accessing data from the Internet"
subtitle: '<br/>ðŸ—º<br/>Transport Data Science'
author: "Robin Lovelace"
date: 'University of Leeds `r # Sys.Date()()`<br/><img class="img-footer" alt="" src="https://comms.leeds.ac.uk/wp-content/themes/toolkit-wordpress-theme/img/logo.png">'
output:
  xaringan::moon_reader:
    # css: ["default", "its.css"]
    # chakra: libs/remark-latest.min.js
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
bibliography: ../tds.bib
---

background-image: url(https://images-a.jpimedia.uk/imagefetch/w_700,f_auto,ar_3:2,q_auto:low,c_fill/if_h_lte_200,c_mfit,h_201/https://www.yorkshireeveningpost.co.uk/webimage/1.9594040.1550081116!/image/image.jpg)
background-position: 50% 50%
class: center, bottom, inverse

---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'alphabetic', 
           style = "markdown",
           first.inits = FALSE,
           hyperlink = FALSE, 
           dashed = FALSE)

download.file("https://github.com/ITSLeeds/TDS/raw/mast../tds.bib", "references.bib")
my_bib = ReadBib("references.bib", check = FALSE)
```

---

```{r, echo=FALSE}
# 
# <!-- Todo: uncomment
# 
# ## This session in context
# 
#  - Introduction to transport data science
#  - **The structure of transport data**
#  - Software for practical data science
#  - Data cleaning and subsetting
#  - ## <font color="red">Accessing data from web sources</font> 
#  - Routing
#  - Data visualization
#  - Project work
#  - Machine learning
#  - Professional issues
#  
# ---
# 
# ## Objectives
# 
# From the course [catalogue](https://github.com/ITSLeeds/TDS/blob/master/catalogue.md):
# 
# -  ## <font color="red">Learn where to find large transport datasets and assess data quality</font> 
# 
# 
# ```{r}
# # Understand the structure of transport datasets: spatial, temporal and demographic
# # Understand how to obtain, clean and store transport related data
# # Gain proficiency in command-line tools for handling large transport datasets
# # Learn machine learning and data modelling techniques
# # Produce data visualizations, static and interactive
# # Learn where to find large transport datasets and assess data quality
# # Learn how to join together the components of transport data science into a cohesive project portfolio 
# ```
# 
# ---
# 
# ## Learning outcomes
# 
# -  ## <font color="red">Identify available datasets and access and clean them</font> 
# 
# 
# ```{r}
# # Identify available datasets and access and clean them
# # Combine datasets from multiple sources
# # Understand what machine learning is, which problems it is appropriate for compared with traditional statistical approaches, and how to implement machine learning techniques
# # Visualise and communicate the results of transport data science, and know about setting-up interactive web applications
# # Deciding when to use local computing power vs cloud services
# ```
# 
# ---
# 
# # This lecture will...
# 
# - Be primarily practical
# 
# --
# 
# - Provide an overview of data access options
# 
# --
# 
# - Show how R packages and web services provide access to some datasets
# 
# 
# -->

```

---

## Data access in context

--

- Data cleaning (or 'tidying' or 'wrangling') is part of a wider process 
`r Citep(my_bib, "grolemund_r_2016", .opts = list(cite.style = "authoryear"))`

```{r, echo=FALSE}
knitr::include_graphics("https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png")
```

--

- It's important to have an idea where you're heading with the analysis

--

- Often best to start with pen and paper

---

## Data access/cleaning vs modelling time

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Tapson&#39;s Rules of Machine Learning:<br>4. Time spent on data cleaning is an order of magnitude more productive than time spent on hyperparameter tuning.<br><br>(Extreme example: achieved a Top 10 result in Kaggle using linear regression, as the only team that cleaned 50/60Hz noise first.)</p>&mdash; Jonathan Tapson (@jontapson) <a href="https://twitter.com/jontapson/status/1103024752019402753?ref_src=twsrc%5Etfw">March 5, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Source: https://twitter.com/jontapson/status/1103024752019402753


---

background-image: url()
background-size: cover
class: center, middle

# A typology of data sources

---

## Information and data pyramids

Data science is climbing the DIKW pyramid

```{r, echo=FALSE}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/DIKW_Pyramid.svg/220px-DIKW_Pyramid.svg.png")
```


---

## A geographic availability pyramid

- Recommendations
- Build this here!

- City-specific datasets
  - Bristol cycle count data

- Hard-to-access national data

- Open international/national datasets
  - Open origin-destination data from UK Census

- Globally available, low-grade data (bottom)
  - OpenStreetMap, Elevation data

---

## An ease-of access pyramid

- Data provision packages
  - Use the pct package
  - stats19 package

- Pre-processed data
  - E.g. downloading data from website www.pct.bike

- Messy official data
  - Raw STATS19 data

---

## A geographic level of detail pyramid

- Agents
- Route networks
- Nodes
- Routes
- Desire lines
- Transport zones


---

## Observations

- Official sources are often smaller in sizes but higher in Quality

- Unofficial sources provide higher volumes but tend to be noisy, e.g.: https://onlinelibrary.wiley.com/doi/full/10.1111/gean.12081

- Another way to classify data is by quality: signal/noise ratios

- Globally available datasets would be at the bottom of this pyramid; local surveys at the top.

Source: https://geocompr.robinlovelace.net/read-write.html


- Which would be best to inform policy?

---

## Portals

- UK geoportal, providing geographic data at many levels: https://geoportal.statistics.gov.uk
- Other national geoportals exist, such as this: http://www.geoportal.org/
- A good source of cleaned origin destination data is the Region downloads tab in the Propensity to Cycle Tool - see the Region data tab for West Yorkshire here, for example: http://www.pct.bike/m/?r=west-yorkshire
- OpenStreetMap is an excellent source of geographic data with global coverage. You can download data on specific queries (e.g. highway=cycleway) from the overpass-turbo service: https://overpass-turbo.eu/ or with the **osmdata** package

---

## Online lists

For other datasets, search online! Good starting points in your research may be:

- The open data section in Geocomputation with R - https://geocompr.robinlovelace.net/read-write.html#retrieving-data
- Transport datasets mentioned here: https://data.world/datasets/transportation
- UK government transport data: https://ckan.publishing.service.gov.uk/publisher/department-for-transport

---

## Data packages

- The **openrouteservice** github package provides routing data
- The stats19 package can get road crash data for anywhere in Great Britain [@lovelace_stats19_2019] see here for info: https://itsleeds.github.io/stats19/
- The pct package provides access to data in the PCT: https://github.com/ITSLeeds/pct
- There are many other R packages to help access data

---

# Practical demo


See practical questions here:

[github.com/ITSLeeds/TDS/blob/master/practicals/oneday-practicals.md](https://github.com/ITSLeeds/TDS/blob/master/practicals/oneday-practicals.md#getting-transport-data)

-
That involves:
- Getting data from OSM: overpass turbo

- Data from stats19

- Data from the Census

- Bonus: getting data from Uber

---

# References

```{r, 'refs', results="asis", echo=FALSE}
PrintBibliography(my_bib)
# RefManageR::WriteBib(my_bib, "refs-geostat.bib")
```
