[
  {
    "objectID": "s3/demo-references.html",
    "href": "s3/demo-references.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "You can cite packages as follows:\n\ncitation(\"osmextract\")\n\nOpen up the .bib file, e.g. with\n\nfile.edit(\"s5/demo-references.bib\")\n\nI used the osmextract package (Gilardi and Lovelace, 2024).\n(Ferster et al., 2019)\n(Kaiser et al., 2024)\n\n\n\n\n\n\n\n1 References\n\nFerster, C., Fischer, J., Manaugh, K., Nelson, T., Winters, M., 2019. Using OpenStreetMap to inventory bicycle infrastructure: A comparison with open data from cities. International Journal of Sustainable Transportation 14, 64–73. https://doi.org/10.1080/15568318.2018.1519746\n\n\nGilardi, A., Lovelace, R., 2024. Osmextract: Download and import open street map data extracts.\n\n\nKaiser, S.K., Klein, N., Kaack, L.H., 2024. From counting stations to city-wide estimates: Data-driven bicycle volume extrapolation. arXiv. https://doi.org/10.48550/ARXIV.2406.18454"
  },
  {
    "objectID": "s3/slides.html#a-brief-history-of-geographic-vizualisation",
    "href": "s3/slides.html#a-brief-history-of-geographic-vizualisation",
    "title": "Visualising transport data",
    "section": "A brief history of geographic vizualisation",
    "text": "A brief history of geographic vizualisation\n\nHumboldt’s Naturgemälde (1807, Geography of Plants)"
  },
  {
    "objectID": "s3/slides.html#good-bad-ugly-wrong",
    "href": "s3/slides.html#good-bad-ugly-wrong",
    "title": "Visualising transport data",
    "section": "Good, bad, ugly, wrong",
    "text": "Good, bad, ugly, wrong\n\nAim must be good graphics, but first it’s important to avoid pitfalls\nSource: Free and open book Data Visualisation"
  },
  {
    "objectID": "s3/slides.html#what-is-data-visualisation",
    "href": "s3/slides.html#what-is-data-visualisation",
    "title": "Visualising transport data",
    "section": "What is data visualisation?",
    "text": "What is data visualisation?\n\nData visualization is part art and part science. The challenge is to get the art right without getting the science wrong and vice versa. A data visualization first and foremost has to accurately convey the data.\nIn my experience, scientists frequently (though not always!) know how to visualize data without being grossly misleading. However, they may not have a well developed sense of visual aesthetics, and they may inadvertantly make visual choices that detract from their desired message. Designers, on the other hand, may prepare visualizations that look beautiful but play fast and loose with the data.\n\nSource: Fundamentals of Data Vizualisation"
  },
  {
    "objectID": "s3/slides.html#viz-4-policy",
    "href": "s3/slides.html#viz-4-policy",
    "title": "Visualising transport data",
    "section": "Viz 4 policy",
    "text": "Viz 4 policy\n\n[visualisations] are also often the best way to present the findings of [transport] research in a way that is accessible. [visualisation] making is therefore a critical part of [transport] and its emphasis not only on describing, but also changing the world.\n\nSource: Geocomputation with R"
  },
  {
    "objectID": "s3/slides.html#illustration-of-policy-impact",
    "href": "s3/slides.html#illustration-of-policy-impact",
    "title": "Visualising transport data",
    "section": "Illustration of policy impact",
    "text": "Illustration of policy impact\n\n\nVisualisations can make findings become ‘real’"
  },
  {
    "objectID": "s3/slides.html#a-brief-history-of-geographic-data-viz-in-r",
    "href": "s3/slides.html#a-brief-history-of-geographic-data-viz-in-r",
    "title": "Visualising transport data",
    "section": "A brief history of geographic data viz in R",
    "text": "A brief history of geographic data viz in R\n“The core R engine was not designed specifically for the display and analysis of maps, and the limited interactive facilities it offers have drawbacks in this area” (Bivand, Pebesma, and G’omez-Rubio 2013).\nFive years later…\n“An example showing R’s flexibility and evolving geographic capabilities is leaflet (Cheng, Karambelkar, and Xie 2018), a package for making interactive maps that has been extended by the R community, as we’ll see in Chapter 9” (Lovelace, Nowosad, and Meunchow 2018)."
  },
  {
    "objectID": "s3/slides.html#base-r-graphics-sf",
    "href": "s3/slides.html#base-r-graphics-sf",
    "title": "Visualising transport data",
    "section": "Base R graphics: sf",
    "text": "Base R graphics: sf"
  },
  {
    "objectID": "s3/slides.html#base-r-graphics-sf-ii",
    "href": "s3/slides.html#base-r-graphics-sf-ii",
    "title": "Visualising transport data",
    "section": "Base R graphics: sf II",
    "text": "Base R graphics: sf II"
  },
  {
    "objectID": "s3/slides.html#sf-graphics-code",
    "href": "s3/slides.html#sf-graphics-code",
    "title": "Visualising transport data",
    "section": "sf graphics: code",
    "text": "sf graphics: code\n\n# facet plots by default\nplot(nz) \n# plot just geometry, ready for new layers:\nplot(st_geometry(nz), reset = FALSE)\n# addition of new layers\nplot(nz_height, add = TRUE)\n# transparency\nsf_cols = sf.colors(n = 2, alpha = 0.2)\nnz$col = factor(x = nz$Island, labels = sf_cols)\nplot(st_geometry(nz), col = as.character(nz$col))\n#  see ?plot.sf for more"
  },
  {
    "objectID": "s3/slides.html#observations",
    "href": "s3/slides.html#observations",
    "title": "Visualising transport data",
    "section": "Observations",
    "text": "Observations\n\nFacets by default: useful for seeing patterns.\nTransparency new, add = ... argument the same\nYou can go far with base R graphics (Murrell 2016)."
  },
  {
    "objectID": "s3/slides.html#tmap",
    "href": "s3/slides.html#tmap",
    "title": "Visualising transport data",
    "section": "tmap",
    "text": "tmap\n\nA diverse dedicated mapping R package\n\n\nlibrary(tmap)\ntmap_mode(\"plot\")\ntm_shape(nz) +\n  tm_polygons(\"Median_income\", palette = \"RdYlBu\")"
  },
  {
    "objectID": "s3/slides.html#why-tmap",
    "href": "s3/slides.html#why-tmap",
    "title": "Visualising transport data",
    "section": "Why tmap?",
    "text": "Why tmap?\n\nIt is powerful and flexible.\nConcise syntax, attractive maps with minimal code, familiar to ggplot2 users.\nUnique capability: same code -&gt; static + interactive maps with switch tmap_mode().\nWide range of spatial classes (including raster objects) supported.\nWell documented + developed — see tmap-nutshell and JSS paper (Tennekes 2018)."
  },
  {
    "objectID": "s3/slides.html#tmap-basics",
    "href": "s3/slides.html#tmap-basics",
    "title": "Visualising transport data",
    "section": "tmap basics",
    "text": "tmap basics\n# Add fill layer#|  to nz shape\ntm_shape(nz) + tm_fill() \n# Add border layer to nz shape\ntm_shape(nz) + tm_borders() \n# Add fill and border layers to nz shape\ntm_shape(nz) + tm_fill() + tm_borders()"
  },
  {
    "objectID": "s3/slides.html#animations",
    "href": "s3/slides.html#animations",
    "title": "Visualising transport data",
    "section": "Animations",
    "text": "Animations\n\nAre easy with tmap (section 9.3 of geocompr)"
  },
  {
    "objectID": "s3/slides.html#interactive-maps-with-mapview",
    "href": "s3/slides.html#interactive-maps-with-mapview",
    "title": "Visualising transport data",
    "section": "Interactive maps with mapview",
    "text": "Interactive maps with mapview"
  },
  {
    "objectID": "s3/slides.html#web-mapping-applications",
    "href": "s3/slides.html#web-mapping-applications",
    "title": "Visualising transport data",
    "section": "Web mapping applications",
    "text": "Web mapping applications\n\nLeaflet integrates with shiny via leaflet::leafletOutput(), enabling web mapping applications built on R\nThese can be set-up to scale nationally, as illustrated by pct.bike (Lovelace et al. 2017)."
  },
  {
    "objectID": "s3/slides.html#summary",
    "href": "s3/slides.html#summary",
    "title": "Visualising transport data",
    "section": "Summary",
    "text": "Summary\n\nVisualisation is an important skill in data science\nVisualisation is particularly valuable for evidence-based decision-making and policy\nOpen source command software like R provides powerful tools for data visualisation\nGeographic data visualisation is possible with R using packages such as sf and tmap\nThese visualisation skills will be useful in the lecture next week on project work"
  },
  {
    "objectID": "s3/slides.html#exercise-with-desire-line-data-in-stplanr",
    "href": "s3/slides.html#exercise-with-desire-line-data-in-stplanr",
    "title": "Visualising transport data",
    "section": "Exercise with desire line data in stplanr",
    "text": "Exercise with desire line data in stplanr\n\nCreate a map showing the number of people walking and cycling in the stplanr dataset flowlines_sf using: -base graphics (hint: use plot()) and -tmap (hint: use tm_shape(flowlines_sf) + ..., palette = \"viridis\" and other options shown in tmaptools::palette_explorer() give different colourschemes).\n\nName two advantages of each approach\nBonus: Other mapping packages could be used to show the same data?\n\n\nFor more information on plotting OD data, see the stplanr-od vignetted, e.g. with\nvignette(\"stplanr-od\")"
  },
  {
    "objectID": "s3/slides.html#result-base-graphics",
    "href": "s3/slides.html#result-base-graphics",
    "title": "Visualising transport data",
    "section": "Result: base graphics",
    "text": "Result: base graphics\n\nlibrary(stplanr)\nlwd = flowlines_sf$All / mean(flowlines_sf$All) * 3\nplot(flowlines_sf[\"Taxi\"], lwd = lwd)"
  },
  {
    "objectID": "s3/slides.html#bonus-exercise",
    "href": "s3/slides.html#bonus-exercise",
    "title": "Visualising transport data",
    "section": "Bonus exercise",
    "text": "Bonus exercise\n\nBased on the routes_fast_sf data in the stplanr package, identify roads where investment in cycling is likely to be effective.\n\nStarting point:\n\n\n[1] 42\n\n\n[1] 42"
  },
  {
    "objectID": "s3/slides.html#sf-results",
    "href": "s3/slides.html#sf-results",
    "title": "Visualising transport data",
    "section": "sf results",
    "text": "sf results"
  },
  {
    "objectID": "s3/slides.html#tmap-view-mode-results",
    "href": "s3/slides.html#tmap-view-mode-results",
    "title": "Visualising transport data",
    "section": "tmap ‘view mode’ results",
    "text": "tmap ‘view mode’ results\n See here for result"
  },
  {
    "objectID": "s3/slides.html#exercises-bonus-optional",
    "href": "s3/slides.html#exercises-bonus-optional",
    "title": "Visualising transport data",
    "section": "Exercises bonus (optional)",
    "text": "Exercises bonus (optional)\n\nUsing data in the pct github package, estimate cycling potential in a city of your choice in the UK, and show the results\nSee the pct_training vignette for further information\nWork on your portfolios, adding a new visualisation"
  },
  {
    "objectID": "s3/slides.html#references",
    "href": "s3/slides.html#references",
    "title": "Visualising transport data",
    "section": "References",
    "text": "References\n\n\n\n\nBivand, Roger S., Edzer Pebesma, and Virgilio G’omez-Rubio. 2013. Applied Spatial Data Analysis with R. 2nd ed. 2013 edition. New York: Springer.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2018. Leaflet: Create Interactive Web Maps with the JavaScript ’Leaflet’ Library. https://CRAN.R-project.org/package=leaflet.\n\n\nLovelace, Robin, Anna Goodman, Rachel Aldred, Nikolai Berkoff, Ali Abbas, and James Woodcock. 2017. “The Propensity to Cycle Tool: An Open Source Online System for Sustainable Transport Planning.” Journal of Transport and Land Use 10 (1). https://doi.org/10.5198/jtlu.2016.862.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Meunchow. 2018. Geocomputation with R. CRC Press. http://robinlovelace.net/geocompr.\n\n\nMurrell, Paul. 2016. R Graphics, Second Edition. CRC Press.\n\n\nTennekes, Martijn. 2018. “Tmap: Thematic Maps in R.” Journal of Statistical Software, Articles 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06."
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Reading List",
    "section": "",
    "text": "This reading list contains key resources for the Transport Data Science module, organized by topic.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#key-skills",
    "href": "reading.html#key-skills",
    "title": "Reading List",
    "section": "2.1 Key Skills",
    "text": "2.1 Key Skills\n\nQuarto documentation (Allaire et al., 2024)\n\n\nThe software used to create the Transport Data Science course materials and numerous websites, presentations, dashboards, and books, Quarto is a powerful tool for creating reproducible documents with code and data.\nSee the technical writing page of Quarto’s documentation for key information on how to add references, figure captions, and more.\n\nIntroduction to GitHub (Heis, 2025)\n\nA good starting point for learning how to use GitHub for version control and collaboration. \nSee also their introduction to Devcontainers at docs.github.com/en/codespaces/",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#python",
    "href": "reading.html#python",
    "title": "Reading List",
    "section": "2.2 Python",
    "text": "2.2 Python\n\nCourse Materials for: Geospatial Data Science (Szell, 2025)\n\nCourse materials covering various aspects of geospatial data science, including data analysis, visualization, and working with street networks using Python.\n\nModern Polars (Heavey, n.d.)\n\nA side-by-side comparison of the Polars and Pandas libraries. \n\nA course on Geographic Data Science (Arribas-Bel, 2019)\n\nFree and open source online book on using GeoPandas and other Python libraries for geographic data analysis.\n\nPython for Data Analysis (McKinney, 2022)\n\nDta wrangling with Pandas, NumPy, and Jupyter, written by the creator of the Pandas library.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#r",
    "href": "reading.html#r",
    "title": "Reading List",
    "section": "2.3 R",
    "text": "2.3 R\n\nAdvanced R\n\nA comprehensive guide to advanced programming in R, covering topics such as functional programming and object-oriented programming.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#miscellaneous",
    "href": "reading.html#miscellaneous",
    "title": "Reading List",
    "section": "5.1 Miscellaneous",
    "text": "5.1 Miscellaneous\n\nData Science for Transport: A Self-Study Guide with Computer Exercises (Fox, 2018)\n\nAn introduction to transport data science with hands-on examples, slightly out of date as of 2025.\n\nReproducible Road Safety Research with R (Lovelace, 2020)\n\nIntroductory guide for analyzing road safety data in R\n\nOpen source tools for geographic analysis in transport planning (Lovelace, 2021)\n\nReview of open source tools available for transport planning and analysis.\n\nPython for Data Science (Turrell et al., 2025)\n\nA modern guide to data science using Python based on R for Data Science, with practical examples and clear explanations.\n\nThe Geography of Transport Systems (Rodrigue et al., 2013)\n\nComprehensive textbook on transport geography and systems\n\nModelling Transport (Ortúzar S. and Willumsen, 2001)\n\nFoundational text on transport modeling methods\n\nBuilding Reproducible Analytical Pipelines with R (Rodrigues, 2023)\n\nA guide to the data engineering side of data science, with a focus on reproducibility and automation.\n\nPapers investigating the relationships between new contraflow interventions and traffic levels and collision rates in London (Tait et al., 2024, 2023)\n\nSee the full bibliography on Zotero for more resources, and feel free to suggest additions by opening an issue in the tds issue tracker.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The timetable below is a user-friently representation of the timetable for the module (see github for .csv and .ics versions). See timetable.leeds.ac.uk and click on “[2025/26] January to May” in the dropdown menu that says “Month from selected day” for the official timetable. If you spot any discrepancies, please let us know.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "d3/report-structure.html",
    "href": "d3/report-structure.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Your report should have a logical structure and clear headings which could include:\n\nIntroduction\n\nClear research question\nContext and motivation\nReference to relevant literature\n\nInput Data and Data Cleaning\n\nDescription of datasets\nData quality considerations\nProcessing steps\n\nExploratory Data Analysis\n\nInitial visualization\nKey patterns\nStatistical summaries\n\nAnalysis and Results\n\nDetailed analysis\nClear presentation\nSupporting visualizations\n\nDiscussion and conclusions\n\nResult, key findings, interpretation\nPolicy implications/recommendations\nStrengths and limitations\nFuture directions\n\nReferences\n\nProperly formatted citations\nMix of academic and technical/policy/other sources\nRecommendation: generate these with Quarto (see Quarto Citation Guide)"
  },
  {
    "objectID": "d2/assessment-brief.html",
    "href": "d2/assessment-brief.html",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "TRAN5340M - Transport Data Science\n\n\n\nFormative Coursework: Portfolio Plan and Reproducible Code\n\n\n\nPortfolio Plan and Reproducible Code Submission\n\n\n\nNon-assessed submission deadline: 27th February 2026, 13:59.\n\n\n\nFor guidance on the assessment, please refer to the Summative Assessment Brief.\n\n\n\nYou can submit up to 4 pages of content for this formative assessment."
  },
  {
    "objectID": "d2/assessment-brief.html#assessment-in-brief",
    "href": "d2/assessment-brief.html#assessment-in-brief",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "TRAN5340M - Transport Data Science\n\n\n\nFormative Coursework: Portfolio Plan and Reproducible Code\n\n\n\nPortfolio Plan and Reproducible Code Submission\n\n\n\nNon-assessed submission deadline: 27th February 2026, 13:59.\n\n\n\nFor guidance on the assessment, please refer to the Summative Assessment Brief.\n\n\n\nYou can submit up to 4 pages of content for this formative assessment."
  },
  {
    "objectID": "d2/index.html",
    "href": "d2/index.html",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "",
    "text": "This is a formative (non-assessed but required) submission that will help you develop your final coursework. The deadline is 27th February 2026, 13:59."
  },
  {
    "objectID": "d2/index.html#what-to-submit",
    "href": "d2/index.html#what-to-submit",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "What to Submit",
    "text": "What to Submit\nSubmit a .zip file containing two key items:\n\nA concise PDF document (recommended length: 2 pages, absolute maximum: 4 pages) outlining:\n\nYour chosen transport-related topic\nThe main dataset(s) you plan to use\nYour research question\nAt least 2 academic references (see Quarto Citation Guide for details)\nAny initial analysis or questions you have\n\nReproducible code as a .qmd file showing how you accessed and processed your data"
  },
  {
    "objectID": "d2/index.html#template-and-example-submission",
    "href": "d2/index.html#template-and-example-submission",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Template and example submission",
    "text": "Template and example submission\nSee the template.qmd file (and rendered result) for guidance on the structure of your submission. An example submission is available in the d2/example.qmd file (rendered here).\nSee an example .zip file with the files needed to reproduce this analysis at gitub.com/itsleeds/tds/releases/.\nSee the source code of these files, including the .bib files for creating references, in the course repository: github.com/itsleeds/tds/tree/main/d2."
  },
  {
    "objectID": "d2/index.html#key-requirements",
    "href": "d2/index.html#key-requirements",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Key Requirements",
    "text": "Key Requirements\n\nMaximum .zip file size: 30 MB\nSubmit via Turnitin\nAI tools can be used in an assistive role (must be acknowledged)\nUse the default quarto referencing style"
  },
  {
    "objectID": "d2/index.html#writing-tips",
    "href": "d2/index.html#writing-tips",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Writing tips",
    "text": "Writing tips\nSee documentation on figures, technical writing and the visual editor mode from quarto.org for help with creating figures and citations."
  },
  {
    "objectID": "d2/index.html#topics-and-datasets",
    "href": "d2/index.html#topics-and-datasets",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Topics and Datasets",
    "text": "Topics and Datasets\nSome suggested areas include:\n\nRoad safety analysis\nInfrastructure and travel behavior\nTraffic congestion patterns\nPublic transport accessibility\nActive travel infrastructure\nTransport equity studies\nOther transport-related topics are encouraged\n\nSpecific examples could include:\n\nWhat is the relationship between travel behaviour (e.g. as manifested in origin-destination data represented as desire lines, routes and route networks) and road traffic casualties in a transport region (e.g. London, West Midlands and other regions in the pct::pct_regions$region_name data)\nAnalysis of a large transport dataset, e.g. https://www.nature.com/articles/sdata201889\nInfrastructure and travel behaviour\n\nWhat are the relationships between specific types of infrastructure and travel, e.g. between fast roads and walking?\nHow do official sources of infrastructure data (e.g. the CID) compare with crowd-sourced datasets such as OpenStreetMap (which can be accessed with the new osmextract R package)\nUsing new data sources to support transport planning, e.g. using data from https://telraam.net/ or https://dataforgood.facebook.com/dfg/tools/high-resolution-population-density-maps\n\nChanging transport systems\n\nModelling change in transport systems, e.g. by comparing before/after data for different countries/cities, which countries had the hardest lockdowns and where have changes been longer term? - see here for open data: https://github.com/ActiveConclusion/COVID19_mobility\nHow have movement patterns changed during the Coronavirus pandemic and what impact is that likely to have long term (see here for some graphics on this)\n\nSoftware / web development\n\nCreating a package to make a particular data source more accessible, see https://github.com/ropensci/stats19 and https://github.com/elipousson/crashapi examples\nDevelopment of a data dashboard, e.g. using Quarto Dashboards\nDevelopment of a web app, e.g. using the shiny package\n\nRoad safety - how can we makes roads and transport systems in general safer?\n\nInfluence of Road Infrastructure:\n\nAssessing the role of well-designed pedestrian crossings, roundabouts, and traffic calming measures in preventing road accidents.\nInvestigating the correlation between road surface quality (e.g., potholes, uneven surfaces) and the frequency of accidents.\n\nInfluence of Traffic Management:\n\nAssessing the role of traffic lights and speed cameras in preventing road accidents.\nInvestigating the correlation between the frequency of accidents and the presence of traffic calming measures (e.g., speed bumps, chicanes, road narrowing, etc.).\n\nLegislation and Enforcement:\n\n\nAssessing the role of speed limits in preventing road accidents.\n\n\n\nTraffic congestion - how can we reduce congestion?\n\nData Collection and Analysis:\n\nUtilizing real-time traffic data from platforms like Waze and Google Maps to forecast congestion patterns.\nAnalyzing historical traffic data to identify recurring congestion patterns and anticipate future traffic bottlenecks.\n\nMachine Learning and Predictive Modeling:\n\nDesigning machine learning models that use past and current traffic data to predict future congestion levels."
  },
  {
    "objectID": "d2/index.html#support-and-feedback",
    "href": "d2/index.html#support-and-feedback",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Support and Feedback",
    "text": "Support and Feedback\n\nFeedback will be provided within 15 working days\n\nFor full details including assessment criteria, formatting guidelines, and academic integrity requirements, see the assessment brief."
  },
  {
    "objectID": "marking-criteria-portable.html",
    "href": "marking-criteria-portable.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Marks for the submitted report, are awarded in 4 categories, accounting for the following criteria:\n\n0.1 Data processing: 20%\n\nThe selection and effective use of input datasets that are large (e.g. covering multiple years), complex (e.g. containing multiple variables) and/or diverse (e.g. input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g. by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g. from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n0.2 Visualization and report: 20%\n\nCreation of figures that are readable and well-described (e.g. with captions and description)\nHigh quality, attractive or advanced techniques (e.g. multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g. mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g. with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n0.3 Code quality, efficiency and reproducibility: 20%\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n0.4 Understanding the data science process, including choice of topic and impact: 40%\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure."
  },
  {
    "objectID": "s4/index.html",
    "href": "s4/index.html",
    "title": "Session 4: Geographic and origin-destination data",
    "section": "",
    "text": "In this session, we will learn how to use geographic and origin-destination data. The contents of the session are as follows:\n\nWe will start with reviewing the homework from the previous session\nA short lecture on geographic and origin-destination data (see slides)\nPractical session working with various data, including analysing origin-destination trip flows in London Cycle Hire System.\nBonus: Geometry operations and spatial analysis\nHomework and next session"
  },
  {
    "objectID": "s4/index.html#pre-requisites",
    "href": "s4/index.html#pre-requisites",
    "title": "Session 4: Geographic and origin-destination data",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nYou need to have a number of packages installed and loaded. Install the packages by typing in the following commands into RStudio (you do not need to add the comments after the # symbol)\nIf you need to install any of these packages use:\n\nRPython\n\n\n\nif (!require(\"pak\")) install.packages(\"pak\")\npak::pkg_install(c(\"sf\", \"tidyverse\", \"remotes\", \"ggspatial\"))\n# GitHub pkgs\n# pak::pkg_install(\"Nowosad/spDataLarge\")\n\n\nlibrary(sf)        # vector data package\nlibrary(tidyverse) # tidyverse packages\nlibrary(ggspatial) # ggspatial package\nlibrary(spData)    # spatial data package\n\n\n\n\n# Install necessary packages (uncomment if not already installed)\n# !pip install geopandas pandas matplotlib seaborn\n\nimport geopandas as gpd       # vector data package\nimport pandas as pd           # data manipulation\nimport matplotlib.pyplot as plt  # plotting\nimport seaborn as sns            # advanced plotting\n# For spatial data, geopandas comes with sample datasets\n# Alternatively, we can use the naturalearth datasets\nimport geopandas.datasets\n\n\n\n\n\nCheck your packages are up-to-date with update.packages() in R (or equivalent in Python)\nCreate a project folder with an appropriate name for this session (e.g. practical3)\nCreate appropriate folders for code, data and anything else (e.g. images)\nCreate a script called learning-OD.R, e.g. with the following command:\n\nmkdir code\ncode code/learning-OD.R # for R\ncode code/learning-OD.py # for Python"
  },
  {
    "objectID": "s4/index.html#basic-sf-operations",
    "href": "s4/index.html#basic-sf-operations",
    "title": "Session 4: Geographic and origin-destination data",
    "section": "3.1 Basic sf operations",
    "text": "3.1 Basic sf operations\nWe will start with a simple map of the world. Load the world object from the spData package. Notice the use of :: to say that you want the world object from the spData package.\n\nRPython\n\n\n\nworld = spData::world\n\n\n\n\nworld = gpd.read_file(\n    'https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip'\n)\n\n\n\n\nUse some basic R functions to explore the world object. e.g. class(world), dim(world), head(world), summary(world). Also view the world object by clicking on it in the Environment panel.\nsf objects can be plotted with plot().\n\nRPython\n\n\n\nplot(world)\n\n\n\n\nprint(type(world))       # Equivalent to class(world)\nprint(world.shape)       # Equivalent to dim(world)\nprint(world.head())      # Equivalent to head(world)\nprint(world.describe())  # Equivalent to summary(world)\n\n# Plotting the world GeoDataFrame\nworld.plot(figsize=(12, 8))\nplt.title('World Map')\nplt.show()\n\n\n\n\nNote that this makes a map of each column in the data frame. Try some other plotting options\n\nRPython\n\n\n\nplot(world[3:6])\nplot(world[\"pop\"])\n\n\n\n\n# Since world is a GeoDataFrame, we can select columns by position\n# However, GeoPandas plots the geometry, so we need to specify columns\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nworld.plot(column='POP_EST', ax=axes[0])\nworld.plot(column='GDP_YEAR', ax=axes[1])\nworld.plot(column='CONTINENT', ax=axes[2])\nplt.show()"
  },
  {
    "objectID": "s4/index.html#basic-spatial-operations",
    "href": "s4/index.html#basic-spatial-operations",
    "title": "Session 4: Geographic and origin-destination data",
    "section": "3.2 Basic spatial operations",
    "text": "3.2 Basic spatial operations\nLoad the nz and nz_height datasets from the spData package.\n\nRPython\n\n\n\nnz = spData::nz\nnz_height = spData::nz_height\n\n\n\n\nnz = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz.gpkg\")\nnz_height = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz_height.gpkg\")\n\n\n\n\nWe can use tidyverse functions like filter and select on sf objects in the same way you did in Practical 1.\n\nRPython\n\n\n\ncanterbury = nz |&gt; filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\n\n\n\n\ncanterbury = nz[nz['Name'] == 'Canterbury']\n\n\n\n\nIn this case we filtered the nz object to only include places called Canterbury and then did and intersection to find objects in the nz_height object that are in Canterbury.\nThis syntax is not very clear. But is the equivalent to\n\nRPython\n\n\n\ncanterbury_height = nz_height[canterbury, , op = st_intersects]\n\n\n\n\ncanterbury_height = gpd.overlay(nz_height, canterbury, how='intersection')\n\n\n\n\nThere are many different types of relationships you can use with op. Try ?st_intersects() to see more. For example this would give all the places not in Canterbury\n\nRPython\n\n\n\nnz_height[canterbury, , op = st_disjoint]\n\n\n\n\ncanterbury_height = gpd.sjoin(nz_height, canterbury, op='intersects')\n\n\n\n\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue. The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string."
  },
  {
    "objectID": "s2/homework.html",
    "href": "s2/homework.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Hi all,\nFor anyone who wanted to recap on Session 1 or Session 2 content, see the recorded sessions + manuscripts here:\n\nSession 1: TDS Session 1_ Introduction to transport data science (RL, MM, ZW, YY)-20250130_100448-Meeting Recording.mp4\nSession 2: TDS Session 2_ Getting transport data (RL, YY)-20250206_100118-Meeting Recording.mp4\n\nAlso, please note the updated homework here (note the new item 4 and bonus 5, if you get stuck or hit any error messages just let me know): https://itsleeds.github.io/tds/s2/#homework\nYou will present the code / .qmd files you wrote as part of this homework to colleagues in the next session, so please come prepared, any visualisations of outputs and questions you would like to asks demonstrators especially welcome.\nGreat work everyone and thanks for engaging so well with the content today and looking forward to the session on origin-destination data next week.\nRobin"
  },
  {
    "objectID": "s2/slides.html#objectives",
    "href": "s2/slides.html#objectives",
    "title": "Accessing data from the Internet",
    "section": "Objectives",
    "text": "Objectives\n\nLearn where to find large transport datasets and assess data quality"
  },
  {
    "objectID": "s2/slides.html#learning-outcomes",
    "href": "s2/slides.html#learning-outcomes",
    "title": "Accessing data from the Internet",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nIdentify available datasets and access and clean them"
  },
  {
    "objectID": "s2/slides.html#this-lecture-will",
    "href": "s2/slides.html#this-lecture-will",
    "title": "Accessing data from the Internet",
    "section": "This lecture will…",
    "text": "This lecture will…\n\nBe primarily practical\nProvide an overview of data access options\nShow how R packages and web services provide access to some datasets"
  },
  {
    "objectID": "s2/slides.html#data-access-in-context",
    "href": "s2/slides.html#data-access-in-context",
    "title": "Accessing data from the Internet",
    "section": "Data access in context",
    "text": "Data access in context\n\nData cleaning (or ‘tidying’ or ‘wrangling’) is part of a wider process (Wickham, Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\nIt’s important to have an idea where you’re heading with the analysis\nOften best to start with pen and paper"
  },
  {
    "objectID": "s2/slides.html#data-accesscleaning-vs-modelling-time",
    "href": "s2/slides.html#data-accesscleaning-vs-modelling-time",
    "title": "Accessing data from the Internet",
    "section": "Data access/cleaning vs modelling time",
    "text": "Data access/cleaning vs modelling time\n\n\nTapson’s Rules of Machine Learning:4. Time spent on data cleaning is an order of magnitude more productive than time spent on hyperparameter tuning.(Extreme example: achieved a Top 10 result in Kaggle using linear regression, as the only team that cleaned 50/60Hz noise first.)\n\n— Jonathan Tapson ((jontapson?)) March 5, 2019\n\n\nSource: https://twitter.com/jontapson/status/1103024752019402753\nbackground-image: url() background-size: cover class: center, middle"
  },
  {
    "objectID": "s2/slides.html#information-and-data-pyramids",
    "href": "s2/slides.html#information-and-data-pyramids",
    "title": "Accessing data from the Internet",
    "section": "Information and data pyramids",
    "text": "Information and data pyramids\nData science is climbing the DIKW pyramid"
  },
  {
    "objectID": "s2/slides.html#a-geographic-availability-pyramid",
    "href": "s2/slides.html#a-geographic-availability-pyramid",
    "title": "Accessing data from the Internet",
    "section": "A geographic availability pyramid",
    "text": "A geographic availability pyramid\n\nRecommendations\nBuild this here!\nCity-specific datasets\n\nBristol cycle count data\n\nHard-to-access national data\nOpen international/national datasets\n\nOpen origin-destination data from UK Census\n\nGlobally available, low-grade data (bottom)\n\nOpenStreetMap, Elevation data"
  },
  {
    "objectID": "s2/slides.html#an-ease-of-access-pyramid",
    "href": "s2/slides.html#an-ease-of-access-pyramid",
    "title": "Accessing data from the Internet",
    "section": "An ease-of access pyramid",
    "text": "An ease-of access pyramid\n\nData provision packages\n\nUse the pct package\nstats19 package\n\nPre-processed data\n\nE.g. downloading data from website www.pct.bike\n\nMessy official data\n\nRaw STATS19 data"
  },
  {
    "objectID": "s2/slides.html#a-geographic-level-of-detail-pyramid",
    "href": "s2/slides.html#a-geographic-level-of-detail-pyramid",
    "title": "Accessing data from the Internet",
    "section": "A geographic level of detail pyramid",
    "text": "A geographic level of detail pyramid\n\nAgents\nRoute networks\nNodes\nRoutes\nDesire lines\nTransport zones"
  },
  {
    "objectID": "s2/slides.html#observations",
    "href": "s2/slides.html#observations",
    "title": "Accessing data from the Internet",
    "section": "Observations",
    "text": "Observations\n\nOfficial sources are often smaller in sizes but higher in Quality\nUnofficial sources provide higher volumes but tend to be noisy\nAnother way to classify data is by quality: signal/noise ratios\nGlobally available datasets would be at the bottom of this pyramid; local surveys at the top.\nWhich would be best to inform policy?"
  },
  {
    "objectID": "s2/slides.html#portals",
    "href": "s2/slides.html#portals",
    "title": "Accessing data from the Internet",
    "section": "Portals",
    "text": "Portals\n\nUK geoportal, providing geographic data at many levels\nOther national geoportals exist\nA good source of cleaned origin destination data is the Region downloads tab in the Propensity to Cycle Tool - see the Region data tab for West Yorkshire here, for example\nOpenStreetMap is an excellent source of geographic data with global coverage. You can download data on specific queries (e.g. highway=cycleway) from the overpass-turbo service or with the osmdata or osmextract packages"
  },
  {
    "objectID": "s2/slides.html#online-lists",
    "href": "s2/slides.html#online-lists",
    "title": "Accessing data from the Internet",
    "section": "Online lists",
    "text": "Online lists\nFor other datasets, search online! Good starting points in your research may be:\n\nThe open data section in Geocomputation with R (r.geocompx.org/read-write) \nTransport datasets mentioned in data.world \nUK government transport data: Department for Transport"
  },
  {
    "objectID": "s2/slides.html#data-packages",
    "href": "s2/slides.html#data-packages",
    "title": "Accessing data from the Internet",
    "section": "Data packages",
    "text": "Data packages\n\nThe openrouteservice github package provides routing data\nThe stats19 package can get road crash data for anywhere in Great Britain (Lovelace et al. 2019) see docs.ropensci.org/stats19 \nThe pct package provides access to data in the PCT project, including origin-destination data for the UK (Lovelace et al. 2017) see github.com/ITSLeeds/pct \nThere are many other R packages to help access data, including the spanishoddata package for Spanish origin-destination data"
  },
  {
    "objectID": "s2/slides.html#demo",
    "href": "s2/slides.html#demo",
    "title": "Accessing data from the Internet",
    "section": "Demo",
    "text": "Demo\nSee session activities at itsleeds.github.io/tds/s2/\n\nThat involves:\nGetting data from OSM: overpass turbo\nData from stats19\nData from the Census\nBonus: getting data from Cadence platform"
  },
  {
    "objectID": "s2/slides.html#references",
    "href": "s2/slides.html#references",
    "title": "Accessing data from the Internet",
    "section": "References",
    "text": "References\n\n\n\n\nLovelace, Robin, Anna Goodman, Rachel Aldred, Nikolai Berkoff, Ali Abbas, and James Woodcock. 2017. “The Propensity to Cycle Tool: An Open Source Online System for Sustainable Transport Planning.” Journal of Transport and Land Use 10 (1). https://doi.org/10.5198/jtlu.2016.862.\n\n\nLovelace, Robin, Malcolm Morgan, Layik Hama, and Mark Padgham. 2019. “Stats19: A Package for Working with Open Road Crash Data.” Journal of Open Source Software. https://doi.org/10/gkb498.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "s5/dodgr-install.html",
    "href": "s5/dodgr-install.html",
    "title": "1 Local Install of dodgr",
    "section": "",
    "text": "1 Local Install of dodgr\nMalcolm Morgan University of Leeds, 2020-02-11\nOnly if you cannot run the dodgr examples.\nThis will locally compile the latest version of dodgr on you computer.\n\nSave your work in progress and close RStudio\nGo to https://github.com/ATFutures/dodgr/ and click Clone or download\nChoose download zip\nUnzip the folder\nIn the unziped folder find and open dodgr.Rproj a new Rstudio session will open.\nRun this code.\n\n\nremove.packages(\"dodgr\")\nif(!\"devtools\" %in% installed.packages()[,1]){\n  install.packages(\"devtools\")\n}\ndevtools::install(\".\", export_all = TRUE, upgrade = \"never\")\nlibrary(dodgr)\n\nClose RStudio and open a new Rstudio session with your TDS work.\nDodgr should now work"
  },
  {
    "objectID": "slides/professions.html#reminder-on-coursework",
    "href": "slides/professions.html#reminder-on-coursework",
    "title": "Professional transport data science workflows and project work",
    "section": "Reminder on Coursework",
    "text": "Reminder on Coursework\n\nWhen’s the deadline?\n\n\nas.Date(\"2025-05-16\")\n\n\nRemember the marking criteria:"
  },
  {
    "objectID": "slides/professions.html#data-science-workflows",
    "href": "slides/professions.html#data-science-workflows",
    "title": "Professional transport data science workflows and project work",
    "section": "Data science workflows",
    "text": "Data science workflows\nSource: R for Data Science open source book"
  },
  {
    "objectID": "slides/professions.html#questions-to-consider-and-discuss-with-the-module-team",
    "href": "slides/professions.html#questions-to-consider-and-discuss-with-the-module-team",
    "title": "Professional transport data science workflows and project work",
    "section": "Questions to consider and discuss with the module team",
    "text": "Questions to consider and discuss with the module team\nAround 1 hour for this, in parallel with solo working on projects, providing time for the module to talk to each student\n\nWhat are the strongest aspects of your coursework idea so far? What are the weakest?\nWhat are the priorities over the next three weeks (break it down into a small number of parts)?\nWhat do you need to find more literature on?\nWhat do you need more data on (you should have all the data already)?\nWhat additional skills do you need (now is a good time to ask)?"
  },
  {
    "objectID": "slides/professions.html#working-on-your-projects",
    "href": "slides/professions.html#working-on-your-projects",
    "title": "Professional transport data science workflows and project work",
    "section": "Working on your projects",
    "text": "Working on your projects\n\nPlan ahead: what else do you need to do on your project?\nSchedule work: when will you find time to do it?\nUse Microsoft Calendar or similar: put it in the calendar.\nReproducibility: ensure your .Rmd files are reproducible\nAsk for help: what do you need help with?\nWork: get you head down and make use of this time!"
  },
  {
    "objectID": "slides/intro.html#who-transport-data-science-team",
    "href": "slides/intro.html#who-transport-data-science-team",
    "title": "Introduction to transport data science",
    "section": "Who: Transport Data Science team",
    "text": "Who: Transport Data Science team\nRobin Lovelace\n\nProfessor of Transport Data Science\nResearching transport futures and active travel planning\nData scientist, web application developer, author of Geocomputation with R\n\nYuanxuan Yang\n\nLecturer in Data Science of Transport\nNew and Emerging Forms of Data: Investigating novel data sources and their applications in urban mobility and transport planning."
  },
  {
    "objectID": "slides/intro.html#tds-team-ii",
    "href": "slides/intro.html#tds-team-ii",
    "title": "Introduction to transport data science",
    "section": "TDS Team II",
    "text": "TDS Team II\nMalcolm Morgan\n\nSenior researcher at ITS with expertise in routing + web\nDeveloper of the Propensity to Cycle Tool and PBCC\n\nZhao Wang\n\nCivil Engineer and Data Scientist with expertise in machine learning\n\nDemonstrators\n\nJuan Pablo Fonseca Zamora\n\nYou!"
  },
  {
    "objectID": "slides/intro.html#what-is-transport-data-science",
    "href": "slides/intro.html#what-is-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "What is transport data science?",
    "text": "What is transport data science?\n\nThe application of data science to transport datasets and problems\nRaising the question…\nWhat is data science?\nA discipline “that allows you to turn raw data into understanding, insight, and knowledge” (Grolemund, 2016)\n\nIn other words…\n\nStatistics that is actually useful!"
  },
  {
    "objectID": "slides/intro.html#why-take-transport-data-science",
    "href": "slides/intro.html#why-take-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "Why take Transport Data Science",
    "text": "Why take Transport Data Science\n\n\n\nNew skills (cutting edge R and/or Python packages)\nPotential for impacts\nAllows you to do new things with data\nIt might get you a job!"
  },
  {
    "objectID": "slides/intro.html#live-demo-npt.scot-web-app",
    "href": "slides/intro.html#live-demo-npt.scot-web-app",
    "title": "Introduction to transport data science",
    "section": "Live demo: npt.scot web app",
    "text": "Live demo: npt.scot web app"
  },
  {
    "objectID": "slides/intro.html#the-history-of-tds",
    "href": "slides/intro.html#the-history-of-tds",
    "title": "Introduction to transport data science",
    "section": "The history of TDS",
    "text": "The history of TDS\n\n2017: Transport Data Science created, led by Dr Charles Fox, Computer Scientist, author of Transport Data Science book (Fox, 2018)\nThe focus was on databases and Bayesian methods\n2019: I inherited the module, which was attended by ITS students\nSummer 2019: Python code published in the module ‘repo’:\n\ngithub.com/ITSLeeds"
  },
  {
    "objectID": "slides/intro.html#history-of-tds-ii",
    "href": "slides/intro.html#history-of-tds-ii",
    "title": "Introduction to transport data science",
    "section": "History of TDS II",
    "text": "History of TDS II\n\n\n\n2020: Available to Data Science students\n2021-2023: Updated module, focus on methods\n2024: Switch to combined lecture and practicals\n2025: Addition of Python and Quarto\n2026: Addition of AI\n2027+: Expansion 🚀"
  },
  {
    "objectID": "slides/intro.html#essential-reading",
    "href": "slides/intro.html#essential-reading",
    "title": "Introduction to transport data science",
    "section": "Essential reading",
    "text": "Essential reading\n\nChapter 13, Transportation of Geocomputation with R, a open book on geographic data in R (available free online) (Lovelace et al. 2019)\nReproducible Road Safety Research with R (RRSRR): https://itsleeds.github.io/rrsrr/"
  },
  {
    "objectID": "slides/intro.html#core-reading-materials",
    "href": "slides/intro.html#core-reading-materials",
    "title": "Introduction to transport data science",
    "section": "Core reading materials",
    "text": "Core reading materials\n\nR for Data Science, an introduction to data science with R (available free online)\nPython equivalent"
  },
  {
    "objectID": "slides/intro.html#optional",
    "href": "slides/intro.html#optional",
    "title": "Introduction to transport data science",
    "section": "Optional",
    "text": "Optional\nThere are many good resources on data science for transport applications. Do your own research and reading! The following are good:\n\nIf you’re interested in network analysis/Python, see this paper on analysing OSM data in Python (Boeing and Waddell, 2017) (available online)\nIf you’re interested in the range of transport modelling tools, see Lovelace (2021). \n\nFor more references, see the bibliography at github.com/ITSLeeds/TDS"
  },
  {
    "objectID": "slides/intro.html#objectives",
    "href": "slides/intro.html#objectives",
    "title": "Introduction to transport data science",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the structure of transport datasets\nUnderstand how to obtain, clean and store transport related data\nGain proficiency in command-line tools for handling large transport datasets\nProduce data visualizations, static and interactive\n Learn how to join together the components of transport data science into a cohesive project portfolio"
  },
  {
    "objectID": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "href": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "title": "Introduction to transport data science",
    "section": "Assessment (for those doing this as credit-bearing)",
    "text": "Assessment (for those doing this as credit-bearing)\n\nYou will build-up a portfolio of work\n100% coursework assessed, you will submit by\nWritten in code - will be graded for reproducibility\nCode chunks and figures are encouraged\nYou will submit a non-assessed 2 page pdf + qmd"
  },
  {
    "objectID": "slides/intro.html#schedule",
    "href": "slides/intro.html#schedule",
    "title": "Introduction to transport data science",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "slides/intro.html#feedback",
    "href": "slides/intro.html#feedback",
    "title": "Introduction to transport data science",
    "section": "Feedback",
    "text": "Feedback\n2024\n\nThe module is taught by two really well organised and enthusiastic professors, great module, the seminars, structured and unstructured learning was great and well thought out, all came together well\n\n\nI wish this module was 60 credits instead of 15 because i just want more of it.\n\n2025\n\nThe leaders were incredibly helpful during practical sessions and feedback was constructive. I liked having the opportunity to discuss what I had achieved so far with peers, as this led to the sharing of ideas and insight from other degree disciplines also."
  },
  {
    "objectID": "assessment-overview.html",
    "href": "assessment-overview.html",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "",
    "text": "This module’s assessments are designed to help you:\n\nDevelop practical data science skills for solving real-world transport problems\nApply programming and analysis techniques to transport datasets\nGenerate insights from transport data that can inform policy and planning decisions\nDemonstrate reproducible research practices in transport studies"
  },
  {
    "objectID": "assessment-overview.html#objectives",
    "href": "assessment-overview.html#objectives",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "",
    "text": "This module’s assessments are designed to help you:\n\nDevelop practical data science skills for solving real-world transport problems\nApply programming and analysis techniques to transport datasets\nGenerate insights from transport data that can inform policy and planning decisions\nDemonstrate reproducible research practices in transport studies"
  },
  {
    "objectID": "assessment-overview.html#module-assessment-structure",
    "href": "assessment-overview.html#module-assessment-structure",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "2 Module Assessment Structure",
    "text": "2 Module Assessment Structure\nThe module is assessed through two coursework assignments:\n\nFormative Assessment (CW1)\n\nRequired but non-assessed (0% of final mark)\nDue: 27th February 2026, 13:59\nProject plan and reproducible code demonstration\nLength: Up to 4 pages\n\nSummative Assessment (CW2)\n\nWorth 100% of module mark\nDue: 15th May 2026, 14:00\nComplete data science project report\nLength: 10 pages maximum + appendices"
  },
  {
    "objectID": "assessment-overview.html#file-naming-convention",
    "href": "assessment-overview.html#file-naming-convention",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "3 File Naming Convention",
    "text": "3 File Naming Convention\nYou must name your files using the following format:\nTRAN5340M_StudentIDNumber.file_type\nFor example: - TRAN5340M_201234567.zip"
  },
  {
    "objectID": "assessment-overview.html#submission-format-requirements",
    "href": "assessment-overview.html#submission-format-requirements",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "4 Submission Format Requirements",
    "text": "4 Submission Format Requirements\n\n4.1 Document Formatting\n\nInclude student ID in the title page\nDo not include your name (for anonymous marking)\nUse the default Quarto referencing style\n\n\n\n4.2 File Requirements\nFormative Assessment Package: - PDF report (max 4 pages) - Reproducible code (.qmd file) - Maximum .zip file size: 30 MB\nSummative Assessment Package: - PDF report (max 10 pages) - Maximum 3,000 words (excluding tables/code/references/captions) - Reproducible code (.Rmd or .qmd file) - Maximum .zip file size: 40 MB"
  },
  {
    "objectID": "assessment-overview.html#submission-process",
    "href": "assessment-overview.html#submission-process",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "5 Submission Process",
    "text": "5 Submission Process\n\nPrepare Your Submission\n\nEnsure correct file naming\nCheck formatting requirements\nTest code reproducibility\nVerify file sizes\n\nSubmission: Via Minerva (Blackboard Assignment)\n\nDeadline is 14:00 on submission day\nEach assignment has its own submission point\nKeep submission confirmation"
  },
  {
    "objectID": "assessment-overview.html#marking-criteria",
    "href": "assessment-overview.html#marking-criteria",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "6 Marking Criteria",
    "text": "6 Marking Criteria\n\n6.1 Summative Assessment (100% of module mark)\n\nData Processing (20%)\n\nDataset selection and usage\nData cleaning and preparation\nFeature engineering\nData transformation\n\nVisualization and Report Quality (20%)\n\nFigure design and clarity\nProfessional formatting\nEffective communication\nDocumentation quality\n\nCode Quality and Reproducibility (20%)\n\nCode efficiency and style\nDocumentation\nReproducibility\nTechnical implementation\n\nUnderstanding and Impact (40%)\n\nResearch question clarity\nMethodological approach\nCritical analysis\nPolicy implications\nLiterature engagement"
  },
  {
    "objectID": "assessment-overview.html#important-notes",
    "href": "assessment-overview.html#important-notes",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "7 Important Notes",
    "text": "7 Important Notes\n\n7.1 Topic Selection\n\nTopics should address real transport planning/policy challenges\nThe module team can provide guidance on topic selection\nGuidance on topics and datasets is provided in the module documents, including the formative assessment brief\n\n\n\n7.2 Use of AI Tools\nBoth assessments are categorized as GREEN for AI usage: - AI tools actively encouraged for coding and problem-solving - Usage should be documented in reflective sections - You must understand and be able to explain all submitted work - Critical evaluation of AI outputs is expected - AI will not be used to assess the submissions.\n\n\n7.3 Academic Integrity\n\nAll work must meet university standards\nProper referencing required\nPlagiarism checks applied through Turnitin"
  },
  {
    "objectID": "assessment-overview.html#support-available",
    "href": "assessment-overview.html#support-available",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "8 Support Available",
    "text": "8 Support Available\n\nAcademic Support\n\nModule team can be contacted via email\nWeekly sessions\n\nTechnical Support\n\nCode templates and examples provided in the course website\nR/RStudio guidance\nData access support\n\nWriting Support\n\nSkills@Library\nAcademic writing guidance\nReferencing support"
  },
  {
    "objectID": "assessment-overview.html#key-dates-2025-26",
    "href": "assessment-overview.html#key-dates-2025-26",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "9 Key Dates (2025-26)",
    "text": "9 Key Dates (2025-26)\n\n27 February 2026, 13:59: Formative assessment (CW1)\n15 May 2026, 14:00: Summative assessment (CW2)\nFeedback provided within 15 working days"
  },
  {
    "objectID": "assessment-overview.html#assessment-checklist",
    "href": "assessment-overview.html#assessment-checklist",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "10 Assessment Checklist",
    "text": "10 Assessment Checklist\n\n10.1 For Both Submissions\n\nCorrect file naming format used\nWork meets length requirements\nCode is reproducible\nZipped file size is within limits (you may need to remove large datasets from a copy of the folder to ensure this)\nAI usage is acknowledged\nReferences section included\n\n\n\n10.2 Additional for Summative\n\nMeets marking criteria\nIncludes policy implications\nReport is cohesive\nProfessional presentation"
  },
  {
    "objectID": "precourse.html",
    "href": "precourse.html",
    "title": "Pre-course self-assessment questionnaire: Transport Data Science",
    "section": "",
    "text": "This self-assessment questionnaire is designed to help you evaluate your readiness for the Transport Data Science (TRAN5340M) module at the University of Leeds. The module focuses on applying data science techniques to solve real-world transport problems using open source software, with a strong emphasis on reproducible workflows.\nPlease check one box for each question unless the question explicitly states to “Tick all that apply”.",
    "crumbs": [
      "Pre-Course Self-Assessment"
    ]
  },
  {
    "objectID": "precourse.html#welcome",
    "href": "precourse.html#welcome",
    "title": "Pre-course self-assessment questionnaire: Transport Data Science",
    "section": "",
    "text": "This self-assessment questionnaire is designed to help you evaluate your readiness for the Transport Data Science (TRAN5340M) module at the University of Leeds. The module focuses on applying data science techniques to solve real-world transport problems using open source software, with a strong emphasis on reproducible workflows.\nPlease check one box for each question unless the question explicitly states to “Tick all that apply”.",
    "crumbs": [
      "Pre-Course Self-Assessment"
    ]
  },
  {
    "objectID": "precourse.html#skills-assessment",
    "href": "precourse.html#skills-assessment",
    "title": "Pre-course self-assessment questionnaire: Transport Data Science",
    "section": "1 Skills assessment",
    "text": "1 Skills assessment\n\n\n1.1 How comfortable are you with managing files and folders on your computer?\n\nVery comfortable (I use file managers like Explorer or Finder to organize deep folder structures and search for files)\nComfortable (I can perform operations like creating, moving, and renaming folders/files)\nSomewhat comfortable (I can do simple tasks but sometimes struggle to find where I saved things)\nNot very comfortable (I often need help finding files or organizing folders)\n\n\n\n\n1.2 How confident are you with installing software on your computer?\n\nVery confident (I can install and configure most software)\nConfident (I can install standard software)\nSomewhat confident (I can install basic software but need guidance for complex tools)\nNot confident (I need help with software installation)\n\n\n\n\n1.3 Have you used a command line interface (e.g., PowerShell, Command Prompt in Windows, Mac Terminal, Linux shell) before?\n\nYes, regularly (I use it for multiple tasks including file management and installing software)\nYes, occasionally (I’ve used it for simple commands, such as simple one-off calculations, navigating directories or listing files)\nNo, but keen to learn",
    "crumbs": [
      "Pre-Course Self-Assessment"
    ]
  },
  {
    "objectID": "precourse.html#data-science-skills",
    "href": "precourse.html#data-science-skills",
    "title": "Pre-course self-assessment questionnaire: Transport Data Science",
    "section": "2 Data science skills",
    "text": "2 Data science skills\n\n\n2.1 Data science experience and outputs\nBy data science tools, we mean editing code using programming languages such as R or Python to work with and modify datasets and to generate results.\nI have used data science to produce:\nTick all that apply\n[Skip the next few questions if you have only followed tutorials]\n\nProfessional or academic-grade outputs\nData visualization (e.g., map or chart)\nClean, processed dataset from raw data\nStatistical analysis or model (e.g., regression)\nInteractive dashboard or web application\nAutomated/reproducible report or technical document\nOnly followed step-by-step tutorials so far\n\n\n\n\n2.2 Which of the following problems have you solved independently (with or without AI help)?\nTick all that apply\n\nWorking Directories: Fixing “File not found” errors by managing relative/absolute paths\nData Types: Troubleshooting why a plot fails because a column is a ‘string’ vs ‘numeric’\nPackage Management: Resolving failed library installations or version conflicts\nDocumentation: Successfully using StackOverflow, reading GitHub issues, implementing instructions from AI tools such as ChatGPT or documentation to fix a specific error message\n\n\n\n\n2.3 Do you have experience with R or Python for data science?\n\nYes, extensive (I use them regularly for data analysis)\nYes, intermediate (I can write scripts and use common packages)\nYes, basic (I’ve completed an introduction)\nNo, but I’m keen to learn\n\n\n\n\n2.4 Which of the following packages (libraries) have you used?\nTick all that apply\n\ntidyverse or pandas (data manipulation)\nsf or geopandas (spatial data handling)\nOther (please specify): ___________________________\n\n\n\n\n2.5 Have you worked with Quarto, R Markdown or Jupyter notebooks?\n\nYes, extensively (I use them for reports and reproducible work)\nYes, occasionally (I’ve created simple documents)\nNo, but I’m familiar with the concept",
    "crumbs": [
      "Pre-Course Self-Assessment"
    ]
  },
  {
    "objectID": "precourse.html#data-science-and-transport",
    "href": "precourse.html#data-science-and-transport",
    "title": "Pre-course self-assessment questionnaire: Transport Data Science",
    "section": "3 Data science and transport",
    "text": "3 Data science and transport\n\n\n3.1 How would you rate your ability to load and clean datasets, perform basic data transformations, and calculate summary statistics?\n\nAdvanced (I can handle complex datasets and analysis)\nIntermediate (I can perform most common data analysis tasks)\nBasic (I understand the concepts but need practice)\nBeginner (I’m just starting to learn these skills)\n\n\n\n\n3.2 Have you worked with spatial/geographic data before?\n\nYes, extensively (I work with spatial data regularly)\nYes, occasionally (I’ve used GIS software or spatial packages)\nNo, but I’m interested in learning\n\n\n\n\n3.3 Are you familiar with any of these transport data sources?\nTick all that apply\n\nOpenStreetMap (OSM)\nUK road safety data (Stats19)\nCensus travel to work data\nOrigin-destination (OD) matrices\n\n\n\n\n3.4 Experience with version control and code sharing?\n\nYes, I have used Git for version control\nYes, I use cloud storage (e.g. Dropbox/OneDrive) to share code\nNo, but I’m keen to learn",
    "crumbs": [
      "Pre-Course Self-Assessment"
    ]
  },
  {
    "objectID": "precourse.html#next-steps",
    "href": "precourse.html#next-steps",
    "title": "Pre-course self-assessment questionnaire: Transport Data Science",
    "section": "4 Next steps",
    "text": "4 Next steps\nAfter completing the questionnaire, use the following guide to assess your readiness and prepare for the module.\n\n\n4.1 Key resources\n\n\n\nTopic\nResource\n\n\n\n\nR\nRStudio Primers / R for Data Science\n\n\nPython\nPython for Data Science / Pandas Guide\n\n\nSpatial\nGeocomputation with R / Geocomputation with Python\n\n\nTransport\nTransport chapter (R) / Geocomputation with Python\n\n\nGit\nGitHub’s Git Guide\n\n\nTerminal\nThe Linux Command Line\n\n\nAdvanced\nReproducible Road Safety / Advanced R\n\n\n\n\n\n4.2 Final preparation\n\n\nInstall required software (R/RStudio, Python, Git) before the first session. \nCheck you have access to the GitHub repository at https://github.com/itsleeds/tds and try downloading materials and/or asking a question in the discussions section.\nPrepare any questions you may have for the first session.\n\n\n\n4.3 Contact information\nIf you have questions about your readiness or need guidance on preparing for the module, please contact:\n\nModule leader: Robin Lovelace (R.Lovelace@leeds.ac.uk)\nTeaching team: See the module website for full details.\n\nBest of luck with your preparation! We look forward to seeing you in the module.",
    "crumbs": [
      "Pre-Course Self-Assessment"
    ]
  },
  {
    "objectID": "ROOM_BOOKING_REQUEST.html",
    "href": "ROOM_BOOKING_REQUEST.html",
    "title": "1 Room Booking Request for TRAN5340M01 - Transport Data Science",
    "section": "",
    "text": "Module Code: TRAN5340M01\nModule Name: Transport Data Science\nAcademic Year: 2025/26 (Semester 2)\nStaff: Dr Robin Lovelace, Dr Malcolm Morgan, Dr Yuanxuan Yang\n\n\n\n\n\n\nTime: Thursdays, 10:00-13:00\nMichael Sadler SR (LG.15) Multi-mode: 1. Thursday, 29 January 2026 - Week 14 - Session 1: Introduction to transport data science 2. Thursday, 5 February 2026 - Week 15 - Session 2: Getting transport data 3. Thursday, 12 February 2026 - Week 16 - Session 3: AI for transport planning 5. Thursday, 5 March 2026 - Week 19 - Session 5: Routing 6. Thursday, 26 March 2026 - Week 22 - Session 6: Joins, models and publishing your work\nInstitute for Transport Studies Room 1.11a-c: 4. Thursday, 19 February 2026 - Week 17 - Session 4: Origin-destination data\n\n\n\nInstitute for Transport Studies Room 1.11a-c:\n\nThursday, 19 February 2026 - Week 17 - Seminar 1: Juan Fonseca (Traffic estimation and transport data visualisation) - 14:00-17:00\nThursday, 19 March 2026 - Week 21 - Seminar 2: Will Deakin, Network Rail (Network simplification) - 10:00-11:00\n\n\n\n\n\n\nTotal bookings: 8 (6 sessions + 2 seminars)\nLocations:\n\nMichael Sadler SR (LG.15) Multi-mode: Sessions 1, 2, 3, 5, 6\nInstitute for Transport Studies Room 1.11a-c: Session 4, Seminar 1, Seminar 2\n\nDays: All Thursdays\nWeeks: 14, 15, 16, 17, 19, 21, 22\n\n\n\n\n\nSession 4 (Week 17) has both a regular session (10:00-13:00) and Seminar 1 (14:00-17:00)\nAll sessions require computer access for students\nMulti-mode room required to accommodate diverse teaching methods\n\n\nGenerated: 21 October 2025"
  },
  {
    "objectID": "ROOM_BOOKING_REQUEST.html#module-details",
    "href": "ROOM_BOOKING_REQUEST.html#module-details",
    "title": "1 Room Booking Request for TRAN5340M01 - Transport Data Science",
    "section": "",
    "text": "Module Code: TRAN5340M01\nModule Name: Transport Data Science\nAcademic Year: 2025/26 (Semester 2)\nStaff: Dr Robin Lovelace, Dr Malcolm Morgan, Dr Yuanxuan Yang"
  },
  {
    "objectID": "ROOM_BOOKING_REQUEST.html#room-bookings-required",
    "href": "ROOM_BOOKING_REQUEST.html#room-bookings-required",
    "title": "1 Room Booking Request for TRAN5340M01 - Transport Data Science",
    "section": "",
    "text": "Time: Thursdays, 10:00-13:00\nMichael Sadler SR (LG.15) Multi-mode: 1. Thursday, 29 January 2026 - Week 14 - Session 1: Introduction to transport data science 2. Thursday, 5 February 2026 - Week 15 - Session 2: Getting transport data 3. Thursday, 12 February 2026 - Week 16 - Session 3: AI for transport planning 5. Thursday, 5 March 2026 - Week 19 - Session 5: Routing 6. Thursday, 26 March 2026 - Week 22 - Session 6: Joins, models and publishing your work\nInstitute for Transport Studies Room 1.11a-c: 4. Thursday, 19 February 2026 - Week 17 - Session 4: Origin-destination data\n\n\n\nInstitute for Transport Studies Room 1.11a-c:\n\nThursday, 19 February 2026 - Week 17 - Seminar 1: Juan Fonseca (Traffic estimation and transport data visualisation) - 14:00-17:00\nThursday, 19 March 2026 - Week 21 - Seminar 2: Will Deakin, Network Rail (Network simplification) - 10:00-11:00"
  },
  {
    "objectID": "ROOM_BOOKING_REQUEST.html#summary",
    "href": "ROOM_BOOKING_REQUEST.html#summary",
    "title": "1 Room Booking Request for TRAN5340M01 - Transport Data Science",
    "section": "",
    "text": "Total bookings: 8 (6 sessions + 2 seminars)\nLocations:\n\nMichael Sadler SR (LG.15) Multi-mode: Sessions 1, 2, 3, 5, 6\nInstitute for Transport Studies Room 1.11a-c: Session 4, Seminar 1, Seminar 2\n\nDays: All Thursdays\nWeeks: 14, 15, 16, 17, 19, 21, 22"
  },
  {
    "objectID": "ROOM_BOOKING_REQUEST.html#notes",
    "href": "ROOM_BOOKING_REQUEST.html#notes",
    "title": "1 Room Booking Request for TRAN5340M01 - Transport Data Science",
    "section": "",
    "text": "Session 4 (Week 17) has both a regular session (10:00-13:00) and Seminar 1 (14:00-17:00)\nAll sessions require computer access for students\nMulti-mode room required to accommodate diverse teaching methods\n\n\nGenerated: 21 October 2025"
  },
  {
    "objectID": "sem2/index.html",
    "href": "sem2/index.html",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The slides and high-resolution versions of some images from the talk are available here\n\n\n\n\nThe railway is international, noting the location of the United Kingdom\n\n\n\n\nThe operation of railway infrastructure in Britain is organised into thirteen routes and six regions.\n\nThis diagram show four routes that make up the Eastern region."
  },
  {
    "objectID": "sem2/index.html#make-things-as-simple-as-possiblebut-no-simpler",
    "href": "sem2/index.html#make-things-as-simple-as-possiblebut-no-simpler",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The slides and high-resolution versions of some images from the talk are available here"
  },
  {
    "objectID": "sem2/index.html#the-global-railway",
    "href": "sem2/index.html#the-global-railway",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The railway is international, noting the location of the United Kingdom"
  },
  {
    "objectID": "sem2/index.html#the-british-railway",
    "href": "sem2/index.html#the-british-railway",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The operation of railway infrastructure in Britain is organised into thirteen routes and six regions.\n\nThis diagram show four routes that make up the Eastern region."
  },
  {
    "objectID": "sem2/index.html#logical-network-model",
    "href": "sem2/index.html#logical-network-model",
    "title": "Seminar 2 -",
    "section": "2.1 Logical Network Model",
    "text": "2.1 Logical Network Model\nThe timetable production process can be modeled as a series of processes and interactions between railway operators and infrastructure manager."
  },
  {
    "objectID": "sem2/index.html#logical-operations-model",
    "href": "sem2/index.html#logical-operations-model",
    "title": "Seminar 2 -",
    "section": "2.2 Logical Operations Model",
    "text": "2.2 Logical Operations Model\nThe operation production process can be modeled as series of processes and data exchange."
  },
  {
    "objectID": "sem2/index.html#green-house-gases-co2",
    "href": "sem2/index.html#green-house-gases-co2",
    "title": "Seminar 2 -",
    "section": "3.1 Green House Gases (CO2)",
    "text": "3.1 Green House Gases (CO2)\nUK produced 455 million tonnes CO2 equivalent (mtCO2e) in 2019\nTransport accounted for 122 mtCO2e (26.8%) of which\n\nHGV 19.5 mtCO2e (16.0%/4.2%)\nAll rail 1.7 mtCO2e (1.4%/0.04%)\nA freight train removes up to 76 HGVs from our roads"
  },
  {
    "objectID": "sem2/index.html#national-greenhouse-emissions-1990-2020",
    "href": "sem2/index.html#national-greenhouse-emissions-1990-2020",
    "title": "Seminar 2 -",
    "section": "3.2 National Greenhouse Emissions 1990-2020",
    "text": "3.2 National Greenhouse Emissions 1990-2020"
  },
  {
    "objectID": "sem2/index.html#transport-greenhouse-emissions-1990-2020",
    "href": "sem2/index.html#transport-greenhouse-emissions-1990-2020",
    "title": "Seminar 2 -",
    "section": "3.3 Transport Greenhouse Emissions 1990-2020",
    "text": "3.3 Transport Greenhouse Emissions 1990-2020\n\n(from DfT Transport Statistics (2021))"
  },
  {
    "objectID": "sem2/index.html#system-visualisation",
    "href": "sem2/index.html#system-visualisation",
    "title": "Seminar 2 -",
    "section": "4.1 System Visualisation",
    "text": "4.1 System Visualisation\nAn Open-TrainTimes (OTT) schematic view of Leeds Stations\n\n\nOpenTrainTimes: here\nNational Rail Enquiries here\nRealtime Trains here"
  },
  {
    "objectID": "sem2/index.html#operation-railway",
    "href": "sem2/index.html#operation-railway",
    "title": "Seminar 2 -",
    "section": "4.2 Operation Railway",
    "text": "4.2 Operation Railway\nThe view of live operational railway systems."
  },
  {
    "objectID": "sem2/index.html#the-power-of-social-media",
    "href": "sem2/index.html#the-power-of-social-media",
    "title": "Seminar 2 -",
    "section": "6.1 The Power of Social Media",
    "text": "6.1 The Power of Social Media\nThe benefit of positive social media interaction."
  },
  {
    "objectID": "sem2/index.html#the-power-of-open-data",
    "href": "sem2/index.html#the-power-of-open-data",
    "title": "Seminar 2 -",
    "section": "6.2 The Power of Open Data",
    "text": "6.2 The Power of Open Data\nAll data used is on the basis that it under open or permissive license.\n\nThe base map of mainland Britain is derived from the WorldPop base maps under CC 4.0 by deed retrieved 2023-09-07.\nThe centre-line track-model is hosted by OpenRailData under the Open Government License(OGL) by Network Rail, retrieved 2023-07-11.\nThe Origin Destination Matrix data, for example ODM 2022-23, were published by the Office of Road and Rail on the Rail Development Group Rail Data Marketplace, under the OGL. Retrieved 2024-02-18.\nThe Station Attributes for All-Mainline Stations published by the Office of Road and Rail under the OGL. Retrieved 2024-02-18.\nThe Network Rail CORPUS dataset is an open data feed which is released under the OGL. Retrieved 2023-11-29 as a local copy.\nThe National Public Transport Access Network (NaPTAN) under the OGL and is updated each time the scripts are run.\nWhile this implementation now uses NaPTAN and CORPUS to validate and identify six closed stations, the Isle of Wight ferry-link continues to use OpenStreetMap data, licensed under CC-BY-SA 2.0 through the OverPassAPI Turbo service, and is updated each time the scripts are run."
  },
  {
    "objectID": "sem2/index.html#seconds-of-fame",
    "href": "sem2/index.html#seconds-of-fame",
    "title": "Seminar 2 -",
    "section": "6.3 15 Seconds of Fame",
    "text": "6.3 15 Seconds of Fame\nThe original 2018/19 visualisation received 500k X/Twitter views within a week, as well as an OpenInnovations blog post and a Bloomberg article Nine Maps Show How Britain Is on the Move.\nAll code and data is published on GitHub under my anisotropi4/kingfisher repository and updated with the ORR publication of the additional 2020-2024 financial year data."
  },
  {
    "objectID": "sem2/index.html#simple-visualisations",
    "href": "sem2/index.html#simple-visualisations",
    "title": "Seminar 2 -",
    "section": "6.4 Simple Visualisations",
    "text": "6.4 Simple Visualisations\nThe squares and coolest countries in Europe."
  },
  {
    "objectID": "sem2/index.html#shortest-path-census-flow",
    "href": "sem2/index.html#shortest-path-census-flow",
    "title": "Seminar 2 -",
    "section": "7.1 Shortest path census flow",
    "text": "7.1 Shortest path census flow"
  },
  {
    "objectID": "sem2/index.html#output-area-oa-census-2011-centroid",
    "href": "sem2/index.html#output-area-oa-census-2011-centroid",
    "title": "Seminar 2 -",
    "section": "7.2 Output Area (OA) Census 2011 centroid",
    "text": "7.2 Output Area (OA) Census 2011 centroid"
  },
  {
    "objectID": "sem2/index.html#full-oa-2011-delaunay-mesh",
    "href": "sem2/index.html#full-oa-2011-delaunay-mesh",
    "title": "Seminar 2 -",
    "section": "7.3 Full OA 2011 Delaunay mesh",
    "text": "7.3 Full OA 2011 Delaunay mesh"
  },
  {
    "objectID": "sem2/index.html#oa-2011-odm-mesh",
    "href": "sem2/index.html#oa-2011-odm-mesh",
    "title": "Seminar 2 -",
    "section": "7.4 OA 2011 ODM mesh",
    "text": "7.4 OA 2011 ODM mesh"
  },
  {
    "objectID": "sem2/index.html#network-simplification-and-parenx",
    "href": "sem2/index.html#network-simplification-and-parenx",
    "title": "Seminar 2 -",
    "section": "7.5 Network simplification and parenx",
    "text": "7.5 Network simplification and parenx\nThe network merge paper is here.\n\nThe parenx PyPi module.\n\n7.5.1 parenx cookbook\nThe parenx cookbook shows examples of how to use the parenx library."
  },
  {
    "objectID": "sem2/index.html#make-it-look-good.",
    "href": "sem2/index.html#make-it-look-good.",
    "title": "Seminar 2 -",
    "section": "8.1 Make it look good.",
    "text": "8.1 Make it look good.\n\nArt E-coli statue."
  },
  {
    "objectID": "sem2/index.html#the-railway",
    "href": "sem2/index.html#the-railway",
    "title": "Seminar 2 -",
    "section": "8.2 The Railway",
    "text": "8.2 The Railway\nBeeching 1963 and today."
  },
  {
    "objectID": "s1/index.html",
    "href": "s1/index.html",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "",
    "text": "Lecture: an introduction to Transport Data Science (30 min)\n\nSee the slides\n\nQ&A (15 min) \nBreak and networking (15 min) \nData science and a good research question (30 min)\nData science foundations (guided): Project set-up and using RStudio or VS Code as an integrated development environment (30 min)\nFocussed work (1 hr)"
  },
  {
    "objectID": "s1/index.html#agenda",
    "href": "s1/index.html#agenda",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "",
    "text": "Lecture: an introduction to Transport Data Science (30 min)\n\nSee the slides\n\nQ&A (15 min) \nBreak and networking (15 min) \nData science and a good research question (30 min)\nData science foundations (guided): Project set-up and using RStudio or VS Code as an integrated development environment (30 min)\nFocussed work (1 hr)"
  },
  {
    "objectID": "s1/index.html#how-to-come-up-with-a-good-research-question",
    "href": "s1/index.html#how-to-come-up-with-a-good-research-question",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "How to come up with a good research question",
    "text": "How to come up with a good research question\n\nThink about the data you have access to\nThink about the problems you want to solve\nThink about the methods you want to use and skills you want to learn\nThink about how the final report will look and hold-together\n\n\nHow much potential is there for cycling across the transport network?\n\n\n\nHow can travel to schools be made safer?\n\n\nHow can hospitals encourage visitors to get there safely?\n\n\nWhere’s the best place to build electric car charging points?\nSee openstreetmap.org or search for other open access datasets for more ideas"
  },
  {
    "objectID": "s1/index.html#data-object-manipulation-basics",
    "href": "s1/index.html#data-object-manipulation-basics",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "1.1 Data object manipulation basics",
    "text": "1.1 Data object manipulation basics\n\nUse the $ operator to print the vehicle_type column of crashes.\n\n\nIn R the $ symbol is used to refer to elemements of a list. So the answer is simply:\n\ncrashes$vehicle_type\n\n[1] \"car\"  \"bus\"  \"tank\"\n\n\n\n\nSubset the crashes with the [,] syntax\n\n\nTry out different combinations on the dataframe crashes to see what happens. For example, try:\n\ncrashes[1,]\n\n  casualty_type casualty_age vehicle_type\n1    pedestrian           20          car\n\ncrashes[,1]\n\n[1] \"pedestrian\" \"cyclist\"    \"cat\"       \n\ncrashes[1,1]\n\n[1] \"pedestrian\"\n\n\n\nSubset the object with the [[ syntax.\n\n\nThe [[ operator is used to extract elements from a list. Try:\n\ncrashes[[1]]\n\n[1] \"pedestrian\" \"cyclist\"    \"cat\"       \n\ncrashes[[2]]\n\n[1] 20 40 60\n\n\n\n\nBonus: what is the class() of the objects created by each of the previous exercises?\n\n\nExplore how many R classes you can find\n\n\nBonus (advanced): reproduce the above with Python using the pandas or polars package"
  },
  {
    "objectID": "s1/index.html#data-science-on-real-data",
    "href": "s1/index.html#data-science-on-real-data",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "1.2 Data science on real data",
    "text": "1.2 Data science on real data\nWork through the following example on road traffic data (recommended for most people) or the NTS data (for people more interested in travel survey data). You can do both if you have time.\n\n1.2.1 UK Road Safety Data\nTo get some larger datasets, try the following (from Chapter 8 of RSRR):\n\nRPython\n\n\n\nremotes::install_cran(\"stats19\")\nlibrary(stats19)\nac = get_stats19(year = 2020, type = \"collision\")\nca = get_stats19(year = 2020, type = \"cas\")\nve = get_stats19(year = 2020, type = \"veh\")\n# population hurt by road traffic collisions in 2020:\n(nrow(ca) / 67e6) * 100\n\n[1] 0.1725134\n\n\n\n\nChallenge: reproduce the above code in Python using the pystats19 package\n# Install the package, e.g. with pip\n!pip install pystats19\nimport pystats19\n# See the documentation at https://github.com/Mayazure/py-stats19\n\n\n\nLet’s go through these exercises together:\n\nSubset the casualty_age object using the inequality (&lt;) so that only elements less than 50 are returned.\nSubset the crashes data frame so that only tanks are returned using the == operator.\nBonus: assign the age of all tanks to 61.\n\n\nTry running the subsetting code on a larger dataset, e.g. the ac object created previously\n\n\nCoerce the vehicle_type column of crashes to the class character.\nCoerce the crashes object into a matrix. What happened to the values?\nBonus: What is the difference between the output of summary() on character and factor variables?\n\n\nWe’ll explore this together\n\n\n\n1.2.2 UK National Travel Survey (NTS) data\n\nNote: you will need to download the modified NTS 2022 data from your Minerva module page and place it in your working directory for this section to work.\n\n# Read CSV file\nNTS_data &lt;- read.csv(\"NTS2022_modifieddata.csv\")\n\n# Look at the column names\nnames(NTS_data)\n\n# Look at the data\nhead(NTS_data)\n\nYou should see something like this:\n&gt; names(NTS_data)\n [1] \"IndividualID\"            \"avg_trip_length\"\n [3] \"avg_trip_length_weekday\" \"avg_trip_length_weekend\"\n [5] \"total_distance\"          \"total_distance_weekday\"\n [7] \"total_distance_weekend\"  \"SD_triplength\"\n [9] \"sd_Total_Distance_wknd\"  \"sd_Total_Distance_wk\"\n \n &gt; head(NTS_data)\n  IndividualID avg_trip_length avg_trip_length_weekday avg_trip_length_weekend\n1   2023000001        4.080000                4.631579                2.333333\n2   2023000002        2.538462                2.400000                3.000000\n3   2023000003        5.916667                6.250000                5.250000\nVisualising datasets is important when dealing with large volumes of data, as visualisations help convey complex information in an easily interpretable format. Consider the histogram plots of average trip lengths over a week in the UK.\n\n# Note: This requires ggplot2 library to be loaded first\nlibrary(tidyverse) # Tidyverse contains ggplot2 and other useful packages\nggplot(NTS_data, aes(x = avg_trip_length)) +\n  geom_histogram(binwidth = 1, fill = \"darkgrey\") +\n  labs(\n    title = \"Avg. Trip Length in Whole Week\",\n    x = \"Trip Length (km)\",\n    y = \"Number of Individuals\"\n  ) +\n  theme_minimal() +\n  xlim(0, 50) \n\nData exploration or “exploratory data analysis” (EDA) involves examining datasets in depth to uncover underlying patterns or differences. The direction of this investigation is largely guided by the research question.\nConsider different histogram plots for weekdays and weekends. Can you identify any differences between them? (Clue: Check the number of individuals between 0-1 Km)\nThink: What could be plausible reasons for such difference?\n\nggplot(NTS_data, aes(x = avg_trip_length_weekday)) +\n  geom_histogram(binwidth = 1, fill = \"darkblue\") + \n  labs(\n    title = \"Avg. Trip Length on Weekdays\",\n    x = \"Trip Length (km)\",\n    y = \"Number of Individuals\"\n  ) +\n  theme_minimal() +\n  xlim(0, 50) \n\n\nggplot(NTS_data, aes(x = avg_trip_length_weekend)) +\n  geom_histogram(binwidth = 1, fill = \"darkred\") + \n  labs(\n    title = \"Avg. Trip Length on Weekends\",\n    x = \"Trip Length (km)\",\n    y = \"Number of Individuals\"\n  ) +\n  theme_minimal() +\n  xlim(0, 50) \n\nYou can more easily compare the two histograms when they are placed in the same plot, with transparency added to the bars:\n\ng_combined = ggplot() +\n  geom_histogram(data = NTS_data, aes(x = avg_trip_length_weekday), \n                 binwidth = 1, fill = \"darkblue\", alpha = 0.5) + \n  geom_histogram(data = NTS_data, aes(x = avg_trip_length_weekend), \n                 binwidth = 1, fill = \"darkred\", alpha = 0.5) + \n  labs(\n    title = \"Avg. Trip Length on Weekdays (blue) and Weekends (red)\",\n    x = \"Trip Length (km)\",\n    y = \"Number of Individuals\"\n  ) +\n  theme_minimal() +\n  xlim(0, 50)\n# Then 'print' the plot to show it:\ng_combined\n\nYou can save the plot with ggsave():\n\nggsave(\"avg_trip_length_weekday_weekend.png\", plot = g_combined, width = 8, height = 6)\n\nAnd (this is how you can show figures in Quarto), in a quarto document (.qmd file) that you will use to write and submit your coursework, you can include the saved figure like this (we will come onto this later in the module):\n![](avg_trip_length_weekday_weekend.png)\n\n\n\nAvg. Trip Length on Weekdays (blue) and Weekends (red)\n\n\nDon’t they largely look the same? Can you stop here and infer that the trip length distributions for weekdays and weekends are largely similar? You might, depending on the resources at your disposal, but from an academic point of view we need to think about other potential dimensions where they could be different.\nConsider different histogram plots for ‘Standard Deviation’ of trip lengths over weekdays and weekends. Can you identify any differences between them? (Clue: Again, check the number of individuals with SD 0-2 Km)\nThink: What could be plausible reasons for such difference?\n\nggplot(NTS_data, aes(x = sd_Total_Distance_wk)) +\n  geom_histogram(binwidth = 0.5, fill = \"darkblue\") + \n  labs(\n    title = \"SD of Trip Length on Weekdays\",\n    x = \"SD of trip length (km)\",\n    y = \"Number of Individuals\"\n  ) +\n  theme_minimal() +\n  xlim(0, 25) + ylim(0,1000) \n\n\nggplot(NTS_data, aes(x = sd_Total_Distance_wknd)) +\n  geom_histogram(binwidth = 0.5, fill = \"darkred\") + \n  labs(\n    title = \"SD of Trip Length on Weekends\",\n    x = \"SD of trip length (km)\",\n    y = \"Number of Individuals\"\n  ) +\n  theme_minimal() +\n  xlim(0, 25) + ylim(0,1000)"
  },
  {
    "objectID": "s1/index.html#bonus-data-science-and-transport",
    "href": "s1/index.html#bonus-data-science-and-transport",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "2.1 Bonus: data science and transport",
    "text": "2.1 Bonus: data science and transport\n\nWork through Chapter 13 of the book Geocomputation with R, taking care to ask questions about any aspects that you don’t understand (your homework will be to complete and make notes on the chapter, including reproducible code)."
  },
  {
    "objectID": "sem1/index.html",
    "href": "sem1/index.html",
    "title": "Seminar 1 - Mini-workshop",
    "section": "",
    "text": "The best way to learn is by exploring data and answering your own questions. Here are some datasets that can help you investigate questions like:"
  },
  {
    "objectID": "sem1/index.html#motorised-vehicles-counts-leeds",
    "href": "sem1/index.html#motorised-vehicles-counts-leeds",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.1 Motorised vehicles counts: Leeds",
    "text": "1.1 Motorised vehicles counts: Leeds\nMany cities/countries publish data from permanent traffic counters e.g. ANPR cameras, induction loops or low-cost sensors. We are going to use data from the sensors in Leeds (available in Data Mill North)\n\nleeds_car_location &lt;- read_csv(\n  \"https://datamillnorth.org/download/e6q0n/9bc51361-d98e-47d3-9963-aeeca3fa0afc/Camera%20Locations.csv\"\n  ) \n\nleeds_car_location_sf &lt;- leeds_car_location |&gt; \n  st_as_sf(coords = c(\"X\",\"Y\"),\n           crs = 27700)\n\n\nleeds_car_2019 &lt;- read_csv(\n  \"https://datamillnorth.org/download/e6q0n/9e62c1e5-8ba5-4369-9d81-a46c4e23b9fb/Data%202019.csv\"\n  )\n\nIf you are interested in open traffic count datasets see this\n\n1.1.0.1 code\n\nleeds_car_2019 |&gt; \n  group_by(Cosit) |&gt; \n  summarise(mean(Volume))\n\n\nmean_daily_volumes &lt;- leeds_car_2019 |&gt;\n  # converting cosit to numeric\n  mutate(Cosit = as.numeric(Cosit)) |&gt; \n  # extracting the date\n  mutate(time_date = dmy_hm(Sdate),\n         # extracts the day\n         date = date(time_date)) |&gt; \n  # calculating the total flows for each day\n  summarise(Volume = sum(Volume,rm.na = T),\n            .by = c(date,Cosit)) |&gt; \n  # Calculating the daily mean \n  summarise(daily_volume = mean(Volume,rm.na = T),\n            .by = Cosit) \n\n\ndaily_volumes &lt;- leeds_car_2019 |&gt; \n    # converting cosit to numeric\n  mutate(Cosit = as.numeric(Cosit)) |&gt;\n  # extracting the date\n  mutate(time_date = dmy_hm(Sdate),\n         # extracts the day\n         date = date(time_date)) |&gt; # calculating the total flows for each day\n  summarise(mean_volume = sum(Volume,rm.na = T),\n          .by = c(date,Cosit))\n\ndaily_volumes |&gt; \n  mutate(Cosit = as.numeric(Cosit)) |&gt; \n  filter(Cosit == 90201)|&gt; \n  ggplot(aes(x = date,y = mean_volume))+\n  geom_line()\n  \n\nmean_daily_volumes |&gt; \n  ggplot(aes(daily_volume))+\n  geom_histogram()\n\nleeds_car_location_sf |&gt;\n  left_join(mean_daily_volumes,by = c(\"Site ID\"=\"Cosit\")) |&gt; \n  tm_shape()+\n  tm_dots(\"daily_volume\",size = \"daily_volume\")"
  },
  {
    "objectID": "sem1/index.html#cycle-counts-for-west-yorkshire",
    "href": "sem1/index.html#cycle-counts-for-west-yorkshire",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.2 Cycle counts for West Yorkshire",
    "text": "1.2 Cycle counts for West Yorkshire\nSome cities would have some dedicated infrastructure to count the number of people using bikes at strategic points of the city. We are going to use some cycle counters from West Yorkshire that you can find here:\n\nleeds_bike_location &lt;- read_csv(\n  \"https://datamillnorth.org/download/e1dmk/a8c8a11e-1616-4915-a897-9ca5ab4e03b8/Cycle%20Counter%20Locations.csv\",skip = 1\n  ) \n\nleeds_bike_location_sf &lt;- leeds_bike_location |&gt;\n  drop_na(Latitude,Longitude) |&gt; \n  st_as_sf(coords = c(\"Longitude\",\"Latitude\"),\n           crs = 4326) |&gt; \n  st_transform(27700)\n\nThe data for 2019:\n\nleeds_bike_2019 &lt;- read_csv(\n  \"https://datamillnorth.org/download/e1dmk/f13f5d49-6128-4619-a3ff-e6e12f88a71f/Cycle%20Data%202019.csv\"\n  )\n\nOther interesting datasets for you to explore are Paris cycling counters or Scotland."
  },
  {
    "objectID": "sem1/index.html#pedestrian-counts-melbourne",
    "href": "sem1/index.html#pedestrian-counts-melbourne",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.3 Pedestrian Counts: Melbourne",
    "text": "1.3 Pedestrian Counts: Melbourne\nCities also monitor the number pedestrians in key locations. We can use data from the sensors in Melbourne accessible here:\n\nmelbourne_locations_sf &lt;- st_read(\"https://data.melbourne.vic.gov.au/api/explore/v2.1/catalog/datasets/pedestrian-counting-system-sensor-locations/exports/geojson?lang=en&timezone=Europe%2FLondon\")\n\nWe will extract\n\nmelbourne_dec2024 &lt;- read_csv(\"https://data.melbourne.vic.gov.au/api/explore/v2.1/catalog/datasets/pedestrian-counting-system-monthly-counts-per-hour/exports/csv?lang=en&refine=sensing_date%3A%222024%2F12%22&timezone=Australia%2FMelbourne&use_labels=true&delimiter=%2C\")"
  },
  {
    "objectID": "sem1/index.html#public-transport-tap-in-data-bogotá",
    "href": "sem1/index.html#public-transport-tap-in-data-bogotá",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.4 Public transport tap-in data: Bogotá",
    "text": "1.4 Public transport tap-in data: Bogotá\nPublic transport ridership data can be difficult to obtain. Fortunately, some cities which have systems managed by a public organisation make this data available for the public. Bogotá’s integrated transport system publishes the tap-in data for the BRT system (see this). We will use one of the daily reports.\n\ntm_stations_sf &lt;- st_read(\"Estaciones_Troncales_de_TRANSMILENIO.geojson\")\n\nMonthly boarding data can be manually obtained in the open data portal of TransMilenio here\n\nurl_tm &lt;- \"https://storage.googleapis.com/validaciones_tmsa/ValidacionTroncal/2024/consolidado_2024.zip\"\nu_bn &lt;- basename(url_tm)\n\n\nif(!file.exists(u_bn)){\n  download.file(url = url_tm,\n                destfile = u_bn,\n                mode = \"wb\")\n}\n\n\n\nurl_tm &lt;- \"https://storage.googleapis.com/validaciones_tmsa/ValidacionTroncal/2024/consolidado_2024.zip\"\n\n\ntm_brt_2024 &lt;- read_csv(unz(u_bn,\"troncal_2024.csv\"))\n\nTfL’s crowding data is also a great source of ridership data. See this.\n\n1.4.1 code\n\ndaily_tapins &lt;- tm_brt_2024 |&gt; \n  summarise(validaciones = sum(validaciones),\n            .by = c(Estacion_Parada,fecha)) |&gt; \n  summarise(validaciones = mean(validaciones),\n            .by = Estacion_Parada) |&gt; \n  mutate(numero_estacion = str_extract(Estacion_Parada,\"\\\\(\\\\d*\\\\)\") |&gt; \n           str_remove_all(\"(\\\\(|\\\\))\")) \n  \ntm_stations_sf |&gt; \n  left_join(daily_tapins,by = \"numero_estacion\") |&gt; \n  tm_shape()+\n  tm_dots(\"validaciones\",size = \"validaciones\")"
  },
  {
    "objectID": "sem1/index.html#network-data-from-osm",
    "href": "sem1/index.html#network-data-from-osm",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.5 Network data from OSM",
    "text": "1.5 Network data from OSM\nYou may be already familiar with getting and using OSM data. This an example of how to obtain the network that can be used for pedestrians.\n\nmy_coordinates &lt;- c(-76.78893552474851,18.01206727612776)\nsf_point &lt;- st_point(my_coordinates) |&gt; st_sfc(crs = 4326)\nsf_buffer &lt;- st_buffer(sf_point,dist = 15e3)\n\n\ntm_basemap(\"OpenStreetMap\")+\n  tm_shape(sf_buffer)+\n  tm_borders()\n\n\nmy_network &lt;- oe_get_network(sf_buffer, mode = \"walking\")\n\n\ntm_shape(my_network)+\n  tm_lines(\"highway\")\n\nNote: you can access a simplified network dataset from Ordnance Survey’s OpenRoads dataset."
  },
  {
    "objectID": "examples/test.html",
    "href": "examples/test.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "x = 1\nprint(x)\n\n1"
  },
  {
    "objectID": "s6/slides.html#objectives",
    "href": "s6/slides.html#objectives",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand different types of joins (spatial and key-based)\nLearn how to perform spatial joins with sf and dplyr\nApply aggregations to summarize data\nVisualize joined datasets\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(stats19)"
  },
  {
    "objectID": "s6/slides.html#what-is-a-join",
    "href": "s6/slides.html#what-is-a-join",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "What is a Join?",
    "text": "What is a Join?\n\nCombining datasets based on common attributes or spatial relationships\nSpatial joins: Link points (e.g., crashes) to polygons (e.g., LSOAs) using geometry\nKey-based joins: Match IDs across tables (e.g., LSOA codes)"
  },
  {
    "objectID": "s6/slides.html#spatial-join-example",
    "href": "s6/slides.html#spatial-join-example",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Spatial Join Example",
    "text": "Spatial Join Example\n\nLoad spatial datasets:\n\n\npath &lt;- \"https://github.com/itsleeds/tds/releases/download/2025/p6-lsoa_boundary_wy.geojson\"\nlsoa &lt;- read_sf(path)\ncrashes &lt;- stats19::get_stats19(\"2023\")\n\n\nPerform spatial join:\n\n\njoined &lt;- st_join(lsoa, crashes, join = st_intersects)\n\n\nAggregate results:\n\n\nagg_data &lt;- joined |&gt;\n  group_by(lsoa_id) |&gt;\n  summarize(total_crashes = n())"
  },
  {
    "objectID": "s6/slides.html#key-based-joins",
    "href": "s6/slides.html#key-based-joins",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Key-Based Joins",
    "text": "Key-Based Joins\nUse dplyr functions like left_join to combine data by identifiers:\n\n# Example: Join crash counts with population data\nfinal &lt;- left_join(agg_data, pop_data, by = \"lsoa_id\")"
  },
  {
    "objectID": "s6/slides.html#aggregations",
    "href": "s6/slides.html#aggregations",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Aggregations",
    "text": "Aggregations\n\nGroup data by spatial units (e.g., LSOA)\nCalculate metrics like crashes per capita:\n\n\nfinal &lt;- final |&gt;\n  mutate(crashes_per_capita = total_crashes / population)"
  },
  {
    "objectID": "s6/slides.html#visualization",
    "href": "s6/slides.html#visualization",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Visualization",
    "text": "Visualization\nCreate maps to visualize crash patterns using tmap:\n\ntmap_mode(\"plot\")\ntm_shape(final) +\n  tm_polygons(\"crashes_per_capita\")"
  },
  {
    "objectID": "s6/slides.html#references",
    "href": "s6/slides.html#references",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "References",
    "text": "References\n{references}"
  },
  {
    "objectID": "datahack.html",
    "href": "datahack.html",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "This event is designed to build data, coding and reproducible research skills for Institute for Transport Studies (ITS) staff and students. It is also specifically designed to support ITS MSc students with their dissertation projects by providing a space to ask questions about importing, processing and visualising data.\nIt will take place on Thursday 7th May 2026. It is open to staff and students at ITS. Contact the organisers (Robin Lovelace) if you are not based at the University of Leeds and would like to join in.\nSee here to sign-up.\n\n\n\nTo create a supportive space for participants to ask questions about working with datasets in general and using data science techniques for working with transport datasets in particular\nGet support importing datasets for MSc dissertations and other projects\nData wrangling with the tidyverse R package and other tools\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nShowcase the potential of open data (transparency, participation, research) and reproducible/open work-flows\n\n\n\n\n\nNone: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details\n\n\n\n\n\n13:00 - 13:30: Introduction to importing, processing and visualising data with RStudio\n\nAn indroduction to RStudio\nAn introduction to Quarto for reproducible reports\nAn example with origin-destination data in Leeds\n\n13:30 - 14:00: Importing your datasets\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: Solo working on your datasets, asking questions, and getting support\n15:15 - 15:45: Presentation of the results (optional for participants)\n\nAn opportunity for participants to share what they learned\n\n15:45 - 16:00: Networking and sharing ideas\n\n\n\n\nThe prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers."
  },
  {
    "objectID": "datahack.html#objectives",
    "href": "datahack.html#objectives",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "To create a supportive space for participants to ask questions about working with datasets in general and using data science techniques for working with transport datasets in particular\nGet support importing datasets for MSc dissertations and other projects\nData wrangling with the tidyverse R package and other tools\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nShowcase the potential of open data (transparency, participation, research) and reproducible/open work-flows"
  },
  {
    "objectID": "datahack.html#prerequisites",
    "href": "datahack.html#prerequisites",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "None: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details"
  },
  {
    "objectID": "datahack.html#schedule",
    "href": "datahack.html#schedule",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "13:00 - 13:30: Introduction to importing, processing and visualising data with RStudio\n\nAn indroduction to RStudio\nAn introduction to Quarto for reproducible reports\nAn example with origin-destination data in Leeds\n\n13:30 - 14:00: Importing your datasets\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: Solo working on your datasets, asking questions, and getting support\n15:15 - 15:45: Presentation of the results (optional for participants)\n\nAn opportunity for participants to share what they learned\n\n15:45 - 16:00: Networking and sharing ideas"
  },
  {
    "objectID": "datahack.html#prizes",
    "href": "datahack.html#prizes",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "The prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers."
  },
  {
    "objectID": "d1/index.html",
    "href": "d1/index.html",
    "title": "Welcome and set-up",
    "section": "",
    "text": "Dear Transport Data Science students,\nAs per your timetable, the first session is Thursday 30th January, from 10:00 to 13:00.\nLocation: Richard Hughes Cluster, in the “Cloth Workers Link Building”. If you’re wondering where that is, you’re not alone, I’m not 100% sure. So the first challenge of the module is to ensure that you get there on time, by 09:50, so you have time to get a seat in time for the 10:00 start."
  },
  {
    "objectID": "d1/index.html#homework-for-next-week-deadline-friday-31st-january-1400",
    "href": "d1/index.html#homework-for-next-week-deadline-friday-31st-january-1400",
    "title": "Welcome and set-up",
    "section": "1 Homework for next week (deadline: Friday 31st January, 14:00)",
    "text": "1 Homework for next week (deadline: Friday 31st January, 14:00)\n\nEnsure that you have the timetable stored safely in your calendar, so you do not miss important sessions or seminars.\nEnsure that you have the necessary software installed on your computer and that you have tested that you can use it for the datasets we will be using in the course, see https://itsleeds.github.io/tds/#software-requirements-and-installation for guidance on installing the software you need.\n\nAny issues you have with the software installation, please get in touch with me as soon as possible.\n\n\nTest that you have the necessary software installed by running the following code in R:\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_cran(\"tidyverse\")\nremotes::install_cran(\"osmextract\")\nlibrary(tidyverse)\nlibrary(osmextract)\nlibrary(sf)\n\n\nits = oe_get(\"ITS Leeds\", download_directory = tempdir())\n\n\nfigure = its |&gt;\n  ggplot() +\n  geom_sf(aes(colour = highway)) +\n  theme_void()\n# Save figure\nggsave(\"its.png\", figure, width = 6, height = 4)\n\n\nbrowseURL(\"its.png\")\n\n\nShow the map by executing the following code, which simply prints the map to the screen:\n\nfigure\n\nIf you see a map of the area around the Institute for Transport Studies, then you are ready for the first session. If you have any issues, please get in touch with me as soon as possible.\n\nTake a look at the reading list at https://itsleeds.github.io/tds/reading.html and have a read of the Transportation chapter of Geocomputation with R book (you will find the link to the book in the reading list).\nSign-up for a GitHub account if you do not already have one, and ensure that you have access to the TDS GitHub repository where you will find the course materials.\n\nPlease send me an email with you GitHub username so I can add you to the private repository that supports the course."
  },
  {
    "objectID": "s6/index.html",
    "href": "s6/index.html",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "",
    "text": "By “publishing” in this context, we primarily mean submitting your project for assessment. However, the skills you learn here apply to various forms of publication:\n\nReports (similar to the documents created in this module)\nAcademic papers for journals or conferences\nPresentations (which can be created with Quarto using format: revealjs)\nWeb applications or interactive dashboards\nBlog posts or technical documentation\n\nThe key principle is that good data science work should be reproducible, well-documented, and clearly communicated. This session will help you develop those skills.\nIn this session, we will explore techniques for joining, combining and aggregating datasets in R, particularly for spatial data. We’ll work with road crash data (STATS19), Lower Super Output Area (LSOA) boundaries, and census population data for West Yorkshire. Through this session, you’ll learn:\n\nHow to perform spatial joins between point and polygon data\nHow to join tables using common identifiers (key-based joins)\nHow to calculate and visualize derived metrics from joined datasets\nHow to create professional-looking reports with Quarto\n\nThe skills developed in this session are essential for transport data scientists who often need to combine data from various sources to gain comprehensive insights.\n\n\nFirst, let’s load the libraries we’ll need for this session:\n\n# Load required libraries\nlibrary(tidyverse)  # Data manipulation and visualisation\nlibrary(sf)         # Simple features for spatial data\nlibrary(stats19)    # Package for road crash data\nlibrary(tmap)       # Thematic mapping"
  },
  {
    "objectID": "s6/index.html#required-libraries",
    "href": "s6/index.html#required-libraries",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "",
    "text": "First, let’s load the libraries we’ll need for this session:\n\n# Load required libraries\nlibrary(tidyverse)  # Data manipulation and visualisation\nlibrary(sf)         # Simple features for spatial data\nlibrary(stats19)    # Package for road crash data\nlibrary(tmap)       # Thematic mapping"
  },
  {
    "objectID": "s6/index.html#downloading-stats19-crash-data",
    "href": "s6/index.html#downloading-stats19-crash-data",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "2.1 Downloading STATS19 crash data",
    "text": "2.1 Downloading STATS19 crash data\nWe’ll begin by downloading road crash data for four years (2019-2022) using the stats19 package:\n\n# Download STATS19 crash data for 2019-2023\n# We're downloading data for multiple years to have a robust dataset\ncrashes_2019 = get_stats19(year = 2019, type = \"accidents\", ask = FALSE)\ncrashes_2020 = get_stats19(year = 2020, type = \"accidents\", ask = FALSE)\ncrashes_2021 = get_stats19(year = 2021, type = \"accidents\", ask = FALSE)\ncrashes_2022 = get_stats19(year = 2022, type = \"accidents\", ask = FALSE)\ncrashes_2023 = get_stats19(year = 2023, type = \"accidents\", ask = FALSE)\n\nThe get_stats19() function downloads the crash data. The parameters used are:\n\nyear: Specifies which year’s data to download\ntype: Selects the type of data (accidents, vehicles, or casualties)\nask = FALSE: Automatically downloads without prompting for confirmation"
  },
  {
    "objectID": "s6/index.html#combining-multiple-years-of-data",
    "href": "s6/index.html#combining-multiple-years-of-data",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "2.2 Combining multiple years of data",
    "text": "2.2 Combining multiple years of data\nOnce we have the individual year datasets, we can combine them into a single dataframe using bind_rows():\n\n# Combine all years into one dataset\n# This creates a unified dataset for analysis across the full time period\ncrashes = bind_rows(crashes_2019, crashes_2020, crashes_2021, crashes_2022, crashes_2023)\n\n# Let's check the dimensions of our combined dataset\ndim(crashes)\n\nbind_rows() from dplyr appends the rows from each dataset, creating a unified dataset spanning all four years.\n\n\n\n\n\n\nNotebind_rows() vs rbind() in R (click to expand)\n\n\n\n\n\nBoth bind_rows() and rbind() combine data frames row-wise, but they differ in behaviour and flexibility.\n\nrbind() (Base R)\n\nComes from base R\nRequires exactly matching column names and types\nWill throw an error if the data frames don’t align perfectly\n\ndf1 = data.frame(a = 1:2, b = c(\"x\", \"y\"))\ndf2 = data.frame(a = 3:4, b = c(\"z\", \"w\"))\nrbind(df1, df2)  # ✅ Works\ndf3 = data.frame(a = 5:6, c = c(\"a\", \"b\"))\nrbind(df1, df3)  # ❌ Error: column names do not match\n\nbind_rows() (from dplyr)\n\nPart of the tidyverse\nMore flexible than rbind()\nAutomatically fills in missing columns with NAs\n\nlibrary(dplyr)\n\ndf1 = data.frame(a = 1:2, b = c(\"x\", \"y\"))\ndf3 = data.frame(a = 5:6, c = c(\"a\", \"b\"))\n\nbind_rows(df1, df3)  # ✅ Works, fills missing columns with NA\n\nSummary Comparison\n\n\n\nFeature\nrbind()\nbind_rows()\n\n\n\n\nFrom\nBase R\ndplyr (tidyverse)\n\n\nRequires same cols?\n✅ Yes\n❌ No\n\n\nFills missing cols\n❌ Error\n✅ With NA\n\n\nIdeal use case\nControlled data\nFlexible data wrangling\n\n\n\n\nTip\n\nUse rbind() if you’re sure the data frames have identical structure.\nUse bind_rows() for robust and flexible row-binding, especially in pipelines."
  },
  {
    "objectID": "s6/index.html#converting-crash-data-to-sf-object",
    "href": "s6/index.html#converting-crash-data-to-sf-object",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "2.3 Converting crash data to sf object",
    "text": "2.3 Converting crash data to sf object\nTo enable spatial operations, we need to convert our crash data into an sf (simple features) object:\n\n# Creating geographic crash data\n# This converts the data frame into a spatial object with point geometries\ncrashes_sf = format_sf(crashes)\n\nhead(crashes_sf)\n\nThe format_sf() function from the stats19 package converts the crash coordinates into a spatial object. The note you see indicates that rows with missing coordinate values are automatically removed, as spatial objects cannot have NA values for coordinates/geometry."
  },
  {
    "objectID": "s6/index.html#filtering-for-west-yorkshire",
    "href": "s6/index.html#filtering-for-west-yorkshire",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "2.4 Filtering for West Yorkshire",
    "text": "2.4 Filtering for West Yorkshire\nWe’ll now filter the crash data to include only incidents that occurred within the West Yorkshire police force area:\n\n# Filter crashes to only those that occurred in West Yorkshire\ncrashes_wy = crashes_sf |&gt; filter(police_force == \"West Yorkshire\")\n\n# Compare the number of rows before and after filtering\n# This shows how many crashes occurred in West Yorkshire vs. the whole dataset\nnrow(crashes_sf)\nnrow(crashes_wy)"
  },
  {
    "objectID": "s6/index.html#what-is-a-spatial-join",
    "href": "s6/index.html#what-is-a-spatial-join",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "3.1 What is a spatial join?",
    "text": "3.1 What is a spatial join?\nA spatial join combines two spatial datasets based on their geographic relationship — e.g., whether one geometry intersects, contains, or is within another.\nIn R, we use the sf package to handle spatial joins with functions like:\nst_join(x, y, join = st_intersects)\n\nx: the primary spatial object (e.g. point or line data)\ny: the reference spatial object (e.g. polygons)\nst_intersects, st_within, st_contains, etc. define the spatial relationship\n\n\n\n\n\n\n\nNoteSpatial relations (click to expand)\n\n\n\n\n\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue. The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string.\n\n\n\n\n\nWhy is it useful in transport data science?\nSpatial joins are essential tools in transport data science for:\n\nMapping transport observations (e.g. crashes, stops, GPS traces) to zones or regions\nEnriching data with attributes from other layers (e.g. population, accessibility, land use)\nAggregating or summarising transport data by spatial units\n\nThey allow you to combine spatially referenced datasets in meaningful ways to gain insights and build models. We’ll use this technique to identify which LSOA each crash occurred in."
  },
  {
    "objectID": "s6/index.html#loading-lsoa-boundary-data",
    "href": "s6/index.html#loading-lsoa-boundary-data",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "3.2 Loading LSOA boundary data",
    "text": "3.2 Loading LSOA boundary data\nFirst, we load the LSOA (Lower Super Output Area) boundaries for West Yorkshire:\n\n# Load the 2021 LSOA boundary data for West Yorkshire\nlsoa_wy = read_sf(\"https://github.com/itsleeds/tds/releases/download/2025/p6-lsoa_boundary_wy.geojson\")\n\n# Retain only useful variables: LSOA code (lsoa21cd) and LSOA name (lsoa21nm)\nlsoa_wy = lsoa_wy |&gt; select(lsoa21cd, lsoa21nm) \n\n# Check the structure of the data to understand what we're working with\nglimpse(lsoa_wy)\n\n# How many LSOAs are in our dataset?\n# The cat() function in R is short for “concatenate and print”\ncat(\"Number of LSOAs in West Yorkshire:\", nrow(lsoa_wy))\n\nLSOAs are small geographic areas in the UK designed for reporting census and other neighborhood statistics. Each LSOA typically contains 400-1,200 households, and usually have a resident population of 1,000-3,000 people. We’re using the 2021 LSOA boundaries, which align with the most recent UK Census."
  },
  {
    "objectID": "s6/index.html#performing-a-spatial-join",
    "href": "s6/index.html#performing-a-spatial-join",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "3.3 Performing a Spatial Join",
    "text": "3.3 Performing a Spatial Join\nNow we’ll perform a spatial join to determine which LSOA each crash occurred in:\n\n# Perform spatial join to determine which LSOA each crash occurred in\n# This adds LSOA information to each crash point\ncrashes_in_lsoa = st_join(lsoa_wy, crashes_wy)\n\n# Check which columns were added from the LSOA dataset\n# setdiff finds column names that are new in crashes_in_lsoa\nadded_columns = setdiff(colnames(crashes_in_lsoa), colnames(crashes_wy))\n\n# The collapse argument is used in the paste() function,\n# and it controls how to combine multiple elements into a single string. \n# Here we add \", \" between elements\ncat(\"Columns added from LSOA data:\", paste(added_columns, collapse=\", \"),\"\\n\")\n\n# Check if any crashes couldn't be assigned to an LSOA\nna_lsoa = sum(is.na(crashes_in_lsoa$lsoa21cd))\ncat(\"Number of crashes not matching any LSOA:\", na_lsoa)\n\nThe st_join() function links each crash point to the LSOA polygon that contains it. By default, it performs a “within” operation, checking if each point in the first dataset falls within any polygon in the second dataset. After this operation, each crash record will have additional columns from the LSOA dataset, including the LSOA code and name.\nWe use the setdiff() function to find column names that are new in crashes_in_lsoa — i.e., the ones that come from lsoa_wy."
  },
  {
    "objectID": "s6/index.html#aggregating-crashes-by-lsoa",
    "href": "s6/index.html#aggregating-crashes-by-lsoa",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "3.4 Aggregating Crashes by LSOA",
    "text": "3.4 Aggregating Crashes by LSOA\nNext, we’ll aggregate the crash data to count how many crashes of each severity occurred in each LSOA:\n\n# Aggregate crash data by LSOA, counting crashes of each severity\nlsoa_crashes_count = crashes_in_lsoa |&gt;\n  # Remove geometry column as we only need tabular data for aggregation\n  st_drop_geometry() |&gt;\n  # Group by LSOA identifiers\n  group_by(lsoa21cd) |&gt;\n  # Count crashes by severity level\n  summarise(\n    fatal_crashes_n = sum(accident_severity == \"Fatal\"),      # Number of fatal crashes\n    serious_crashes_n = sum(accident_severity == \"Serious\"),  # Number of serious crashes\n    slight_crashes_n = sum(accident_severity == \"Slight\"),     # Number of slight crashes\n    all_crashes_n = fatal_crashes_n + serious_crashes_n + slight_crashes_n # Total number of crashes\n  )\n\n# Display the first few rows of the aggregated data\nhead(lsoa_crashes_count)"
  },
  {
    "objectID": "s6/index.html#joining-aggregated-crash-count-data-to-lsoa-boundaries",
    "href": "s6/index.html#joining-aggregated-crash-count-data-to-lsoa-boundaries",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "4.1 Joining aggregated crash count data to LSOA Boundaries",
    "text": "4.1 Joining aggregated crash count data to LSOA Boundaries\nNow we’ll join the aggregated crash counts back to the LSOA boundary data:\n\n# Join crash count data back to LSOA boundaries\n# This preserves the spatial information while adding the crash statistics\nlsoa_crashes_wy = lsoa_wy |&gt;\n  left_join(lsoa_crashes_count, by = \"lsoa21cd\") \n\n# Check the columns in our joined dataset\ncolnames(lsoa_crashes_wy)\n\n# Replace NA values with 0 for LSOAs that had no crashes\nlsoa_crashes_wy = lsoa_crashes_wy |&gt;\n  mutate(across(c(all_crashes_n, fatal_crashes_n, serious_crashes_n, slight_crashes_n), \n                ~replace_na(., 0)))\n\n# Count how many LSOAs had zero crashes\nzero_crash_lsoas = sum(lsoa_crashes_wy$all_crashes_n == 0)\ncat(\"Number of LSOAs with zero recorded crashes:\", zero_crash_lsoas, \"\\n\")\ncat(\"Percentage of LSOAs with zero crashes:\", round((zero_crash_lsoas/nrow(lsoa_crashes_wy))*100, 2), \"%\\n\")\n\nWe use left_join() to preserve all LSOAs, even those with no crashes. The by parameter specifies the columns to use for matching rows between the datasets. After this join, we have the spatial boundaries with the crash counts attached.\nWe also replace NA values with 0 for LSOAs that had no crashes, and calculate how many LSOAs had zero crashes recorded.\nQuestion: What happens if you use right_join() instead?"
  },
  {
    "objectID": "s6/index.html#loading-census-population-data",
    "href": "s6/index.html#loading-census-population-data",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "4.2 Loading census population data",
    "text": "4.2 Loading census population data\nWe’ll now load census population data for the LSOAs, the census data is obtained from nomis, specifically the Number of usual residents in households and communal establishments in each LSOA. The data has been further cleaned for better processing.\n\n# Load 2021 Census population data for LSOAs\npop_lsoa = read_csv(\"https://github.com/itsleeds/tds/releases/download/2025/p6-census2021_lsoa_pop.csv\")\n\n# Display the first few rows\nhead(pop_lsoa)\n\n# Basic summary statistics of the population data\nsummary(pop_lsoa$pop)\n\nThis dataset contains population figures from the 2021 UK Census for each LSOA in England and Wales."
  },
  {
    "objectID": "s6/index.html#joining-population-data",
    "href": "s6/index.html#joining-population-data",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "4.3 Joining population data",
    "text": "4.3 Joining population data\nNext, we’ll join the population data to our LSOA crash data using the LSOA codes and names as keys:\n\n# Join population data to our LSOA crash data\nlsoa_crashes_wy = lsoa_crashes_wy |&gt;\n  left_join(pop_lsoa, by = c(\"lsoa21cd\", \"lsoa21nm\"))\n\n# Check the first few rows of our joined dataset\nhead(lsoa_crashes_wy)\n\n# Check if we have any missing population values after the join\nmissing_pop = sum(is.na(lsoa_crashes_wy$pop))\ncat(\"Number of LSOAs with missing population data:\", missing_pop, \"\\n\")\n\nThis operation adds the population data to our existing dataset, allowing us to calculate per-capita crash rates."
  },
  {
    "objectID": "s6/index.html#calculating-crashes-per-person",
    "href": "s6/index.html#calculating-crashes-per-person",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "5.1 Calculating Crashes Per Person",
    "text": "5.1 Calculating Crashes Per Person\nWe’ll calculate the number of crashes per person for each LSOA:\n\n# Calculate crashes per person for each LSOA\nlsoa_crashes_wy = lsoa_crashes_wy |&gt;\n  mutate(\n    # Crashes per person (raw rate)\n    crash_pp = all_crashes_n/pop,\n    \n    # Crashes per 1000 people (more intuitive scale)\n    crash_per_1000 = crash_pp * 1000,\n    \n    # Proportion of crashes that were fatal or serious\n    severity_ratio = (fatal_crashes_n + serious_crashes_n) / all_crashes_n\n  )\n\n# Replace NaN values in severity_ratio (from dividing by zero)\nlsoa_crashes_wy$severity_ratio[is.nan(lsoa_crashes_wy$severity_ratio)] = 0\n\n# Summary statistics of our derived metrics\nsummary(lsoa_crashes_wy$crash_pp)\nsummary(lsoa_crashes_wy$crash_per_1000)\nsummary(lsoa_crashes_wy$severity_ratio)\n\nThese derived metrics normalize the crash counts by population, allowing for more meaningful comparisons between areas with different population sizes. Areas with higher values have more crashes relative to their population."
  },
  {
    "objectID": "s6/index.html#citations-and-references",
    "href": "s6/index.html#citations-and-references",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "7.1 Citations and references",
    "text": "7.1 Citations and references\nProperly citing sources is crucial for academic work and professional reports. Quarto makes this easy with built-in citation support.\n\n7.1.1 Adding citations\nTo cite a source, use the @ symbol followed by the citation key from your .bib file:\nRecent research shows that cycling infrastructure reduces casualty rates [@lovelace_stats19_2019].\n\nYou can also use multiple citations [@lovelace_stats19_2019; @morgan_2020].\n\nFor in-text citations with the author's name, use: As shown by @lovelace_stats19_2019, ...\n\n\n7.1.2 Managing your bibliography\n\nCreate a .bib file (e.g., references.bib) with your references\nAdd it to your YAML header:\n---\ntitle: \"My Transport Analysis\"\nbibliography: references.bib\n---\nCitations will automatically appear in your document with a References section at the end\n\nYou can find citation keys from Google Scholar, Zotero, or other reference managers."
  },
  {
    "objectID": "s6/index.html#cross-references-for-figures-and-tables",
    "href": "s6/index.html#cross-references-for-figures-and-tables",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "7.2 Cross-references for figures and tables",
    "text": "7.2 Cross-references for figures and tables\nCross-references allow you to refer to figures, tables, and sections by number, which automatically updates if you reorganize your document.\n\n7.2.1 Figure cross-references\n```{r}\n#| label: fig-crash-map\n#| fig-cap: \"Distribution of road crashes in West Yorkshire\"\n\n# Your plotting code here\ntm_shape(crashes_sf) + tm_dots()\n```\n\nAs shown in @fig-crash-map, crashes are concentrated in urban areas.\n\n\n7.2.2 Table cross-references\n```{r}\n#| label: tbl-crash-summary\n#| tbl-cap: \"Summary statistics for road crashes\"\n\ncrash_summary |&gt; \n  knitr::kable()\n```\n\n@tbl-crash-summary presents the key statistics.\n\n\n7.2.3 Section cross-references\n# Introduction {#sec-intro}\n\nSee @sec-intro for background information."
  },
  {
    "objectID": "s6/index.html#making-your-reports-look-professional",
    "href": "s6/index.html#making-your-reports-look-professional",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "7.3 Making your reports look professional",
    "text": "7.3 Making your reports look professional\n\n7.3.1 Choosing output formats\nQuarto supports multiple output formats. Specify in your YAML header:\n---\ntitle: \"My Report\"\nformat: \n  html:\n    theme: cosmo\n    toc: true\n    number-sections: true\n  pdf:\n    documentclass: article\n  revealjs:  # For presentations\n    theme: dark\n---\n\n\n7.3.2 Formatting tips\nCode chunk options control how code appears:\n```{r}\n#| echo: false      # Hide code, show only output\n#| warning: false   # Suppress warnings\n#| message: false   # Suppress messages\n#| fig-width: 8     # Set figure width\n#| fig-height: 6    # Set figure height\n\n# Your code here\n```\nCallout blocks highlight important information:\n:::{.callout-note}\nThis analysis uses STATS19 data from 2019-2023.\n:::\n\n:::{.callout-warning}\nData completeness varies by year and region.\n:::\n\n:::{.callout-tip}\nConsider using interactive maps for web outputs.\n:::\nTables can be formatted nicely with knitr::kable() or the gt package:\nlibrary(knitr)\ncrash_summary |&gt; \n  knitr::kable(\n    caption = \"Crash statistics by severity\",\n    digits = 2,\n    format.args = list(big.mark = \",\")\n  )\n\n\n7.3.3 Document structure\nA well-organized report should include:\n\nTitle and metadata (author, date, affiliation)\nAbstract or Executive Summary (brief overview)\nIntroduction (context and objectives)\nMethods (data sources, analysis techniques)\nResults (findings with figures and tables)\nDiscussion (interpretation and implications)\nConclusion (summary and recommendations)\nReferences (automatically generated by Quarto)\n\n\n\n7.3.4 Tips for submission\n\nUse version control: Commit your work regularly with Git\nCheck your output: Always render your document before submission to catch errors\nFollow formatting guidelines: Check if there are specific requirements (word count, citation style, etc.)\nInclude reproducible code: Make sure others can run your analysis\nProofread carefully: Check for typos, broken links, and missing figures"
  },
  {
    "objectID": "s6/index.html#additional-resources",
    "href": "s6/index.html#additional-resources",
    "title": "Session 6: Joins, models and publishing your work",
    "section": "7.4 Additional resources",
    "text": "7.4 Additional resources\n\nQuarto documentation\nQuarto citations guide\nQuarto cross-references\nR Markdown: The Definitive Guide\nBuilding Reproducible Analytical Pipelines with R (Rodrigues, 2023)"
  },
  {
    "objectID": "sem1/vis-practical.html",
    "href": "sem1/vis-practical.html",
    "title": "Seminar 1: Traffic estimation and transport data visualisation",
    "section": "",
    "text": "1 Introduction\nIn this seminar, we will build on the routing techniques from the previous sessions by exploring data visualization methods for transport analysis. By the end of this session, you should be able to:\n\nLoad and preprocess OD flow data\nVisualize OD lines and proportional symbol maps\nCompare walking, driving, and cycling flows\nAggregate flows along the road network\nIdentify critical road segments via network centrality\n\n\n\n2 Setup\nBelow are the libraries we will use throughout this session:\n\nknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  fig.width = 7,\n  fig.height = 5,\n  out.width = \"700px\"\n)\n\n# Load necessary libraries\nlibrary(opentripplanner)  # Routing engine (OpenTripPlanner client)\nlibrary(sf)               # Spatial data handling\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\nlibrary(tmap)             # Thematic mapping\nlibrary(stplanr)          # Transport data functions\nlibrary(dplyr)            # Data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(osmextract)       # OSM data handling\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\nCheck the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nlibrary(dodgr)            # Network analysis\n\n\n# Set interactive mapping mode\ntmap_mode(\"view\")\n\n\n\n3 Flow Map Visualization\nFlow maps are useful for understanding the volume of travel between origins and destinations. In this section, we will:\n\nLoad desire lines (flows) data from a GeoJSON file.\nVisualize these lines with widths or colors proportional to demand.\nOptionally aggregate route geometries for more realistic depiction of flows along an actual road network.\n\n\n# Load Demand Data\ndesire_lines = read_sf(\"https://github.com/ITSLeeds/TDS/releases/download/22/NTEM_flow.geojson\") |&gt;\n  select(from, to, all, walk, drive, cycle)\n\ndim(desire_lines)\n\n[1] 502   7\n\n# Let's take the top 50 car trips for demonstration\ndesire_lines_top = desire_lines |&gt;\n  arrange(desc(drive)) |&gt;\n  head(50)\n\n# Quick map to see the distribution of car trips\ntm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"drive\",\n    lwd.scale = tm_scale_continuous(values.scale = 9)\n  ) +\n  tm_layout(legend.bg.color = \"white\")\n\n\n\n\n\n\n\n\n4 Proportional Symbol Flow Maps\nNow, let’s illustrate an alternative method: proportional symbols at origin or destination points. This is useful when you want to quickly see where demand is concentrated.\n\n# Summarize total flows by origin\n\norigin_flows = desire_lines |&gt;\n  group_by(from) |&gt;\n  summarise(\n    total_drive = sum(drive, na.rm = TRUE),\n    total_walk  = sum(walk, na.rm = TRUE),\n    total_cycle = sum(cycle, na.rm = TRUE),\n    `% drive` = total_drive / sum(all, na.rm = TRUE),\n    geometry = st_centroid(st_union(geometry))  \n  )\n\n# Simple map with proportional circles for drive volumes\n\ntm_shape(origin_flows) +\n  tm_bubbles(\n    size    = \"total_drive\",       # bubble size ~ drive volume\n    size.scale = tm_scale_intervals(values.scale = 2, values.range = c(0.5, 2)),\n    fill = \"% drive\",\n    fill.scale = tm_scale_continuous(values = \"brewer.reds\")\n  ) +\n  tm_title(\"Proportional Symbol Map of Drive Demand by Origin\")\n\n\n\n\n\n\n\nEach origin is represented by a circle whose radius and color intensity reflect the total number of driving trips. You can modify palettes, breaks, and scaling to highlight variations.\n\n\n5 Mode-Specific Analysis\nWe have have columns walk, drive, cycle in desire_lines. We can map them separately or side-by-side. We can also color lines by the dominant mode.\n\n# Let's create 3 separate maps: drive, walk, cycle\ntmap_mode(\"plot\")\nm_drive = tm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"drive\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"red\"\n  ) +\n  tm_title(\"Driving Flows\")\n\nm_walk = tm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"walk\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"green\"\n  ) +\n  tm_title(\"Walking Flows\")\n\nm_cycle = tm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"cycle\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"blue\"\n  ) +\n  tm_title(\"Cycling Flows\")\n\ntmap_arrange(m_drive, m_walk, m_cycle, ncol=3)\n\n\n\n\n\n\n\n\nThis tmap_arrange() will output a single figure with three columns, each illustrating flows by one mode. Students can visually compare the differences: maybe driving flows are much thicker on longer corridors, while walking flows are concentrated in the city center.\n\n\n6 Aggregating Flows with Actual Routes\nRather than drawing direct origin-destination lines, we can route each flow along the road network and then aggregate them to see which streets carry the most traffic. This uses stplanr::overline() to merge lines that overlap.\n\n# Download pre-routed lines for demonstration\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/routes_drive_25.geojson\"\nroutes_drive = read_sf(u)\n\n# Inspect the summary of the drive.x variable (car trips)\nsummary(routes_drive$drive.x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   41.0   171.2   327.0   415.1   546.5  2346.0 \n\ntm_shape(routes_drive) +\n  tm_lines(\n    lwd = \"drive.x\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"red\"\n  ) +\n  tm_title(\"Road Congestion (drive_total)\")\n\n\n\n\n\n\n\n\n\n\n7 Network Centrality Analysis\nBetweenness centrality indicates how often a road (or node) lies on the shortest path between other points in a network. Roads with high centrality are typically crucial for overall connectivity.\nHere, we demonstrate how to:\n\nDownload roads from OpenStreetMap (OSM) for the Isle of Wight.\nWeight the network for motor vehicle usage.\nCompute betweenness centrality with dodgr.\nConvert the results back to sf for mapping.\n\n\n# Get Isle of Wight road network\n# We choose 'primary', 'secondary', 'tertiary' roads for demonstration\nroads = oe_get(\"Isle of Wight\", \n                extra_tags = c(\"maxspeed\", \"oneway\")) |&gt;\n  filter(highway %in% c(\"primary\", \"secondary\", \"tertiary\"))\n\n# Weight the street network for motorcar usage\ngraph = weight_streetnet(\n  roads,\n  wt_profile = \"motorcar\",\n  type_col = \"highway\",\n  id_col = \"osm_id\",\n  keep_cols = c(\"maxspeed\", \"oneway\")\n)\n\n# Calculate betweenness centrality\ncentrality = dodgr_centrality(graph)\n\n# Convert to sf for visualization\ncentrality_sf = dodgr_to_sf(centrality)\n\n# Visualize critical links\ntm_shape(centrality_sf) +\n  tm_lines(\n    col = \"centrality\",\n    col.scale = tm_scale_intervals(style = \"fisher\", values = \"-viridis\"),\n    col.legend = tm_legend(title = \"Betweenness Centrality\"),\n    lwd = 3\n  ) \n\nThe code above should generate a map that looks something like this:\n\nHigh values in the centrality column indicate roads that act as vital connectors in the regional transport network.\n\n\n8 Extra Exercises: 3D Visualization\nA 3D perspective can often reveal relationships between travel flows and the underlying topography more effectively. Below, we demonstrate how to retrieve elevation data and render a 3D hillshade using the rayshader package. You may also be interested in overlaying flow lines onto a 3D terrain model to enhance visualization.\nNote: the following code requires you to install the rayshader elevatr gifski rgl package. Results not shown in website.\n\nlibrary(rayshader)        # 3D data visualization\nlibrary(elevatr)          # Elevation data\nlibrary(gifski)           # Creating GIF animations\nlibrary(rgl)              # 3D visualization device\n\nassign(\"has_internet_via_proxy\", TRUE, environment(curl::has_internet))\ncurl::has_internet()\n# Example: Elevation data near a location in the UK\ncoords = data.frame(x = -2.087918, y = 53.71534)\ncoords_sf = st_as_sf(coords, coords = c(\"x\", \"y\"), crs = 4326)\n# Get an elevation raster at zoom level 11 (~ 10m resolution, depending on region)\nelevation = elevatr::get_elev_raster(\n  locations = coords_sf,\n  z = 11\n)\n# Convert the raster to a matrix for rayshader\nelev_matrix = rayshader::raster_to_matrix(elevation)\n# Create a hillshade layer\nhillshade_matrix = rayshader::ray_shade(elev_matrix, zscale = 15)\n# Clear existing rgl device\nrgl::rgl.clear()\n# Render a 3D plot of the terrain\nrayshader::plot_3d(\n  heightmap = elev_matrix,\n  hillshade = hillshade_matrix,\n  zscale = 15,\n  windowsize = c(1000, 800)\n)\n# Adjust camera view\nrgl::view3d(theta = 30, phi = 30, zoom = 0.75)\nrgl::rglwidget()\n\n\n\n9 Conclusions\nIn this session, you learned how to:\n\nCreate flow maps to visualize travel demand from an OD dataset.\nCompare flows by mode (driving, walking, cycling) to understand differences in spatial patterns.\nAggregate routes along the road network (using stplanr::overline) to highlight heavily used corridors.\nCompute betweenness centrality (using dodgr) to pinpoint critical road segments crucial for connectivity."
  },
  {
    "objectID": "s1/slides.html#who-transport-data-science-team",
    "href": "s1/slides.html#who-transport-data-science-team",
    "title": "Introduction to transport data science",
    "section": "Who: Transport Data Science team",
    "text": "Who: Transport Data Science team\nRobin Lovelace\n\nProfessor of Transport Data Science\nResearching transport futures and active travel planning\nR developer and teacher, author of Geocomputation with R\n\nYuanxuan Yang\n\nLecturer in Data Science of Transport\nNew and Emerging Forms of Data: Investigating novel data sources and their applications in urban mobility and transport planning."
  },
  {
    "objectID": "s1/slides.html#tds-team-ii",
    "href": "s1/slides.html#tds-team-ii",
    "title": "Introduction to transport data science",
    "section": "TDS Team II",
    "text": "TDS Team II\nMalcolm Morgan\n\nSenior researcher at ITS with expertise in routing + web\nDeveloper of the Propensity to Cycle Tool and PBCC\n\nDemonstrators\nYou!"
  },
  {
    "objectID": "s1/slides.html#what-is-transport-data-science",
    "href": "s1/slides.html#what-is-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "What is transport data science?",
    "text": "What is transport data science?\n\nThe application of data science to transport datasets and problems\nRaising the question…\nWhat is data science?\nA discipline “that allows you to turn raw data into understanding, insight, and knowledge” (Grolemund, 2016)\n\nIn other words…\n\nStatistics that is actually useful!"
  },
  {
    "objectID": "s1/slides.html#why-take-transport-data-science",
    "href": "s1/slides.html#why-take-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "Why take Transport Data Science",
    "text": "Why take Transport Data Science\n\n\n\nNew skills (cutting edge R and/or Python packages)\nPotential for impacts\nAllows you to do new things with data\nIt might get you a job!"
  },
  {
    "objectID": "s1/slides.html#example",
    "href": "s1/slides.html#example",
    "title": "Introduction to transport data science",
    "section": "Example",
    "text": "Example\nData science spin-out company: ImpactML"
  },
  {
    "objectID": "s1/slides.html#data-science-employability",
    "href": "s1/slides.html#data-science-employability",
    "title": "Introduction to transport data science",
    "section": "Data science employability",
    "text": "Data science employability\n\n\n\nThe Bureau of Labor Statistics in the US projects a 35% increase in data science roles in decade 2022-2032.” Source: visualisecurious.com"
  },
  {
    "objectID": "s1/slides.html#live-demo-npt.scot-web-app",
    "href": "s1/slides.html#live-demo-npt.scot-web-app",
    "title": "Introduction to transport data science",
    "section": "Live demo: npt.scot web app",
    "text": "Live demo: npt.scot web app"
  },
  {
    "objectID": "s1/slides.html#the-history-of-tds",
    "href": "s1/slides.html#the-history-of-tds",
    "title": "Introduction to transport data science",
    "section": "The history of TDS",
    "text": "The history of TDS\n\n2017: Transport Data Science created, led by Dr Charles Fox, Computer Scientist, author of Transport Data Science book (Fox, 2018)\nThe focus was on databases and Bayesian methods\n2019: I inherited the module, which was attended by ITS students\nSummer 2019: Python code published in the module ‘repo’:\n\ngithub.com/ITSLeeds"
  },
  {
    "objectID": "s1/slides.html#history-of-tds-ii",
    "href": "s1/slides.html#history-of-tds-ii",
    "title": "Introduction to transport data science",
    "section": "History of TDS II",
    "text": "History of TDS II\n\nJanuary 2020: Available, Data Science MSc course\nMarch 2020: Switch to online teaching\n2021-2023: Updated module, focus on methods\n2024: Switch to combined lecture and practical sessions\n2025+: Expand, online course? book? stay in touch!\n\n\n\nMilestone passed in my academic career, first online-only delivery of lecture ITSLeeds (2025), seems to have worked, live code demo with #rstats/rstudio, recording, chat + all🎉Thanks students for ‘attending’ + remote participation, we’ll get through this together.#coronavirus pic.twitter.com/wlAUxmZj5r\n\n— Robin Lovelace March 17, 2020"
  },
  {
    "objectID": "s1/slides.html#reading-list",
    "href": "s1/slides.html#reading-list",
    "title": "Introduction to transport data science",
    "section": "Reading list",
    "text": "Reading list\nSee the reading list for details"
  },
  {
    "objectID": "s1/slides.html#objectives",
    "href": "s1/slides.html#objectives",
    "title": "Introduction to transport data science",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the structure of transport datasets\nUnderstand how to obtain, clean and store transport related data\nGain proficiency in command-line tools for handling large transport datasets\nProduce data visualizations, static and interactive\n Learn how to join together the components of transport data science into a cohesive project portfolio"
  },
  {
    "objectID": "s1/slides.html#assessment-for-those-doing-this-as-credit-bearing",
    "href": "s1/slides.html#assessment-for-those-doing-this-as-credit-bearing",
    "title": "Introduction to transport data science",
    "section": "Assessment (for those doing this as credit-bearing)",
    "text": "Assessment (for those doing this as credit-bearing)\n\nYou will build-up a portfolio of work\n100% coursework assessed, you will submit by\nWritten in code - will be graded for reproducibility\nCode chunks and figures are encouraged\nYou will submit a non-assessed 2 page pdf + qmd"
  },
  {
    "objectID": "s1/slides.html#feedback",
    "href": "s1/slides.html#feedback",
    "title": "Introduction to transport data science",
    "section": "Feedback",
    "text": "Feedback\n\nThe module is taught by two really well organised and enthusiastic professors, great module, the seminars, structured and unstructured learning was great and well thought out, all came together well\n\n\nI wish this module was 60 credits instead of 15 because i just want more of it."
  },
  {
    "objectID": "s1/slides.html#timetable",
    "href": "s1/slides.html#timetable",
    "title": "Introduction to transport data science",
    "section": "Timetable",
    "text": "Timetable\nSee the schedule for details"
  },
  {
    "objectID": "s1/slides.html#what-is-science",
    "href": "s1/slides.html#what-is-science",
    "title": "Introduction to transport data science",
    "section": "What is science?",
    "text": "What is science?\n\n\n\nScientific knowledge is hypotheses that can be falsified\nScience is the process of generating falsifiable hypotheses and testing them\nIn a reproducible way\nSystematically\n\n\n\n\nFalsifiability is central to the scientific process (Popper 1959)\nAll of which requires software conducive to reproducibility"
  },
  {
    "objectID": "s1/slides.html#transport-planning-software",
    "href": "s1/slides.html#transport-planning-software",
    "title": "Introduction to transport data science",
    "section": "Transport planning software",
    "text": "Transport planning software\nTransport modelling software products are a vital component of modern transport planning and research.\n\nThey generate the evidence base on which strategic investments are made and, furthermore,\nprovide a powerful mechanism for researching alternative futures.\n\nIt would not be an overstatement to say that software determines the range of futures that are visible to policymakers. This makes status of transport modelling software and how it may evolve in the future important questions.\nWhat will transport software look like? What will their capabilities be? And who will control? Answers to each of these questions will affect the future of transport systems.\n\nPremise: transport planning/modelling software used in practice will become is becoming increasingly data-driven, modular and open."
  },
  {
    "objectID": "s1/slides.html#current-transport-software",
    "href": "s1/slides.html#current-transport-software",
    "title": "Introduction to transport data science",
    "section": "Current transport software",
    "text": "Current transport software\n\n\n\n\n\n\n\n\n\n4-stage model still dominates transport planning models (Boyce and Williams 2015)"
  },
  {
    "objectID": "s1/slides.html#the-four-stage-model",
    "href": "s1/slides.html#the-four-stage-model",
    "title": "Introduction to transport data science",
    "section": "The four stage model",
    "text": "The four stage model\n\nImpacts the current software landscape\nDominated by a few proprietary products\nLimited support community online\nHigh degree of lock-in\nLimited cross-department collaboration"
  },
  {
    "objectID": "s1/slides.html#existing-products",
    "href": "s1/slides.html#existing-products",
    "title": "Introduction to transport data science",
    "section": "Existing products",
    "text": "Existing products\nSample of transport modelling software in use by practitioners. \n\n\n\n\n\nSoftware\nCompany/Developer\nCompany HQ\nLicence\nCitations\n\n\n\n\nVisum\nPTV\nGermany\nProprietary\n1810\n\n\nMATSim\nTU Berlin\nGermany\nOpen source (GPL)\n1470\n\n\nTransCAD\nCaliper\nUSA\nProprietary\n1360\n\n\nSUMO\nDLR\nGermany\nOpen source (EPL)\n1310\n\n\nEmme\nINRO\nCanada\nProprietary\n780\n\n\nCube\nCitilabs\nUSA\nProprietary\n400\n\n\nsDNA\nCardiff University\nUK\nOpen source (GPL)\n170"
  },
  {
    "objectID": "s1/slides.html#user-support",
    "href": "s1/slides.html#user-support",
    "title": "Introduction to transport data science",
    "section": "User support",
    "text": "User support\nGetting help is vital for leaning/improving software\n\n“10-Hour Service Pack $2,000” (source: caliper.com/tcprice.htm)"
  },
  {
    "objectID": "s1/slides.html#online-communities",
    "href": "s1/slides.html#online-communities",
    "title": "Introduction to transport data science",
    "section": "Online communities",
    "text": "Online communities\n\ngis.stackexchange.com has 21,314 questions\nr-sig-geo has 1000s of posts\nRStudio’s Discourse community has 65,000+ posts already!\nNo clear transport equivalent (e.g. earthscience.stackexchange.com is in beta)\nSolution: build our own community!\n\nSee https://github.com/ITSLeeds/TDS/issues for example\nPlace for discussions: https://github.com/itsleeds/tds/discussions"
  },
  {
    "objectID": "s1/slides.html#best-way-to-get-support-is-peer-to-peer",
    "href": "s1/slides.html#best-way-to-get-support-is-peer-to-peer",
    "title": "Introduction to transport data science",
    "section": "Best way to get support is peer-to-peer:",
    "text": "Best way to get support is peer-to-peer:\n\nSource: https://community.rstudio.com/about"
  },
  {
    "objectID": "s1/slides.html#how-is-data-science-used-in-the-pct",
    "href": "s1/slides.html#how-is-data-science-used-in-the-pct",
    "title": "Introduction to transport data science",
    "section": "How is data science used in the PCT?",
    "text": "How is data science used in the PCT?\n\nIt’s all reproducible, e.g.:\nFind commuting desire lines in West Yorkshire between 1 and 3 km long in which more people drive than cycle:"
  },
  {
    "objectID": "s1/slides.html#visualising-data",
    "href": "s1/slides.html#visualising-data",
    "title": "Introduction to transport data science",
    "section": "Visualising data",
    "text": "Visualising data\nA fundamental part of data science is being able to understand your data.\nThat requires visualisation, R is great for that:"
  },
  {
    "objectID": "s1/slides.html#interactively",
    "href": "s1/slides.html#interactively",
    "title": "Introduction to transport data science",
    "section": "Interactively",
    "text": "Interactively"
  },
  {
    "objectID": "s1/slides.html#processing-data-with-code",
    "href": "s1/slides.html#processing-data-with-code",
    "title": "Introduction to transport data science",
    "section": "Processing data with code",
    "text": "Processing data with code\n\nNow we have data in our computer, and verified it works, we can use it\nWhich places are most car dependent?"
  },
  {
    "objectID": "s1/slides.html#checking-the-results",
    "href": "s1/slides.html#checking-the-results",
    "title": "Introduction to transport data science",
    "section": "Checking the results:",
    "text": "Checking the results:"
  },
  {
    "objectID": "s1/slides.html#r-vs-python",
    "href": "s1/slides.html#r-vs-python",
    "title": "Introduction to transport data science",
    "section": "R vs Python",
    "text": "R vs Python\n\nLots of debate on this topic - see https://blog.usejournal.com/python-vs-and-r-for-data-science-833b48ccc91d\n\nHow to decide?\n\nIf priority: getting things done quick (with support from me ;) go with R\nIf you already know Python and are 100% confident you can generate reproducible results, go with that\nIf you want to be avant-garde and try something else like Julia, do it (as long as it’s reproducible)\n\n\nGamification\n\n\n\n\n\n\n\n\n\n\nCompletely open source, written in rust\nSource: video at https://github.com/dabreegster/abstreet/#ab-street"
  },
  {
    "objectID": "s1/slides.html#summary",
    "href": "s1/slides.html#summary",
    "title": "Introduction to transport data science",
    "section": "Summary",
    "text": "Summary\n\nWalk and understand the data before doing complex things\nVisualise the data, ask questions of it, descriptive stats\nOnly then add complexity to your analysis\nStarting point for this: Transport chapter of Geocomputation with R (Lovelace, Nowosad, and Münchow 2025)"
  },
  {
    "objectID": "s1/s1project/foundations.html",
    "href": "s1/s1project/foundations.html",
    "title": "1 Python example",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\n\nHello this is some text.\n\ncasualty_type = c(\"cat\", \"dog\", \"person\")\ncasualty_age = seq(from = 20, to = 60, by = 20)\ncrashes = data.frame(casualty_type, casualty_age)\nplot(crashes$casualty_age)\n\nSubsetting.\n\ncrashes$casualty_type\ncrashes[[1]]\ncrashes[2,1]\n\n\ncrashes |&gt;\n  select(casualty_type)\n\ncrashes |&gt; \n  filter(casualty_age &gt; 35)\n\ncrashes |&gt; \n  filter(casualty_age-20 &gt; 35)\n\ncrashes |&gt;\n  ggplot() +\n  geom_bar(aes(x = casualty_age, fill = casualty_type))\n\n\nac = stats19::get_stats19(year = 2020, type = \"collision\")\nclass(ac)\ndim(ac)\nac_2021 = stats19::get_stats19(year = 2021, type = \"collision\")\nnrow(ac)\nnrow(ac_2021)\n# # After googling \"combine 2 data frames\" let's try rbind\n# ??combine\n# ?rbind\nac = rbind(ac, ac_2021)\ndim(ac)\nac_datetime = c(ac$datetime, ac_2021$datetime)\nlength(ac_datetime)\nrange(ac_datetime)\n\nclass(ac)\nstr(ac)\nnames(ac)\n# aggregate this by day to show \n# how crash numbers varied over the year\nac_by_year = ac |&gt;\n  group_by(date) |&gt;\n  summarise(\n    n_crashes = n()\n  )\nac_by_year |&gt;\n  mutate(\n    `N. crashes per year` = n_crashes,\n    `Week average` = zoo::rollmean(n_crashes, 7, na.pad = TRUE),\n    Date = date,\n  ) |&gt; \n  ggplot(aes(x = Date, y = `N. crashes per year`)) +\n  geom_point(alpha = 0.1) +\n  ylim(c(0, NA)) +\n  # geom_smooth() +\n  # weekly rolling average\n  geom_line(aes(Date, `Week average`), colour = \"red\") +\n  theme_minimal()\n\n\n# Updated plot with title and legend...\nac_by_year |&gt;\n  mutate(\n    `N. crashes per year` = n_crashes,\n    `Week average` = zoo::rollmean(n_crashes, 7, na.pad = TRUE),\n    Date = date,\n  ) |&gt; \n  ggplot(aes(x = Date, y = `N. crashes per year`)) +\n  geom_point(alpha = 0.1) +\n  ylim(c(0, NA)) +\n  # geom_smooth() +\n  # weekly rolling average\n  geom_line(aes(Date, `Week average`, colour = \"Week average\")) +\n  theme_minimal() +\n  labs(\n    colour = \"Legend\"\n  ) +\n  scale_colour_manual(values = c(\"Week average\" = \"red\")) +\n  ggtitle(\"Collions/day, 2020 to 2021\") +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n1 Python example\n\ncasualty_type_py = [\"a\", \"B\", \"c\"]\ncasualty_type_py"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "This module focuses on applying data science techniques to solve real-world transport problems. Based at the University of Leeds’ Institute for Transport Studies (module code TRAN5340M), the course is led by Robin Lovelace, Professor of Transport Data Science and developer of several data-driven solutions for effective transport planning.\nThe course has evolved over a decade of teaching and research in the field. It aims to equip you with up-to-date and future-proof skills through practical examples and reproducible workflows using industry-standard data science tools.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#hardware",
    "href": "index.html#hardware",
    "title": "Transport Data Science",
    "section": "Hardware",
    "text": "Hardware\nWe highly recommend having access to a computer with at least 8 GB of RAM that you have permission to install software on.\nAlternatively, you could use cloud-based services such as RStudio Cloud, Google Colab, or GitHub Codespaces. However, you would need to be comfortable using these services and may miss out on some benefits of using your own computer.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#computing-experience",
    "href": "index.html#computing-experience",
    "title": "Transport Data Science",
    "section": "Computing experience",
    "text": "Computing experience\nYou should be comfortable with general computing tasks, such as:\n\nCreating folders and managing files\nInstalling software\nUsing command line interfaces (PowerShell in Windows, Terminal in macOS, or Linux shell)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#data-science-experience-prerequisites",
    "href": "index.html#data-science-experience-prerequisites",
    "title": "Transport Data Science",
    "section": "Data science experience prerequisites",
    "text": "Data science experience prerequisites\nPrior experience using R or Python is essential. This could include:\n\nUsing these languages in professional work\nExperience from previous degrees\nCompletion of relevant online courses\n\nStudents can demonstrate this prerequisite knowledge by showing evidence they have:\n\nWorked with R previously\nCompleted online courses such as the first 4 sessions in the RStudio Primers series\nCompleted DataCamp’s Free Introduction to R course\n\nSubstantial programming and data science experience in previous professional or academic work using languages like R or Python also satisfies the prerequisite requirements.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#quickstart-with-github-codespaces",
    "href": "index.html#quickstart-with-github-codespaces",
    "title": "Transport Data Science",
    "section": "Quickstart with GitHub Codespaces",
    "text": "Quickstart with GitHub Codespaces\nFor a quick cloud-based setup, you can use GitHub Codespaces to access the course materials:\n\nSign up to GitHub\nFork the repository\nClick the “Open in GitHub Codespaces” button above\n\nAlternatively, use the following link:\n\n\n\nOpen in GitHub Codespaces",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#r",
    "href": "index.html#r",
    "title": "Transport Data Science",
    "section": "R",
    "text": "R\nInstall a recent version of R (4.3.0 or above) and an IDE:\n\nR from cran.r-project.org\nRStudio from rstudio.com (recommended)\nAlternatively, VS Code with the R extension installed (if you have prior experience with it)\n\nYou’ll also need to install R packages:\n\nIndividual packages can be installed by opening RStudio and typing commands like install.packages(\"stats19\") in the R console\nTo install all dependencies for the module at once, run the following command in the R console:\n\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"itsleeds/tds\")\n\nSee Section 1.5 of the online guide Reproducible Road Safety Research with R for instructions on how to install key packages we will use in the module.1",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Transport Data Science",
    "section": "Python",
    "text": "Python\nIf you choose to use Python, you should be comfortable with:\n\nInstalling Python\nManaging your own Python environment\nInstalling packages and resolving package conflicts\n\nFor Python users, we recommend using an environment manager such as:\n\npixi (which can manage both R and Python environments)\nDocker (best practice for reproducibility and isolation)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#docker-advanced",
    "href": "index.html#docker-advanced",
    "title": "Transport Data Science",
    "section": "Docker (advanced)",
    "text": "Docker (advanced)\nWe maintain a Docker image containing all necessary software to complete the course with VS Code, Quarto, and a Devcontainer setup.\nAdvantages:\n\nEnsures reproducibility\nSaves time installing software\n\nDisadvantages:\n\nDocker can be challenging to install\nDifficult to use if you’re unfamiliar with Docker\n\nWe recommend this approach only for people who are confident with Docker or willing to invest time learning it.\nFor guidance, see:\n\nDocker installation instructions\nDevcontainers documentation on github.com\nThe tds Dockerfile and devcontainer.json",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Transport Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n For further guidance on setting-up your computer to run R and RStudio for spatial data, see these links, we recommend Chapter 2 of Geocomputation with R (the Prerequisites section contains links for installing spatial software on Mac, Linux and Windows): https://r.geocompx.org/spatial-class.html and Chapter 2 of the online book Efficient R Programming, particularly sections 2.3 and 2.5, for details on R installation and set-up and the project management section.↩︎",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "minihack-transport-data.html",
    "href": "minihack-transport-data.html",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "This event is designed to build data, coding and reproducible research skills for Institute for Transport Studies staff and students. Please contact the organisers (Robin Lovelace) if you are not based at the University of Leeds and would like to join in. It will take place on the 8th May as a pre-event before a lecture on data science and is open to staff and students at the University of Leeds and Transport professions. See here to sign-up.\n\n\n\nReactivate the Transport Data Science Hackathons\nFacilitate learning and collaboration among participants\nOutcomes for participants:\n\nLearning basics of packaging and modular coding\nData wrangling with tidyverse\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nDemonstrate the potential of open data (transparency, participation, research) and reproducible/open work-flows.\n\n\n\n\n\n\nNone: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details\n\n\n\n\n\n13:00 - 13:30: Presentation of the challenges\n\nTransmilenio: Victor Cantillo García\nBring your own data (BYD)\n\n5 minute pitches by anyone who wants to work on their own data challenge\n\n\n13:30 - 14:00: Importing the data\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: The hackathon\n15:15 - 15:45: Presentation of the results\n15:45 onwards: Networking and lecture (optional, see ticketsource.us for tickets)\n\n\n\n\nThe prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#objectives",
    "href": "minihack-transport-data.html#objectives",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "Reactivate the Transport Data Science Hackathons\nFacilitate learning and collaboration among participants\nOutcomes for participants:\n\nLearning basics of packaging and modular coding\nData wrangling with tidyverse\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nDemonstrate the potential of open data (transparency, participation, research) and reproducible/open work-flows.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#prerequisites",
    "href": "minihack-transport-data.html#prerequisites",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "None: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#schedule",
    "href": "minihack-transport-data.html#schedule",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "13:00 - 13:30: Presentation of the challenges\n\nTransmilenio: Victor Cantillo García\nBring your own data (BYD)\n\n5 minute pitches by anyone who wants to work on their own data challenge\n\n\n13:30 - 14:00: Importing the data\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: The hackathon\n15:15 - 15:45: Presentation of the results\n15:45 onwards: Networking and lecture (optional, see ticketsource.us for tickets)",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#prizes",
    "href": "minihack-transport-data.html#prizes",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "The prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#transmilenio-data",
    "href": "minihack-transport-data.html#transmilenio-data",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "2.1 Transmilenio data",
    "text": "2.1 Transmilenio data\n\nTransMilenio (TM) is the organisation in charge of managing all components of Bogotá’s integrated public transport system.\nTM publishes a lot of their data for public use, in line with the open data policy of Bogotá.\nData includes:\n\nSpatial: GTFS, station location and lines of BRT, regular buses, and cable lines.\nCounts: Raw daily tap-in records, and aggregated boarding / alighting and exit counts by station and 15 minutes interval.\n\n\n\n2.1.1 Motivation:\n\nTM published some useful maps but they are not easily reproducible.\nAccessing the data is not straightforward as the count information is saved in individual .csv files by day.\n\n\n\n2.1.2 Goal:\n\nDevelop a set of functions that can be integrated into an R library to access and analyse the open data published by TM.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#origin-destination-data",
    "href": "minihack-transport-data.html#origin-destination-data",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "2.2 Origin destination data",
    "text": "2.2 Origin destination data\n\nSee https://github.com/itsleeds/2021-census-od-data for 2021 OD data from the Census",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#bring-your-own-data",
    "href": "minihack-transport-data.html#bring-your-own-data",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "2.3 Bring your own data",
    "text": "2.3 Bring your own data\nParticipants are welcome to bring their own data to the event. Please mention the dataset in the sign-up form (see link above).",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "module-info.html",
    "href": "module-info.html",
    "title": "Module Information and Help",
    "section": "",
    "text": "Welcome to the Transport Data Science module.\nModule Leader: Robin Lovelace\nDescription: This module focuses on applying data science techniques to solve real-world transport problems. It teaches data science and AI-driven code development techniques to solve applied transport problems using a range of transport datasets. This involves origin-destination modelling, routing, mode choice estimation, and geocomputation using open source software. You’ll gain practical skills for sharing code and publishing reproducible results for maximum societal benefit.\nTeaching Delivery: Topics will be delivered in a series of weekly sessions in which is a lecture followed by a practical. Two seminars throughout the the module provide an opportunity to apply the methods and techniques to real-world problems, with input from external experts working in the field. Material for lectures will be posted on Minerva and/or this website, usually 48 hours before the lectures.\nIn the following sections you will find more information about the module.\nPlease browse the Reading List for material that will be relevant to your classes."
  },
  {
    "objectID": "module-info.html#welcome",
    "href": "module-info.html#welcome",
    "title": "Module Information and Help",
    "section": "",
    "text": "Welcome to the Transport Data Science module.\nModule Leader: Robin Lovelace\nDescription: This module focuses on applying data science techniques to solve real-world transport problems. It teaches data science and AI-driven code development techniques to solve applied transport problems using a range of transport datasets. This involves origin-destination modelling, routing, mode choice estimation, and geocomputation using open source software. You’ll gain practical skills for sharing code and publishing reproducible results for maximum societal benefit.\nTeaching Delivery: Topics will be delivered in a series of weekly sessions in which is a lecture followed by a practical. Two seminars throughout the the module provide an opportunity to apply the methods and techniques to real-world problems, with input from external experts working in the field. Material for lectures will be posted on Minerva and/or this website, usually 48 hours before the lectures.\nIn the following sections you will find more information about the module.\nPlease browse the Reading List for material that will be relevant to your classes."
  },
  {
    "objectID": "module-info.html#module-teaching-staff",
    "href": "module-info.html#module-teaching-staff",
    "title": "Module Information and Help",
    "section": "2 Module Teaching Staff",
    "text": "2 Module Teaching Staff\n\nModule Leader: Robin Lovelace\n\nEmail: R.Lovelace@leeds.ac.uk\nStaff Website: Professor Robin Lovelace\nOffice Hours: 14:00-16:00, contact via email or Teams."
  },
  {
    "objectID": "module-info.html#module-structure",
    "href": "module-info.html#module-structure",
    "title": "Module Information and Help",
    "section": "3 Module Structure",
    "text": "3 Module Structure\n\nThe module consists of 6 lectures and 6 practical sessions, delivered in 3-hour blocks each week. There are also two 3-hour seminars.\nSee the Schedule for the module timetable, showing the week, class title, tutor, and content of each session.\nThe assessment structure includes a draft portfolio submission for formative feedback, followed by the final coursework deadline. Key assessments and submission details can be found on the Assessment Overview page."
  },
  {
    "objectID": "module-info.html#student-support-details",
    "href": "module-info.html#student-support-details",
    "title": "Module Information and Help",
    "section": "4 Student Support Details",
    "text": "4 Student Support Details\n\nPlease refer to the Student Support section on Minerva for full details."
  },
  {
    "objectID": "module-info.html#module-accessibility-statement",
    "href": "module-info.html#module-accessibility-statement",
    "title": "Module Information and Help",
    "section": "5 Module Accessibility Statement",
    "text": "5 Module Accessibility Statement\n\n\nWe have made the contents of the module as accessible as possible long-term, by creating an open-access website that you can navigate to at itsleeds.github.io/tds. The website contains some of the key materials that can be used with assistive technologies, while Minerva contains additional resources, such as datasets and information in additional formats.\nThe assessment on this module involves computer lab work and coursework. Please contact us regarding reasonable adjustments.\nIf you are unable to access any of the content of this module for disability-related reasons, please contact the module leader. You should also ensure that you have registered for support from Disability Services."
  },
  {
    "objectID": "module-info.html#how-to-give-module-feedback",
    "href": "module-info.html#how-to-give-module-feedback",
    "title": "Module Information and Help",
    "section": "6 How to Give Module Feedback",
    "text": "6 How to Give Module Feedback\n\nFeedback is important to ensure your module can be improved and developed to provide the best learning experience possible.\n\nYou can give feedback about your module to your course reps to raise at the Student Staff Partnership Forum. If you are unsure who your course rep is, email academicsupport@luu.leeds.ac.uk.\nYou can speak to your module leader, seminar tutors, or personal tutor.\nYour module leader might ask for mid-module feedback, and you’ll be asked to complete a module evaluation survey towards the end of the module.\n\nIf you have any questions or concerns, please contact the Module Leader. We encourage you to raise issues with the module leader as soon as they arise so that they can be dealt with in a timely manner.\nWe also welcome your module evaluation at the end of the module. All students at Leeds have the opportunity to offer feedback on their modules. Feedback is important to us and it helps us to make improvements, to understand the student experience, and to see what is already working well.\nYour feedback will be used by your module leaders, and sometimes also your seminar tutor or programme leader, to reflect on the delivery of the module and to make improvements.\n\n6.1 Explorance Blue\nExplorance Blue is the University of Leeds’ system for processing module evaluations. This section explains how Explorance Blue works in practice.\nWhen your module evaluation goes live, you will receive an email inviting you to log into the system. This link will be unique to you, so please do not share it. \nModule evaluations go live towards the end of a semester. When you have evaluations to complete you will be directed via an individualised link to a page with your open evaluations listed on it. It looks like the image below. The list of modules they have been asked to evaluate will appear under ‘Tasks’. \nFor further information, please refer to Student Education Service’s “HOW TO” guide on Explorance Blue.\nYou can also provide feedback through your student reps who can take this information to the regular Staff Student Partnership Forums."
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Introduction to Transport Data Science"
  },
  {
    "objectID": "slides/road-safety.html#welcome",
    "href": "slides/road-safety.html#welcome",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Welcome!",
    "text": "Welcome!\nReproducible data science for road safety research\nRS5C Pre-conference Workshop\n2nd September 2025"
  },
  {
    "objectID": "slides/road-safety.html#agenda",
    "href": "slides/road-safety.html#agenda",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Agenda",
    "text": "Agenda\n\n13:00-14:00 Networking lunch\n14:00-14:15 Introduction to reproducible research\n14:15-14:30 New datasets and tools (Richard Owen, Agilysis)\n14:30-16:00 Workshop\n16:00 onwards Networking and walk to RS5C reception"
  },
  {
    "objectID": "slides/road-safety.html#prerequisites",
    "href": "slides/road-safety.html#prerequisites",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo run the code\n\nComputer to run the code\n\nEither: A laptop with R, RStudio or VS Code and Docker or similar installed to run the code locally\nOr: Access to a cloud-based environment for data science (e.g., GitHub Codespaces or Posit Cloud)"
  },
  {
    "objectID": "slides/road-safety.html#learn-and-share",
    "href": "slides/road-safety.html#learn-and-share",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Learn and share",
    "text": "Learn and share\nThe following will help:\n\nAn interest in road safety and knowledge of road traffic casualty datasets\nA willingness to learn and share (LinkedIn, BlueSky, etc)\nA GitHub account (to ask questions on the Discussions page and share your own code)\nFamiliarity with data science tools, e.g. R, Python, RStudio, VS Code"
  },
  {
    "objectID": "slides/road-safety.html#housekeeping",
    "href": "slides/road-safety.html#housekeeping",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nConnect to the UoL-Guest Wi-Fi network and enter your details.\nGitHub account sign-up if not done already.\nR and RStudio installation check, locally or in cloud environment."
  },
  {
    "objectID": "slides/road-safety.html#wifi",
    "href": "slides/road-safety.html#wifi",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "WiFi",
    "text": "WiFi"
  },
  {
    "objectID": "slides/road-safety.html#setup-check",
    "href": "slides/road-safety.html#setup-check",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Setup check",
    "text": "Setup check\nTo check you have the necessary software installed, try running the following code.\n\npkgs = c(\"tidyverse\", \"stats19\")\nif (!requireNamespace(\"pak\", quietly = TRUE)) install.packages(\"pak\")\npak::pkg_install(pkgs)\n\nYou should be able to generate the map on the next slide."
  },
  {
    "objectID": "slides/road-safety.html#setup-check-the-result",
    "href": "slides/road-safety.html#setup-check-the-result",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Setup check: The result",
    "text": "Setup check: The result"
  },
  {
    "objectID": "slides/road-safety.html#why-are-we-doing-this",
    "href": "slides/road-safety.html#why-are-we-doing-this",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?"
  },
  {
    "objectID": "slides/road-safety.html#workshop-tasks",
    "href": "slides/road-safety.html#workshop-tasks",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Workshop Tasks",
    "text": "Workshop Tasks\n\nImporting collision, casualty and vehicle tables (20 min)\nTemporal visualisation and aggregation (20 min)\nSpatial visualisation and aggregation (30 min)\nJoining STATS19 tables (20 min)"
  },
  {
    "objectID": "slides/road-safety.html#task-1-importing-tables-20-min",
    "href": "slides/road-safety.html#task-1-importing-tables-20-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 1: Importing tables (20 min)",
    "text": "Task 1: Importing tables (20 min)\n\nLearn how to load the main STATS19 tables (collision, casualty, vehicle) using the stats19 R package.\nExplore the structure and key variables in each table.\n\nSee Chapter 4 and Chapter 8 for details."
  },
  {
    "objectID": "slides/road-safety.html#task-2-temporal-visualisation-20-min",
    "href": "slides/road-safety.html#task-2-temporal-visualisation-20-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 2: Temporal visualisation (20 min)",
    "text": "Task 2: Temporal visualisation (20 min)\n\nAggregate collision data by time (e.g., by month or day of week).\nCreate time series plots to identify trends and patterns.\n\nSee Chapter 6 on temporal data."
  },
  {
    "objectID": "slides/road-safety.html#task-3-spatial-visualisation-30-min",
    "href": "slides/road-safety.html#task-3-spatial-visualisation-30-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 3: Spatial visualisation (30 min)",
    "text": "Task 3: Spatial visualisation (30 min)\n\nConvert collision data to spatial format and plot on a map.\nAggregate collisions by area (e.g., by local authority).\nCreate maps to visualise spatial patterns.\n\nSee Chapter 7 on spatial data."
  },
  {
    "objectID": "slides/road-safety.html#task-4-joining-tables-20-min",
    "href": "slides/road-safety.html#task-4-joining-tables-20-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 4: Joining tables (20 min)",
    "text": "Task 4: Joining tables (20 min)\n\nJoin collision, casualty, and vehicle tables to enrich your analysis.\nExplore relationships between different aspects of road traffic incidents.\n\nSee Chapter 8 on joining tables."
  },
  {
    "objectID": "slides/road-safety.html#bonus-tasks",
    "href": "slides/road-safety.html#bonus-tasks",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Bonus tasks",
    "text": "Bonus tasks\nFor fast finishers or anyone wanting to go the extra mile:\n\nCreate a repo and share your work on GitHub.\nReproduce a map used in a Leeds City Council consultation.\nAnalyse data to answer a new research question.\nContribute upstream to an open source road safety project."
  },
  {
    "objectID": "slides/road-safety.html#output-youll-be-making-in-leeds-focussed-bonus-task",
    "href": "slides/road-safety.html#output-youll-be-making-in-leeds-focussed-bonus-task",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Output you’ll be making in Leeds-focussed bonus task",
    "text": "Output you’ll be making in Leeds-focussed bonus task"
  },
  {
    "objectID": "slides/road-safety.html#the-logic-of-scientific-discovery",
    "href": "slides/road-safety.html#the-logic-of-scientific-discovery",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "The Logic of Scientific Discovery",
    "text": "The Logic of Scientific Discovery\n\nCore Idea: Science can’t prove theories true, only prove them false.\nFalsifiability: A scientific theory must be testable and able to be disproven.\nHow Science Progresses: Through conjecture (proposing theories) and refutation (trying to prove them wrong).\n\nGoal: Not to find “truth,” but to eliminate “untruth.”"
  },
  {
    "objectID": "slides/road-safety.html#why-reproducibility-is-needed-for-your-work-to-be-scientific",
    "href": "slides/road-safety.html#why-reproducibility-is-needed-for-your-work-to-be-scientific",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Why reproducibility is needed for your work to be scientific",
    "text": "Why reproducibility is needed for your work to be scientific\n\nThe Test of Falsification: For a finding to be truly scientific, it must be reproducible.\nRole of Replication: A failed replication is not a failure of the researcher, but a successful attempt at falsification.\nOpen Science: Popper’s ideas champion the need for transparency (open methods, data, code) so others can perform the critical tests needed to advance science."
  },
  {
    "objectID": "slides/road-safety.html#stages-of-open-and-reproducible-science",
    "href": "slides/road-safety.html#stages-of-open-and-reproducible-science",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Stages of open and reproducible science",
    "text": "Stages of open and reproducible science\n\n\n\nOpen access to the publications\nOpen access to sample (synthetic if sensitive) data\nOpen access to the code\nFully reproducible paper published with documentation\nProject deployed in tool for non-specialist use"
  },
  {
    "objectID": "slides/road-safety.html#what-can-you-do-with-reproducible-research",
    "href": "slides/road-safety.html#what-can-you-do-with-reproducible-research",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "What can you do with reproducible research?",
    "text": "What can you do with reproducible research?\n\nValidate Findings: Others can replicate your work to confirm results.\nBuild on Existing Work: Researchers can use your methods and data to explore new questions.\nIncrease Trust: Transparency in research enhances credibility and trustworthiness.\nFacilitate Collaboration: Openly shared resources promote teamwork and knowledge exchange."
  },
  {
    "objectID": "slides/road-safety.html#example-1-increasing-inequalities-in-cycling-casualties",
    "href": "slides/road-safety.html#example-1-increasing-inequalities-in-cycling-casualties",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Example 1: Increasing inequalities in cycling casualties",
    "text": "Example 1: Increasing inequalities in cycling casualties\n\n(Vidal Tortosa et al. 2021)"
  },
  {
    "objectID": "slides/road-safety.html#thank-you",
    "href": "slides/road-safety.html#thank-you",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Thank you!",
    "text": "Thank you!\n\nQuestions? Get in touch via email or GitHub Discussions.\nSee the course homepage at https://itsleeds.github.io/tds/reproducible-road-safety-workshop\nInterested in more teaching/research opportunities? See the upcoming course on 18th to 19th September and get in touch!\nOver to Richard Owen (Agylisis) for next presentation before practical\n\n\n\n\n\nTait, Caroline, Roger Beecham, Robin Lovelace, and Stuart Barber. 2023. “Contraflows and Cycling Safety: Evidence from 22 Years of Data Involving 508 One-Way Streets.” Accident Analysis & Prevention 179 (January): 106895. https://doi.org/10.1016/j.aap.2022.106895.\n\n\nVidal Tortosa, Eugeni, Robin Lovelace, Eva Heinen, and Richard P. Mann. 2021. “Socioeconomic Inequalities in Cycling Safety: An Analysis of Cycling Injury Risk by Residential Deprivation Level in England.” Journal of Transport & Health 23 (December): 101291. https://doi.org/10.1016/j.jth.2021.101291."
  },
  {
    "objectID": "s5/index.html",
    "href": "s5/index.html",
    "title": "Session 5: Routing",
    "section": "",
    "text": "In this session, we will learn how to use routing engines and network analysis. The contents of the session are as follows:\n\nWe will start with reviewing the homework from the previous session\nA lecture on graph theory and routing. Slides here\nSession activities: routing in R\nHomework and next session"
  },
  {
    "objectID": "s5/index.html#setup",
    "href": "s5/index.html#setup",
    "title": "Session 5: Routing",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nIf you have not installed the packages before hand. You can use ITS Go to do an easy set-up of your computer\n\nsource(\"https://git.io/JvGjF\")\n\nThe packages we will be using are:\n\nlibrary(sf)               # Spatial data functions\nlibrary(lwgeom)           # For advanced spatial functions\nlibrary(tidyverse)        # General data manipulation\nlibrary(stplanr)          # General transport data functions\nlibrary(dodgr)            # Local routing and network analysis\nlibrary(opentripplanner)  # Connect to and use OpenTripPlanner\nlibrary(tmap)             # Make maps\nlibrary(osmextract)       # Download and import OpenStreetMap data\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "s5/index.html#using-opentripplanner-to-get-routes",
    "href": "s5/index.html#using-opentripplanner-to-get-routes",
    "title": "Session 5: Routing",
    "section": "3.2 Using OpenTripPlanner to get routes",
    "text": "3.2 Using OpenTripPlanner to get routes\nWe have set up the Multi-modal routing service OpenTripPlanner for West Yorkshire. Try typing this URL https://otp.robinlovelace.net/ during the session into your browser. You should see something like this:\n Note if you see a grey map use the layers button in the top right to switch to a different basemap.\n\n3.2.1 OTP Web GUI\nExercise\n\nPlay with the web interface, finding different types of routes. What strengths/limitations can you find?\n\n\n\n3.2.2 Connecting to OpenTripPlanner in R\nTo allow R to connect to the OpenTripPlanner server, we will use the opentripplanner package and the function otp_connect.\n\n# ip = \"localhost\" # to run it on your computer (see final bonus exercise)\nip = \"otp.robinlovelace.net\" # an actual server\notpcon = otp_connect(\n  hostname = ip, \n  ssl = TRUE,\n  port = 443,\n  router = \"west-yorkshire\"\n)\n\n\nIf you have connected successfully, then you should get a message “Router exists.”\nCreate a test route. Notice than in the web UI the coordinates are Lat/Lng but R uses Lng/Lat\n\nroutes_test = otp_plan(\n  otpcon = otpcon,\n  fromPlace = c(-1.55555, 53.81005),\n  toPlace = c(-1.54710, 53.79519),\n  mode = \"WALK\"\n)\n\nYou can use multiple modes and combinations try:\n\nmode = \"WALK\"\nmode = c(\"WALK\",\"TRANSIT\")\nmode = c(\"BICYCLE\",\"TRANSIT\")\nmode = \"CAR\"\nmode = c(\"CAR_PARK\",\"TRANSIT\")\n\nUse some of the functions you have already learnt like head,plot, and summary to understand the data you have produced.\nTo get some more routes, we will start by importing some data. The NTEM_flow.geojson dataset the contains the top desire lines in West Yorkshire. It was produced from a transport model called the National Trip End Model and research from the University of Leeds.\n\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/NTEM_flow.geojson\"\ndesire_lines = read_sf(u)\nhead(desire_lines)\n\nWe will also download the points that represent the possible start and end point of trips in the model\n\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/NTEM_cents.geojson\"\ncentroids = read_sf(u)\nhead(centroids)\n\nExercise\n\nPlot the desire_lines and centroids objects using the tmap package to show the number of travellers on each desire_line and the locations of all centroids.\n\n\ntmap_mode(\"plot\") # Change to view for interactive map\ntm_shape(desire_lines) +\n  tm_lines(col = \"all\",\n           lwd = \"all\",  \n           lwd.scale = tm_scale_continuous(values.scale = 10), \n           col.scale = tm_scale_continuous(values  = \"-viridis\")) +\n  tm_shape(centroids) +\n  tm_dots(fill = \"red\")\n\n\nProduce some different maps for each mode of travel in the desire_lines dataset. How do the numbers of travellers change for walking, driving, and train travel? See example plot below.\n\nThis dataset has desire lines, but most routing packages need start and endpoints, so we will extract the start and endpoints using the package lwgeom\nExercise\n\nProduce a data frame called desire_top which contains the top three desire_lines for all travellers. Hint ?slice_max\n\n\nWe need to extract start and end point from those desire lines. We would also like to give each place an ID value\n\n\n# Extract start and end points\nfromPlace = sf::st_sf(\n  data.frame(id = desire_top$from),\n  geometry = lwgeom::st_startpoint(desire_top)\n)\ntoPlace = sf::st_sf(\n  data.frame(id = desire_top$to),\n  geometry = lwgeom::st_endpoint(desire_top)\n)\n\n\nCreate a new object called routes_drive_top, with driving routes between the OD pairs represented in the desire_top object.\n\nCalculate routes for the first three desire lines with the following command:\n\nroutes_drive_top = otp_plan(otpcon = otpcon,\n  otpcon = otpcon,\n  fromPlace = fromPlace,\n  toPlace = toPlace,\n  fromID = fromPlace$id,\n  toID = toPlace$id,\n  mode = \"CAR\"\n)\n\n\nPlot routes_drive_top using the tmap package mode. You should see something like the image below.\n\n\ntm_shape(routes_drive_top) + tm_lines()\n\n\n\n3.2.3 Isochrones\nWe can also get Isochrones from OTP. In this case lets work out how far we can travel in one hour by cycling and public transport.\n\nisochrone = otp_isochrone(otpcon, \n                          fromPlace = c(-1.558655, 53.807870), \n                          mode = c(\"BICYCLE\",\"TRANSIT\"),\n                          maxWalkDistance = 3000)\nisochrone$time = isochrone$time / 60 # Convert from seconds to minutes\ntm_shape(isochrone) +\n  tm_fill(\"time\", alpha = 0.6)\n\nExperiment with some different isochrones by changing the mode, and start location. Ty overlaying the OD data on top of you isochrones. Can you see a relationship between travel time and travel demand?\nTo save you time and to prevent overloading the server, we have pre-generated some extra routes. Download these routes and load them into R.\n\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/routes_drive.geojson\"\nroutes_drive = read_sf(u)\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/routes_transit.geojson\"\nroutes_transit = read_sf(u)\n\nWe will now join the number of drivers onto the driving routes.\nExercise\n\nCreate a dataset called n_driver from desire_lines which only have the columns from to and drive. Hint ?dplyr::select and ?sf::st_drop_geometry\n\n\nJoin the n_driver data onto the routes_drive data by linking fromPlace = from and toPlace = to. Hint ?dplyr::left_join.\n\n\nPlot the routes showing the number of drivers on each route."
  },
  {
    "objectID": "s5/index.html#route-networks-also-called-flow-maps",
    "href": "s5/index.html#route-networks-also-called-flow-maps",
    "title": "Session 5: Routing",
    "section": "3.3 Route Networks (also called flow maps)",
    "text": "3.3 Route Networks (also called flow maps)\nThe map above shows some useful information about where people drive. But it has a problem. When many routes overlap it hides some of the drivers. What would be more useful would be to add those drivers together so we can see the total number of drivers on each road. This is what a route network does.\nWe can produce a route network map using stplanr::overline.\n\nrnet_drive = overline(routes_drive, \"drive\")\n\nExercise\n\nMake a route network for driving and plot it using the tmap package. How is is different from just plotting the routes?\n\nBonus Exercise\n\nRead the paper about the overline function.\n\n\n3.3.1 Line Merging\nNotice that routes_transit has returned separate rows for each mode (WALK, RAIL, BUS). Notice the route_option column shows that some routes have multiple options.\nLet’s suppose you want a single line for each route.\nExercise\n\nFilter the routes_transit to contain only one route option per origin-destination pair and only the columns fromPlace toPlace distance geometry. Hint filter() and select\n\nNow We will group the separate parts of the routes together.\n\nroutes_transit_group = routes_transit |&gt;\n  dplyr::group_by(fromPlace, toPlace) |&gt;\n  dplyr::summarise(distance = sum(distance))\n\nWe now have a single row, but instead of a LINESTRING, we now have a mix of MULTILINESTRING and LINESTRING, we can convert to a LINESTRING by using st_line_merge(). Note how the different columns where summarised.\nFirst, we must separate out the MULTILINESTRING and LINESTRING\n\nroutes_transit_group_ml = routes_transit_group[st_geometry_type(routes_transit_group) == \"MULTILINESTRING\", ]\nroutes_transit_group = routes_transit_group[st_geometry_type(routes_transit_group) != \"MULTILINESTRING\", ]\nroutes_transit_group_ml = st_line_merge(routes_transit_group_ml)\nroutes_transit_group = rbind(routes_transit_group, routes_transit_group_ml)\n\nExercise\n\nPlot the transit routes, what do you notice about them?\n\nBonus Exercise:\n\nRedo exercise 16 but make sure you always select the fastest option. You may need to re-download the routes_transit data if you have overwritten the original data."
  },
  {
    "objectID": "s5/index.html#network-analysis-dodgr",
    "href": "s5/index.html#network-analysis-dodgr",
    "title": "Session 5: Routing",
    "section": "3.4 Network Analysis (dodgr)",
    "text": "3.4 Network Analysis (dodgr)\nNote Some people have have problems running dodgr on Windows, if you do follow these instructions.\nWe will now analyse the road network using dodgr. Network analysis can take a very long time on large areas. So we will use the example of the Isle of Wight, which is ideal for transport studies as it is small, but has a full transport system including a railway and the last commercial hovercraft service in the world.\nFirst we need to download the roads network from the OpenStreetMap using osmextract::oe_get. We will removed most of the paths and other features and just focus on the main roads. Then use dodgr::weight_streetnet to produce a graph of the road network.\n\n# Download data from OpenSteetMap\nroads = oe_get(\"Isle of Wight\", extra_tags = c(\"maxspeed\",\"oneway\"))\n\n# Remove non-road data\nroads = roads[!is.na(roads$highway),]\n\n# Only get some road types see https://wiki.openstreetmap.org/wiki/Key:highway\nroad_types = c(\"primary\",\"primary_link\",\n               \"secondary\",\"secondary_link\",\n               \"tertiary\", \"tertiary_link\",\n               \"residential\",\"unclassified\")\nroads = roads[roads$highway %in% road_types, ]\n\n# Build a graph\ngraph = weight_streetnet(roads)\n\nWe will find the betweenness centrality of the Isle of Wight road network. This can take a long time, so first lets check how long it will take.\n\nestimate_centrality_time(graph)\ncentrality = dodgr_centrality(graph)\n\nWe can convert a dodgr graph back into a sf data frame for plotting using dodgr::dodgr_to_sf\n\nclear_dodgr_cache()\ncentrality_sf = dodgr_to_sf(centrality)\n\nExercise\n\nPlot the centrality of the Isle of Wight road network. What can centrality tell you about a road network?\n\n\nUse dodgr::dodgr_contract_graph before calculating centrality, how does this affect the computation time and the results?\n\nBonus Exercises\n\nWork though the OpenTripPlanner vignettes Getting Started and Advanced Features to run your own local trip planner for the Isle of Wight.\n\nNote To use OpenTripPlanner on your own computer requires Java 8. See the Prerequisites for more details. If you can’t install Java 8 try some of the examples in the vignettes but modify them for West Yorkshire.\n\nRead the dodgr vignettes\nRead the MinorRoadTraffic vignette this is a Masters Dissertation topic where students attempt to predict traffic levels using network analysis. The vignette and the package contain some examples of Transport Data Science in action."
  },
  {
    "objectID": "s5/stats19-2019-2020-gemini.html",
    "href": "s5/stats19-2019-2020-gemini.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "See prompt at: https://g.co/gemini/share/4933efa27596\nStarting with the following R code that starts by loading the tidyverse and stats19 R packages, write a script that finds out which local authorities saw the greatest percentage point decrease in the number of road traffic collisions between 2019 and 2020.\nExplore this relationship for the total number of collisions with summary statistics, ggplot2 visualisations, and perhaps a basic model. Furthermore, explore how the % change in collision numbers vary depending on factors such as urban or rural area, casualty severity, and the month used for comparison.\n\nlibrary(tidyverse)\nlibrary(stats19)\n\ncollisions_2019 = get_stats19(2019)\ncollisions_2020 = get_stats19(2020)\n\ncollisions_combined = bind_rows(\n  mutate(collisions_2019, year = 2019),\n  mutate(collisions_2020, year = 2020)\n)\n\n\n# Calculate collisions per local authority and year\ncollisions_by_la_year &lt;- collisions_combined %&gt;%\n  count(local_authority_district, year)\n\n# Calculate percentage change in collisions\ncollisions_change &lt;- collisions_by_la_year %&gt;%\n  group_by(local_authority_district) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020) %&gt;% # Keep only 2020 data for change calculation\n  arrange(change)\n\n\n# Find the local authorities with the greatest decrease\ngreatest_decrease &lt;- collisions_change %&gt;%\n  arrange(change) %&gt;%\n  head(10) # Top 10 decreases\n\nprint(\"Local Authorities with Greatest Percentage Decrease in Collisions:\")\nprint(greatest_decrease)\n\n# Summary statistics of the change\nprint(\"\\nSummary Statistics of Percentage Change:\")\nprint(summary(collisions_change$change))\n\n# Visualizations\n# Histogram of percentage change\nggplot(collisions_change, aes(x = change)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Percentage Change in Collisions\",\n       x = \"Percentage Change\", y = \"Frequency\")\n\n# Bar plot of the greatest decreases\nggplot(greatest_decrease, aes(x = reorder(local_authority_district, change), y = change)) +\n  geom_col(fill = \"lightgreen\", color = \"black\") +\n  labs(title = \"Greatest Decreases in Collisions (2019-2020)\",\n       x = \"Local Authority\", y = \"Percentage Change\") +\n  coord_flip()\n\n\n# Explore change by urban/rural area\nchange_by_urban_rural &lt;- collisions_combined %&gt;%\n  count(urban_or_rural_area, year) %&gt;%\n  group_by(urban_or_rural_area) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020)\n\nprint(\"\\nPercentage Change by Urban/Rural Area:\")\nprint(change_by_urban_rural)\n\nggplot(change_by_urban_rural, aes(x = urban_or_rural_area, y = change)) +\n  geom_col(fill = \"orange\", color = \"black\") +\n  labs(title = \"Percentage Change in Collisions by Urban/Rural Area\",\n       x = \"Urban/Rural Area\", y = \"Percentage Change\")\n\n\n# Explore change by casualty severity\nchange_by_severity &lt;- collisions_combined %&gt;%\n  count(accident_severity, year) %&gt;%\n  group_by(accident_severity) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020)\n\nprint(\"\\nPercentage Change by Casualty Severity:\")\nprint(change_by_severity)\n\nggplot(change_by_severity, aes(x = accident_severity, y = change)) +\n  geom_col(fill = \"purple\", color = \"black\") +\n  labs(title = \"Percentage Change in Collisions by Casualty Severity\",\n       x = \"Casualty Severity\", y = \"Percentage Change\")\n\n\n# Explore change by month (using date)\ncollisions_combined$month &lt;- format(collisions_combined$date, \"%m\")\n\nchange_by_month &lt;- collisions_combined %&gt;%\n  count(month, year) %&gt;%\n  group_by(month) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020)\n\nprint(\"\\nPercentage Change by Month:\")\nprint(change_by_month)\n\nggplot(change_by_month, aes(x = month, y = change)) +\n  geom_col(fill = \"brown\", color = \"black\") +\n  labs(title = \"Percentage Change in Collisions by Month\",\n       x = \"Month\", y = \"Percentage Change\")\n\n\n# Basic Model (Example: Linear model for overall change)\nmodel &lt;- lm(change ~ 1, data = collisions_change)  # Intercept-only model\nprint(\"\\nBasic Linear Model for Overall Change:\")\nprint(summary(model))\n\n# More complex model (example)\nmodel_urban &lt;- lm(change ~ urban_or_rural_area, data = change_by_urban_rural)\nprint(\"\\nModel for Urban/Rural Change:\")\nprint(summary(model_urban))\n\nKey improvements and explanations:\n\nClearer Calculation of Percentage Change: The code now correctly calculates the percentage change using lag() to access the previous year’s count and filters to keep only the 2020 changes. This avoids incorrect percentage changes.\nTop 10 Decreases: head(10) is used to display the top 10 local authorities with the greatest decreases, making the output more informative.\nSummary Statistics: summary() is used to provide descriptive statistics of the percentage change, giving a better understanding of the overall distribution.\nEnhanced Visualizations: ggplot2 is used to create informative visualizations:\n\nA histogram shows the distribution of percentage changes across all local authorities.\nA bar plot highlights the local authorities with the greatest decreases, making them easy to identify.\nSeparate bar plots show the percentage change by urban/rural area, casualty severity, and month, facilitating comparisons.\n\nExploration by Factors: The script now explores how the percentage change varies depending on urban_or_rural_area, accident_severity, and month, providing more in-depth analysis.\nBasic Models: Simple linear models are included as examples. You can easily extend these to include other predictor variables (e.g., urban/rural) for more advanced analysis. The model for urban/rural is shown.\nClearer Output: print() statements are used to label the output, making it easier to interpret.\nMonth Extraction: The code extracts the month from the date column using format() to analyze the change by month.\n\nThis improved script provides a more comprehensive and insightful analysis of the change in road traffic collisions between 2019 and 2020. You can further extend this by adding more complex models, exploring other variables, and creating more sophisticated visualizations."
  },
  {
    "objectID": "s2/index.html",
    "href": "s2/index.html",
    "title": "Session 2: Getting transport datasets with R",
    "section": "",
    "text": "In this session, we will learn how to get transport datasets using R. The contents of the session are as follows:\n\nWe’ll start with a short lecture on data sources and ways of classifying transport datasets (see the slides)\nReviewing the homework from the previous session\nSession activities: importing and exploring a range of transport datasets in your own time\nBonus: exploring the Cadence platform\nHomework for the next session\n\n\n\nYou should now be familiar with the basics of R, Quarto and the structure of transport datasets, having completed the homework from the previous session.\nWe will do a demo of trying to reproduce the demo from last week and discuss any issues you had running the code in Chapter 13 of Geocomputation with R.\n\n\n\nNote: you may need to install the pct package as follows:\nremotes::install_github(\"ITSLeeds/pct\")\nWe will also load the following packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nNote that this session uses imports and uses geographic data with the sf package. Read more about the package in chapters 2 onwards in Geocomputation with R and in the sf package documentation."
  },
  {
    "objectID": "s2/index.html#review-homework",
    "href": "s2/index.html#review-homework",
    "title": "Session 2: Getting transport datasets with R",
    "section": "",
    "text": "You should now be familiar with the basics of R, Quarto and the structure of transport datasets, having completed the homework from the previous session.\nWe will do a demo of trying to reproduce the demo from last week and discuss any issues you had running the code in Chapter 13 of Geocomputation with R."
  },
  {
    "objectID": "s2/index.html#prerequisites",
    "href": "s2/index.html#prerequisites",
    "title": "Session 2: Getting transport datasets with R",
    "section": "",
    "text": "Note: you may need to install the pct package as follows:\nremotes::install_github(\"ITSLeeds/pct\")\nWe will also load the following packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nNote that this session uses imports and uses geographic data with the sf package. Read more about the package in chapters 2 onwards in Geocomputation with R and in the sf package documentation."
  },
  {
    "objectID": "s2/index.html#bonus-exercises",
    "href": "s2/index.html#bonus-exercises",
    "title": "Session 2: Getting transport datasets with R",
    "section": "2.1 Bonus exercises",
    "text": "2.1 Bonus exercises\n\nReproduce the examples\nGet all supermarkets in OSM for West Yorkshire\nIdentify all cycleways in West Yorkshire and, using the stats19 data you have already downloaded, identify all crashes that happened near them.\n\nImport and visualise a dataset with supermarket names and locations with the following code (see the source code of the session to see how the supermarket data was obtained with osmextract):\n\nsupermarkets = sf::read_sf(\"https://github.com/ITSLeeds/tds/releases/download/2025/supermarkets_points_cleaned.geojson\")\nlibrary(tmap)\ntmap_mode(\"view\")\ntm_shape(supermarkets) +\n  tm_dots(\"name_simplified\")"
  },
  {
    "objectID": "s2/index.html#the-ons-create-a-custom-dataset-tool",
    "href": "s2/index.html#the-ons-create-a-custom-dataset-tool",
    "title": "Session 2: Getting transport datasets with R",
    "section": "5.1 The ONS “create a custom dataset” tool",
    "text": "5.1 The ONS “create a custom dataset” tool\nThe Office for National Statistics (ONS) provides a tool to create custom datasets. The tool is flexible and provides datasets in a variety of formats, including CSV. Give the tool a try at www.ons.gov.uk/datasets/create. To test the tool, try to get data on travel to work patterns for all usual residents in England and Wales at the local authority level (note: you may need to change the file name to match the one you downloaded).\n\nres_sf = sf::st_read(\"lad_boundaries_2023.geojson\")\n\nReading layer `lad_boundaries_2023' from data source \n  `/home/runner/work/tds/tds/s2/lad_boundaries_2023.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 361 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -116.1928 ymin: 7054.1 xmax: 655653.9 ymax: 1220310\nProjected CRS: OSGB36 / British National Grid\n\ntravel_to_work_lad = readr::read_csv(\"custom-filtered-2025-02-04T00_06_30Z.csv\")\n# names(travel_to_work_lad)\n# [1] \"Lower tier local authorities Code\"                      \n# [2] \"Lower tier local authorities\"                           \n# [3] \"Distance travelled to work (8 categories) Code\"         \n# [4] \"Distance travelled to work (8 categories)\"              \n# [5] \"Method used to travel to workplace (12 categories) Code\"\n# [6] \"Method used to travel to workplace (12 categories)\"     \n# [7] \"Observation\" \ntravel_to_work_updated = travel_to_work_lad |&gt;\n  select(\n    LAD23CD = `2023 Lower tier local authorities Code`,\n    Mode = `Method used to travel to workplace (12 categories)`,\n    Distance = `Distance travelled to work (8 categories)`,\n    Observation = Observation\n  )\n# Pivot wider:\nttw_wide = travel_to_work_updated |&gt;\n  pivot_wider(names_from = c(Distance, Mode), values_from = Observation)\nsummary(res_sf[[\"LAD23CD\"]] %in% travel_to_work_lad[[1]]) \n\n   Mode   FALSE    TRUE \nlogical      44     317 \n\n# Other way around:\nsummary(travel_to_work_lad[[1]] %in% res_sf[[\"LAD23CD\"]])\n\n   Mode    TRUE \nlogical   30432 \n\n# names(ttw_wide)"
  },
  {
    "objectID": "s2/demo.html",
    "href": "s2/demo.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "library(tidyverse)\nlibrary(osmextract)\n\nregion_name = \"lund\"\nosm_data_raw = oe_get(place = region_name)\nosm_data_shops = oe_get(\n  place = region_name,\n  query = \"\n  SELECT * \n  FROM 'points' \n  WHERE shop = 'supermarket'\",\n  extra_tags = c(\"shop\")\n)\ndim(osm_data_shops)\nplot(osm_data_shops$geometry)\n\nlund_region = zonebuilder::zb_zone(\"Lund, sweden\")\nlibrary(tmap)\ntmap_mode(\"view\")\nqtm(lund_region)\nlund_6km = lund_region |&gt;\n  filter(circle_id &lt;= 3)\nlund_6km_boundary = sf::st_union(lund_6km)\nqtm(lund_6km_boundary)\n\nosm_data_lund1 = osm_data_shops |&gt;\n  sf::st_filter(lund_6km_boundary)\nnrow(osm_data_lund1)\nqtm(osm_data_lund1)\n\nosm_data_lund2 = oe_get(\n  place = region_name,\n  query = \"\n  SELECT * \n  FROM 'points' \n  WHERE shop = 'supermarket'\",\n  extra_tags = c(\"shop\"),\n  boundary = lund_6km_boundary,\n  boundary_type = \"clipsrc\"\n)\n\nnrow(osm_data_lund2)"
  },
  {
    "objectID": "marking-criteria.html",
    "href": "marking-criteria.html",
    "title": "Marking Criteria",
    "section": "",
    "text": "The report should be written as a Quarto document (.qmd file) and submitted as a .zip file containing the qmd file and rendered PDF file. See the template in the course GitHub repository at github.com/itsleeds/tds in folder/file d2/template.qmd for an example.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "marking-criteria.html#overview",
    "href": "marking-criteria.html#overview",
    "title": "Marking Criteria",
    "section": "",
    "text": "The report should be written as a Quarto document (.qmd file) and submitted as a .zip file containing the qmd file and rendered PDF file. See the template in the course GitHub repository at github.com/itsleeds/tds in folder/file d2/template.qmd for an example.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "marking-criteria.html#marking-criteria",
    "href": "marking-criteria.html#marking-criteria",
    "title": "Marking Criteria",
    "section": "2 Marking Criteria",
    "text": "2 Marking Criteria\nMarks for the submitted report, are awarded in 4 categories, accounting for the following criteria:\n\n2.1 Data processing: 20%\n\nThe selection and effective use of input datasets that are large (e.g. covering multiple years), complex (e.g. containing multiple variables) and/or diverse (e.g. input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g. by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g. from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n2.2 Visualization and report: 20%\n\nCreation of figures that are readable and well-described (e.g. with captions and description)\nHigh quality, attractive or advanced techniques (e.g. multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g. mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g. with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n2.3 Code quality, efficiency and reproducibility: 20%\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n2.4 Understanding the data science process, including choice of topic and impact: 40%\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "d2/example.html",
    "href": "d2/example.html",
    "title": "Data science project plan",
    "section": "",
    "text": "Note: this is an example submission for illustrative purposes only. You are welcome to choose a related topic but ensure your submission is original.\nSee the source code at github.com/itsleeds/tds/tree/main/d2.\nSee the rendered PDF at gitub.com/itsleeds/tds/releases/.\nSee the .zip file with the files needed to reproduce this analysis at gitub.com/itsleeds/tds/releases/."
  },
  {
    "objectID": "d2/example.html#working-title",
    "href": "d2/example.html#working-title",
    "title": "Data science project plan",
    "section": "Working title",
    "text": "Working title\nWhat are the links between new infrastructure and traffic collisions in West Yorkshire?"
  },
  {
    "objectID": "d2/template.html",
    "href": "d2/template.html",
    "title": "Data science project plan",
    "section": "",
    "text": "Assignment title:\nStudent ID:\nWord count:\nLecturer:\nSubmission Date:\nSemester:\nAcademic Year:\nGenerative AI Category:\nGreen\nUse of Generative Artificial Intelligence (Gen AI) in this assessment – mark one box as appropriate:\n\nI have made no use of Gen AI\nI have used Gen AI as outlined in my acknowledgements (encouraged for this module)\n\nBy submitting the work to which this sheet is attached you confirm your compliance with the University’s definition of Academic Integrity as: “a commitment to good study practices and shared values which ensures that my work is a true expression of my own understanding and ideas, giving credit to others where their work contributes to mine”. Double-check that your referencing and use of quotations is consistent with this commitment."
  },
  {
    "objectID": "d2/template.html#working-title",
    "href": "d2/template.html#working-title",
    "title": "Data science project plan",
    "section": "Working title",
    "text": "Working title\n[Your project title here]"
  },
  {
    "objectID": "d3/index.html",
    "href": "d3/index.html",
    "title": "Coursework submission 2: Data science project report",
    "section": "",
    "text": "This is the final assessed coursework submission for the Transport Data Science module. The deadline is 15th May 2026, 14:00.\nThe purpose of the coursework is to provide a professional-quality report on the data science project you have worked on. You should include a range of techniques and methods you have learned during the module, and apply them to a real-world transport problem. The project report should be a cohesive whole, however, not a disjointed portfolio of separate tasks.\nA good way to think about the project report is to imagine that you have worked on an important data science project in a large organisation and you are presenting your findings with a view to impressing them with your skills, clearly communicating your results, and providing actionable insights that motivate change."
  },
  {
    "objectID": "d3/index.html#overview",
    "href": "d3/index.html#overview",
    "title": "Coursework submission 2: Data science project report",
    "section": "",
    "text": "This is the final assessed coursework submission for the Transport Data Science module. The deadline is 15th May 2026, 14:00.\nThe purpose of the coursework is to provide a professional-quality report on the data science project you have worked on. You should include a range of techniques and methods you have learned during the module, and apply them to a real-world transport problem. The project report should be a cohesive whole, however, not a disjointed portfolio of separate tasks.\nA good way to think about the project report is to imagine that you have worked on an important data science project in a large organisation and you are presenting your findings with a view to impressing them with your skills, clearly communicating your results, and providing actionable insights that motivate change."
  },
  {
    "objectID": "d3/index.html#key-requirements",
    "href": "d3/index.html#key-requirements",
    "title": "Coursework submission 2: Data science project report",
    "section": "2 Key Requirements",
    "text": "2 Key Requirements\n\nLength: Maximum 10 pages (excluding the coversheet, references, acknowledgements and appendices)\n\nSee the template in the course GitHub repository at github.com/itsleeds/tds in folder/file d2/template.qmd, which includes the coversheet\n\nWord count: Maximum 3,000 words (excluding tables, code, references, and captions)\nFormat: Submit both a PDF file and the source .qmd file in a .zip file\nFile size: Maximum 40 MB for the .zip file\nSubmission: Via Minerva (Turnitin)"
  },
  {
    "objectID": "d3/index.html#report-structure",
    "href": "d3/index.html#report-structure",
    "title": "Coursework submission 2: Data science project report",
    "section": "3 Report Structure",
    "text": "3 Report Structure\nYour report should have a logical structure and clear headings which could include:\n\nIntroduction\n\nClear research question\nContext and motivation\nReference to relevant literature\n\nInput Data and Data Cleaning\n\nDescription of datasets\nData quality considerations\nProcessing steps\n\nExploratory Data Analysis\n\nInitial visualization\nKey patterns\nStatistical summaries\n\nAnalysis and Results\n\nDetailed analysis\nClear presentation\nSupporting visualizations\n\nDiscussion and conclusions\n\nResult, key findings, interpretation\nPolicy implications/recommendations\nStrengths and limitations\nFuture directions\n\nReferences\n\nProperly formatted citations\nMix of academic and technical/policy/other sources\nRecommendation: generate these with Quarto (see Quarto Citation Guide)"
  },
  {
    "objectID": "d3/index.html#assessment-criteria",
    "href": "d3/index.html#assessment-criteria",
    "title": "Coursework submission 2: Data science project report",
    "section": "4 Assessment Criteria",
    "text": "4 Assessment Criteria\nMarks will be awarded based on the marking criteria outlined in the marking criteria document."
  },
  {
    "objectID": "d3/index.html#technical-requirements",
    "href": "d3/index.html#technical-requirements",
    "title": "Coursework submission 2: Data science project report",
    "section": "5 Technical Requirements",
    "text": "5 Technical Requirements\n\nWrite the report in a Quarto document (.qmd file)\n\nSee the template in the course GitHub repository at github.com/itsleeds/tds in folder/file d2/template.qmd. \nSee the rendered results at itsleeds.github.io/tds/d2/template (html version) and github.com/itsleeds/tds/releases/download/2026/template.pdf (pdf version)\n\nInclude all necessary code for reproducibility\nDocument any external data sources\nFollow R coding style guidelines"
  },
  {
    "objectID": "d3/index.html#academic-integrity",
    "href": "d3/index.html#academic-integrity",
    "title": "Coursework submission 2: Data science project report",
    "section": "6 Academic Integrity",
    "text": "6 Academic Integrity\n\nClearly acknowledge any use of AI tools (GREEN category - encouraged)\nProperly cite all sources\nEnsure you understand and can explain all submitted work\nDocument any collaboration or assistance received\n\nFor questions or clarifications, please use the module Teams channel or contact the module leader."
  },
  {
    "objectID": "d3/assessment-brief.html",
    "href": "d3/assessment-brief.html",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "TRAN5340M - Transport Data Science\n\n\n\nSummative Coursework: Data Science Project Report\n\n\n\nProject Report and Reproducible Code\n\n\n\n\nTo demonstrate advanced data science techniques applied to a transport problem\nTo show proficiency in data processing, visualization, and analysis\nTo produce high-quality, reproducible research with clear implications for transport planning/policy\nTo critically evaluate methodological approaches and results\n\n\n\n\nA PDF document (max 10 pages)\n\n\n\n100%\n\n\n\nFriday 15th May 2026 at 2pm: 2026-05-15 14:00 BST\n\n\n\n\nA .zip file containing:\n\nA PDF document (max 10 pages)\nReproducible code (.qmd file)\nAny necessary data files, but do not include any large (above around 10 MB) datasets: provide links to these instead\nMaximum file size: 40 MB\n\nSubmission: Via Minerva (Blackboard Assignment)\n\n\n\n\nWhen there is more than one assessment on a module, you will usually receive your feedback before your next assessment for the module is due. Where it is appropriate to do so, and feedback can be released without invalidating the integrity of ongoing assessments, this will typically be no later than 15 working days post submission. Please be mindful that some students may have approved extensions for assessments which mean it is not appropriate to release feedback within 15 working days after individual submissions. In these cases, feedback will be released no later than 15 working days following the submission of all outstanding work for the assessment.\nFeedback will be provided via Minerva and during practical sessions with dedicated time for coursework.\n\n\n\nRobin Lovelace, r.lovelace [at] leeds.ac.uk\n\n\n\nIn ITS Assessment Instructions will be made available separately. Your module leader will tell you when to expect the Assessment Instructions to be available. When the Assessment Instructions are made available you will be able to find them in the Assessment and Feedback Folder on the module Minerva Page."
  },
  {
    "objectID": "d3/assessment-brief.html#assessment-in-brief",
    "href": "d3/assessment-brief.html#assessment-in-brief",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "TRAN5340M - Transport Data Science\n\n\n\nSummative Coursework: Data Science Project Report\n\n\n\nProject Report and Reproducible Code\n\n\n\n\nTo demonstrate advanced data science techniques applied to a transport problem\nTo show proficiency in data processing, visualization, and analysis\nTo produce high-quality, reproducible research with clear implications for transport planning/policy\nTo critically evaluate methodological approaches and results\n\n\n\n\nA PDF document (max 10 pages)\n\n\n\n100%\n\n\n\nFriday 15th May 2026 at 2pm: 2026-05-15 14:00 BST\n\n\n\n\nA .zip file containing:\n\nA PDF document (max 10 pages)\nReproducible code (.qmd file)\nAny necessary data files, but do not include any large (above around 10 MB) datasets: provide links to these instead\nMaximum file size: 40 MB\n\nSubmission: Via Minerva (Blackboard Assignment)\n\n\n\n\nWhen there is more than one assessment on a module, you will usually receive your feedback before your next assessment for the module is due. Where it is appropriate to do so, and feedback can be released without invalidating the integrity of ongoing assessments, this will typically be no later than 15 working days post submission. Please be mindful that some students may have approved extensions for assessments which mean it is not appropriate to release feedback within 15 working days after individual submissions. In these cases, feedback will be released no later than 15 working days following the submission of all outstanding work for the assessment.\nFeedback will be provided via Minerva and during practical sessions with dedicated time for coursework.\n\n\n\nRobin Lovelace, r.lovelace [at] leeds.ac.uk\n\n\n\nIn ITS Assessment Instructions will be made available separately. Your module leader will tell you when to expect the Assessment Instructions to be available. When the Assessment Instructions are made available you will be able to find them in the Assessment and Feedback Folder on the module Minerva Page."
  },
  {
    "objectID": "d3/assessment-brief.html#use-of-genai",
    "href": "d3/assessment-brief.html#use-of-genai",
    "title": "Summative assessment brief: Data science project report",
    "section": "2 Use of GenAI",
    "text": "2 Use of GenAI\nGenerative AI category: GREEN\nUnder this category, AI tools are actively encouraged and can be used extensively.\nIn this assessment, AI tools can be utilised to:\n\nGenerate, test, and debug code for your transport data analysis\nAssist with data visualization and mapping\nProvide explanations of transport concepts and methods\nHelp with code optimization and best practices\nSupport your research on the topic by suggesting areas to investigate\nGive feedback on content and provide proofreading\nAccelerate your learning and productivity\n\nImportant: You must understand and be able to explain all code and analysis you submit, whether AI-generated or not. Document your AI usage in reflective sections of your report.\nIn this assessment, AI tools cannot be utilised to:\n\nproduce the entirety of, or sections of, a piece of work that you submit for assessment beyond that which is outlined above.\n\nThe use of Generative AI must be acknowledged in an ‘Acknowledgements’ section of any piece of academic work where it has been used as a functional tool to assist in the process of creating academic work.\nThe minimum requirement to include in acknowledgement:\n\nName and version of the generative AI system used e.g. Gemini 3\nPublisher (company that made the AI system) e.g. Google\nURL of the AI system\nBrief description (single sentence) of context in which the tool was used.\n\nFor example: “I acknowledge the use of Gemini 3 to prototype the visualisation of my results and test different software packages, including ggplot2, base graphics, and plotly.” Best practice is to include a link to the exact prompt used to generate the content or an appendix of up to 1 page in length with key prompts and session information.\nThe standard Academic Misconduct procedure applies for students believed to have ignored this categorisation.\nFor detailed guidance see  https://generative-ai.leeds.ac.uk/ai-and-assessments/categories-of-assessments/.\nGeneral guidance\nSkills@library hosts useful guidance on academic skills including specific guidance on academic writing and referencing Academic skills."
  },
  {
    "objectID": "d3/assessment-brief.html#assessment-criteria-and-process",
    "href": "d3/assessment-brief.html#assessment-criteria-and-process",
    "title": "Summative assessment brief: Data science project report",
    "section": "3 Assessment criteria and process",
    "text": "3 Assessment criteria and process\nThe detailed marking criteria are made available on Minerva and online for accessibility.\nAssessment length penalties are applied during the marking process & will normally be applied before you receive your mark. Late penalties are normally applied after you receive your mark – if you know you submitted late without a permitted extension, please be aware that your mark may change.\nIn ITS written English is assessed in line with the University’s policies on Assessment of written English (Student Education Service | University of Leeds). You should use spelling, punctuation & grammar to communicate your ideas clearly.\nMarks for the submitted report, are awarded in 4 categories, accounting for the following criteria:\n\n3.1 Data processing: 20%\n\nThe selection and effective use of input datasets that are large (e.g. covering multiple years), complex (e.g. containing multiple variables) and/or diverse (e.g. input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g. by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g. from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n3.2 Visualization and report: 20%\n\nCreation of figures that are readable and well-described (e.g. with captions and description)\nHigh quality, attractive or advanced techniques (e.g. multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g. mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g. with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n3.3 Code quality, efficiency and reproducibility: 20%\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n3.4 Understanding the data science process, including choice of topic and impact: 40%\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure.\n\n\n3.5 Presentation/Formatting and referencing\nYou must appropriately cite all supporting evidence using Quarto’s citation system: https://quarto.org/docs/authoring/citations.html"
  },
  {
    "objectID": "d3/assessment-brief.html#academic-misconduct-and-plagiarism",
    "href": "d3/assessment-brief.html#academic-misconduct-and-plagiarism",
    "title": "Summative assessment brief: Data science project report",
    "section": "4 Academic misconduct and plagiarism",
    "text": "4 Academic misconduct and plagiarism\nThe university expects that all the work you do which includes all forms of assessments submitted and examinations taken, meet the university’s standard for Academic Integrity. All forms of Academic Integrity are investigated through the Academic Misconduct Procedure. This applies to all taught elements of your study, including undergraduate programmes, taught postgraduate study and taught elements of research degrees. Breaching academic integrity standards can lead to serious penalties.\nGuidance on Academic Integrity and Academic Misconduct can be found on the For Students website pages (https://students.leeds.ac.uk/info/10110/academic-integrity) and full definitions of offences under the Academic Misconduct Procedure can be found in the Academic Misconduct Procedure on the Student Cases website page (https://secretariat.leeds.ac.uk/student-cases/academic-misconduct/)."
  },
  {
    "objectID": "d3/assessment-brief.html#assessment-criteria-rubric",
    "href": "d3/assessment-brief.html#assessment-criteria-rubric",
    "title": "Summative assessment brief: Data science project report",
    "section": "5 Assessment criteria rubric",
    "text": "5 Assessment criteria rubric\nThe assessment criteria rubric will be included in the Assessment Instructions that are made available separately."
  },
  {
    "objectID": "dstp.html#course-overview",
    "href": "dstp.html#course-overview",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "1 Course Overview",
    "text": "1 Course Overview\nBased on demand, we’re organising a 2-day course teaching modern data science skills for transport planning, focussed on transport planning practitioners. This course first took place 18th and 19th of September 2025.\nCourse materials can be found at the DSTP course homepage.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#learning-objectives",
    "href": "dstp.html#learning-objectives",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "2 Learning Objectives",
    "text": "2 Learning Objectives\n\nUnderstand the role of data science in transport planning.\nLearn how to find, import, clean, and analyze transport data.\nDevelop skills in data visualization and reporting.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#prerequisites",
    "href": "dstp.html#prerequisites",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "3 Prerequisites",
    "text": "3 Prerequisites\n\nExperience with transport planning concepts and datasets, such as origin-destination data and route networks.\nBasic programming skills in R, Python or similar.\nA laptop with R and RStudio (recommended) or a Python distribution such as Anaconda and an editor such as VS Code or Jupyter Notebook set-up.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#schedule",
    "href": "dstp.html#schedule",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "4 Schedule",
    "text": "4 Schedule\n\n4.1 Day 1: Introduction to R/RStudio\n\n10:00 - 11:00 Introduction to Data Science for Transport Planning\n11:00 - 12:30 Finding, importing and cleaning transport datasets\n\nOrigin-destination datasets\nOpenStreetMap (OSM) and Ordnance Survey (OS) OpenRoads datasets\nStats19 road safety data\n\n12:30 - 13:30: lunch\n13:30 - 15:00 Origin-destination data analysis\n15:00 - 15:15 break and refreshments\n15:15 - 17:00 Routing and route network analysis\n\nThis will cover setting up an interface to a routing engine and using it to calculate routes and distances using GTFS data.\n\n\n\n\n4.2 Day 2:\nCourse times each day:\n\n09:00 - 10:45 spatio-temporal data\n\nDemonstration of open-access OD data with hourly resolution\nDemonstration with stats19 data for road safety analysis\n\n10:45 - 11:15 break and refreshments\n11:15 - 12:30 OD Transport data visualisation\n12:30 - 13:30 lunch\n13:30 - 15:00 Best practices for data science in transport planning\n\nVersion control with Git and GitHub\nReproducible research with Quarto\n\n15:00 - 16:00 Advanced topics\n\nVisualising large datasets\nRoute network integration\n\nWe’ll present ways to join different networks, e.g. OSM networks\n\nDeploying your work as web applications",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#registration",
    "href": "dstp.html#registration",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "5 Registration",
    "text": "5 Registration\nSee store.leeds.ac.uk for registration details.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#contact",
    "href": "dstp.html#contact",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "6 Contact",
    "text": "6 Contact\nFor inquiries, please contact Robin Lovelace.\nWe look forward to seeing you at the course!",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html",
    "href": "reproducible-road-safety-workshop.html",
    "title": "Reproducible data science for road safety research",
    "section": "",
    "text": "This workshop will take place at the University of Leeds’ Institute for Transport Studies (ITS) as part of the RS5C conference that runs from 3rd to 5th September 2025. The workshop takes place on the 2nd September 2025, 13:00-16:00 (including 1 hour for a networking lunch), the day before the main conference starts.\nThe workshop will cover the fundamentals of reproducible data science for road safety research, building on a decade’s worth of experience working with road traffic casualty datasets for policy-relevant road safety research. The UK’s open access STATS19 database will be the basis of the session but the skills learned will be applicable to any road safety datasets. The session will cover:\n\nImporting collision, casualty and vehicle tables: See Chapter 4 on R packages and Chapter 8 on joining tables.\nTemporal visualisation and aggregation: See Chapter 6 on temporal data.\nSpatial visualisation and aggregation: See Chapter 7 on spatial data.\nJoining STATS19 tables: See Chapter 8 on joining tables.\nSpatial joins linking infrastructure to collisions: See Chapter 7 on spatial data.\n\nThe course will be taught in R, a free and open-source programming language for data analysis and visualisation that excels at the kind of statistical modelling and visualisation workflows required for high-impact, reproducible and correct road safety research. The course will be taught by Professor Robin Lovelace, who has over a decade of experience teaching R for data science and is author of the popular book Geocomputation with R. You will learn how to add value to road traffic casualty date for more data-driven and effective interventions to save lives in relation to the largest cause of death for young people worldwide, as highlighted in the map below.\n\nRoad danger levels worldwide in 2016. Data source: World Bank. Reproducible source code: Reproducible Road Safety Research with R, freely available at itsleeds.github.io/rrsrr/.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#importing-collision-casualty-and-vehicle-tables-20-min",
    "href": "reproducible-road-safety-workshop.html#importing-collision-casualty-and-vehicle-tables-20-min",
    "title": "Reproducible data science for road safety research",
    "section": "Importing collision, casualty and vehicle tables (20 min)",
    "text": "Importing collision, casualty and vehicle tables (20 min)\n\nLearn how to load the main STATS19 tables (collision, casualty, vehicle) using the stats19 R package.\nExplore the structure and key variables in each table.\nSee Chapter 4 on R packages and Chapter 8 on joining tables for details.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#temporal-visualisation-and-aggregation-20-min",
    "href": "reproducible-road-safety-workshop.html#temporal-visualisation-and-aggregation-20-min",
    "title": "Reproducible data science for road safety research",
    "section": "Temporal visualisation and aggregation (20 min)",
    "text": "Temporal visualisation and aggregation (20 min)\n\nAggregate collision data by time (e.g., by month or day of week).\nCreate time series plots to identify trends and patterns.\nSee Chapter 6 on temporal data.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#spatial-visualisation-and-aggregation-30-min",
    "href": "reproducible-road-safety-workshop.html#spatial-visualisation-and-aggregation-30-min",
    "title": "Reproducible data science for road safety research",
    "section": "Spatial visualisation and aggregation (30 min)",
    "text": "Spatial visualisation and aggregation (30 min)\n\nConvert collision data to spatial format and plot on a map.\nAggregate collisions by area (e.g., by local authority or police force).\nCreate maps to visualise spatial patterns in road safety data.\nSee Chapter 7 on spatial data.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#joining-stats19-tables-20-min",
    "href": "reproducible-road-safety-workshop.html#joining-stats19-tables-20-min",
    "title": "Reproducible data science for road safety research",
    "section": "Joining STATS19 tables (20 min)",
    "text": "Joining STATS19 tables (20 min)\n\nJoin collision, casualty, and vehicle tables to enrich your analysis.\nExplore relationships between different aspects of road traffic incidents.\nSee Chapter 8 on joining tables.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#bonuses",
    "href": "reproducible-road-safety-workshop.html#bonuses",
    "title": "Reproducible data science for road safety research",
    "section": "Bonuses",
    "text": "Bonuses\nWe have developed a series of bonus exercises for fast finishers, for people who already have the skills covered in the main workshop, or for anyone who wants to go the extra mile. So feel free to work on these bonus exercises if you:\n\nComplete the main tasks early, or\nThink you will learn more by exploring additional resources or tackling more complex problems, and\nFeel free to continue working on these bonus exercises after the workshop ends, there is plenty of work represented here, especially in Bonus 4, and we’re not expecting anyone to develop an entire R/Python package during the 1.5 hour workshop!\n\nThese tasks are also designed to support people managing others or developing teaching/research programs to think about tasks and activities to assign to their students or teams.\n\nBonus 1 (intermediate): Create a repo and share your work on GitHub\n\nCreate a GitHub repository for your workshop materials.\nCommit your code and minimal results, e.g. a figure.\nShare the link to your repository with others, e.g. in the discussion thread at github.com/itsleeds/tds/discussions or on social media.\n\n\n\nBonus 2 (intermediate): reproducing a map used in consultation by Leeds City Council\nSee https://tdscience.github.io/course/collisions.html and reproduce the following map on your own set-up:\n\n\n\nBonus 3 (advanced): analysing collision data to answer a research question\n\nFormulate a research question related to road safety (e.g., “How do cycle lanes impact collision rates?”).\nDecide a spatial and temporal extent for the study (e.g. Birmingham, 2020-2023)\nGenerate as many informative visualisations as possible to explore the data, using any technique you want, e.g.:\n\nUse the osmactive R package to download cycle lane data from OpenStreetMap.\nPerform spatial joins to link collision locations with infrastructure data (e.g., cycle lanes, speed limits).\nAnalyse how infrastructure relates to collision patterns.\nSee Chapter 7 on spatial data.\n\n\n\n\nBonus 4 (advanced): contribute upstream\n\nContribute to an existing codebase for making road safety data easier to access for reproducible research.\n\nFor example, you could open an issue in the stats19 repository.\nOr find a related project that could benefit from your expertise and contribute to it, by first opening an issue.\n\nPropose a new project or feature that could help improve road safety data analysis.\nPropose a new codebase to enable reproducible access to analysis-ready road traffic casualty data in a part of the world you are interested in outside the UK.\n\nFeel free to open a Discussion thread at github.com/itsleeds/tds/discussions",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#prize",
    "href": "reproducible-road-safety-workshop.html#prize",
    "title": "Reproducible data science for road safety research",
    "section": "Prize",
    "text": "Prize\nFor completing bonus activities and sharing them with others, a free copy of Geocomputation with R Second Edition or Geocomputation with Python will be made available.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "s3/index.html",
    "href": "s3/index.html",
    "title": "Session 3: AI for Transport Planning",
    "section": "",
    "text": "Artificial Intelligence (AI) is rapidly transforming transport planning and data science. In this session, we explore how AI tools—particularly GitHub Copilot and large language models (LLMs)—can enhance productivity, code quality, and problem-solving in transport data analysis.\nBy the end of this session, you should be able to:\n\nUnderstand the role of AI in modern transport planning workflows\nUse GitHub Copilot effectively for R and Python coding\nApply prompt engineering techniques for transport data tasks\nRecognize ethical considerations and limitations of AI tools\nIntegrate AI-assisted coding into reproducible research practices"
  },
  {
    "objectID": "s3/index.html#what-is-github-copilot",
    "href": "s3/index.html#what-is-github-copilot",
    "title": "Session 3: AI for Transport Planning",
    "section": "3.1 What is GitHub Copilot?",
    "text": "3.1 What is GitHub Copilot?\nGitHub Copilot is an AI pair programmer that suggests code completions in your editor (VS Code, RStudio, etc.). It is powered by large language models trained on billions of lines of public code."
  },
  {
    "objectID": "s3/index.html#setting-up-copilot",
    "href": "s3/index.html#setting-up-copilot",
    "title": "Session 3: AI for Transport Planning",
    "section": "3.2 Setting Up Copilot",
    "text": "3.2 Setting Up Copilot\nStudents can access GitHub Copilot for free through the GitHub Student Developer Pack:\n\nVerify your student status at education.github.com\nInstall the Copilot extension in your IDE (VS Code, RStudio, etc.)\nSign in with your GitHub account\nStart coding - Copilot will suggest completions automatically"
  },
  {
    "objectID": "s3/index.html#practical-examples",
    "href": "s3/index.html#practical-examples",
    "title": "Session 3: AI for Transport Planning",
    "section": "3.3 Practical Examples",
    "text": "3.3 Practical Examples\n\n3.3.1 Example 1: Loading and Filtering Transport Data\nPrompt (as a comment in R):\n\n# Load cycle hire data from London and filter trips longer than 30 minutes\n\nCopilot might suggest:\n\nlibrary(dplyr)\nlibrary(readr)\n\n# Load cycle hire data\ncycle_data = read_csv(\"cycle_hire.csv\")\n\n# Filter trips longer than 30 minutes\nlong_trips = cycle_data |&gt;\n  filter(duration_minutes &gt; 30)\n\n\n\n3.3.2 Example 2: Creating an Interactive Map\nPrompt:\n\n# Create an interactive map showing bike share stations in Leeds using tmap\n\nCopilot might suggest:\n\nlibrary(sf)\nlibrary(tmap)\n\n# Load bike share stations\nstations = st_read(\"bike_stations.geojson\")\n\n# Create interactive map\ntmap_mode(\"view\")\ntm_shape(stations) +\n  tm_dots(size = 0.5, col = \"blue\", alpha = 0.7) +\n  tm_basemap(\"OpenStreetMap\")\n\n\n\n3.3.3 Example 3: Routing Analysis\nPrompt:\n\n# Calculate cycling routes between origin-destination pairs using stplanr\n\nCopilot might suggest:\n\nlibrary(stplanr)\nlibrary(sf)\n\n# Create OD data\nod = data.frame(\n  from_lon = c(-1.55, -1.56),\n  from_lat = c(53.80, 53.81),\n  to_lon = c(-1.54, -1.53),\n  to_lat = c(53.79, 53.78)\n)\n\n# Convert to spatial lines\nod_sf = od_to_sf(od, crs = 4326)\n\n# Route using CycleStreets API\nroutes = route_cyclestreets(od_sf, plan = \"fastest\")"
  },
  {
    "objectID": "s3/index.html#best-practices-for-using-copilot",
    "href": "s3/index.html#best-practices-for-using-copilot",
    "title": "Session 3: AI for Transport Planning",
    "section": "3.4 Best Practices for Using Copilot",
    "text": "3.4 Best Practices for Using Copilot\n\n3.4.1 ✅ Do:\n\nWrite clear comments describing what you want\nReview suggestions carefully - Copilot can make mistakes\nTest generated code before using in production\nUse Copilot for learning - examine suggestions to understand new approaches\nIterate on prompts - refine your requests if initial suggestions aren’t helpful\n\n\n\n3.4.2 ❌ Don’t:\n\nBlindly accept suggestions without understanding them\nShare sensitive data in prompts (API keys, personal information)\nRely solely on AI - develop your own understanding\nIgnore licensing - be aware of code provenance\nSkip testing - AI-generated code needs validation"
  },
  {
    "objectID": "s3/index.html#structure-of-a-good-prompt",
    "href": "s3/index.html#structure-of-a-good-prompt",
    "title": "Session 3: AI for Transport Planning",
    "section": "4.1 Structure of a Good Prompt",
    "text": "4.1 Structure of a Good Prompt\n\nContext - What domain/dataset are you working with?\nTask - What do you want to accomplish?\nConstraints - Any specific requirements (packages, formats)?\nExample - Show desired input/output if possible"
  },
  {
    "objectID": "s3/index.html#example-prompts",
    "href": "s3/index.html#example-prompts",
    "title": "Session 3: AI for Transport Planning",
    "section": "4.2 Example Prompts",
    "text": "4.2 Example Prompts\n\n4.2.1 Poor Prompt:\n\n# Make a map\n\n\n\n4.2.2 Better Prompt:\n\n# Create an interactive map of Leeds showing:\n# - Bike share stations as blue circles\n# - Cycle routes as green lines\n# - Background OpenStreetMap tiles\n# Use tmap and sf packages\n\n\n\n4.2.3 Excellent Prompt:\n\n# Create an interactive map of Leeds bike infrastructure:\n# Input: bike_stations.geojson (sf points), cycle_routes.geojson (sf lines)\n# Requirements:\n#   - Use tmap in view mode\n#   - Stations: blue circles, size proportional to capacity\n#   - Routes: green lines, width 2px\n#   - Popup showing station name and capacity\n#   - OpenStreetMap basemap\n# Output: Interactive HTML map"
  },
  {
    "objectID": "s3/index.html#chatgpt-claude-for-transport-analysis",
    "href": "s3/index.html#chatgpt-claude-for-transport-analysis",
    "title": "Session 3: AI for Transport Planning",
    "section": "5.1 ChatGPT / Claude for Transport Analysis",
    "text": "5.1 ChatGPT / Claude for Transport Analysis\n\nData exploration - “How do I analyze origin-destination matrices in R?”\nDebugging - Paste error messages for explanations\nCode review - Ask for suggestions to improve efficiency\nDocumentation - Generate README files or method descriptions"
  },
  {
    "objectID": "s3/index.html#ai-powered-tools-for-transport",
    "href": "s3/index.html#ai-powered-tools-for-transport",
    "title": "Session 3: AI for Transport Planning",
    "section": "5.2 AI-Powered Tools for Transport",
    "text": "5.2 AI-Powered Tools for Transport\n\nOpenAI Codex - Powers GitHub Copilot\nCursor - AI-first code editor\nCopilot Chat - Conversational AI in VS Code\nTabnine - Alternative code completion tool\nAmazon CodeWhisperer - AWS-focused AI assistant"
  },
  {
    "objectID": "s3/index.html#bias-and-fairness",
    "href": "s3/index.html#bias-and-fairness",
    "title": "Session 3: AI for Transport Planning",
    "section": "6.1 Bias and Fairness",
    "text": "6.1 Bias and Fairness\n\nAI models are trained on historical data, which may contain biases\nBe critical when AI suggests transport solutions that may disadvantage certain communities\nValidate outputs against equity and accessibility principles"
  },
  {
    "objectID": "s3/index.html#data-privacy",
    "href": "s3/index.html#data-privacy",
    "title": "Session 3: AI for Transport Planning",
    "section": "6.2 Data Privacy",
    "text": "6.2 Data Privacy\n\nAvoid sending sensitive transport data (personal GPS traces, private surveys) to AI services\nUse anonymized or synthetic data for demonstrations\nReview your organization’s AI usage policies"
  },
  {
    "objectID": "s3/index.html#academic-integrity",
    "href": "s3/index.html#academic-integrity",
    "title": "Session 3: AI for Transport Planning",
    "section": "6.3 Academic Integrity",
    "text": "6.3 Academic Integrity\n\nCite AI tools when used significantly in your work\nUnderstand the code you submit - you are responsible for it\nFollow assignment guidelines on AI usage (this module encourages AI use!)\nDevelop your skills - AI is a tool to enhance, not replace, learning"
  },
  {
    "objectID": "s3/index.html#limitations",
    "href": "s3/index.html#limitations",
    "title": "Session 3: AI for Transport Planning",
    "section": "6.4 Limitations",
    "text": "6.4 Limitations\n\nAI can generate plausible but incorrect code\nAI lacks domain expertise in specialized transport methods\nAI cannot validate results - you must check outputs\nAI may suggest outdated approaches or deprecated functions"
  },
  {
    "objectID": "s3/index.html#task-1-generate-data-loading-code",
    "href": "s3/index.html#task-1-generate-data-loading-code",
    "title": "Session 3: AI for Transport Planning",
    "section": "8.1 Task 1: Generate Data Loading Code",
    "text": "8.1 Task 1: Generate Data Loading Code\nUse GitHub Copilot or ChatGPT to:\n\nLoad origin-destination data for a UK city\nFilter for trips by a specific mode (e.g., cycling)\nCalculate summary statistics (total trips, average distance)\n\nPrompt template:\n\n# Load OD data from [city] and calculate cycling trip statistics\n# Data format: CSV with columns origin, destination, mode, distance, trips\n# Filter for mode == \"bike\" and calculate:\n#   - Total number of trips\n#   - Average trip distance\n#   - Top 5 origin zones by trip count"
  },
  {
    "objectID": "s3/index.html#task-2-create-visualization",
    "href": "s3/index.html#task-2-create-visualization",
    "title": "Session 3: AI for Transport Planning",
    "section": "8.2 Task 2: Create Visualization",
    "text": "8.2 Task 2: Create Visualization\nUse AI to generate code for:\n\nAn interactive map showing top cycling routes\nA bar chart of trips by hour of day\nA spatial heatmap of trip density"
  },
  {
    "objectID": "s3/index.html#task-3-debug-and-optimize",
    "href": "s3/index.html#task-3-debug-and-optimize",
    "title": "Session 3: AI for Transport Planning",
    "section": "8.3 Task 3: Debug and Optimize",
    "text": "8.3 Task 3: Debug and Optimize\n\nIntentionally introduce an error in your code\nUse Copilot Chat or ChatGPT to diagnose and fix it\nAsk AI to suggest performance optimizations"
  },
  {
    "objectID": "s3/index.html#learning-more",
    "href": "s3/index.html#learning-more",
    "title": "Session 3: AI for Transport Planning",
    "section": "9.1 Learning More",
    "text": "9.1 Learning More\n\nGitHub Copilot Documentation\nPrompt Engineering Guide\nAnthropic’s Prompt Engineering Interactive Tutorial"
  },
  {
    "objectID": "s3/index.html#transport-specific-ai-applications",
    "href": "s3/index.html#transport-specific-ai-applications",
    "title": "Session 3: AI for Transport Planning",
    "section": "9.2 Transport-Specific AI Applications",
    "text": "9.2 Transport-Specific AI Applications\n\nTraffic prediction - ML models for congestion forecasting\nDemand modeling - Neural networks for trip generation\nRoute optimization - Reinforcement learning for vehicle routing\nImage analysis - Computer vision for traffic counting\nNatural language processing - Analyzing transport surveys and reports# Summary\n\nAI tools like GitHub Copilot are transforming how we work with transport data:\n\nAccelerate coding with intelligent suggestions\nLearn faster by examining AI-generated examples\nFocus on analysis rather than syntax\nMaintain critical thinking - AI is a tool, not a replacement for expertise\n\nKey Takeaways:\n\nUse AI to enhance productivity, not replace understanding\nWrite clear prompts for better results\nAlways review, test, and validate AI suggestions\nBe mindful of ethical considerations and limitations\nEmbrace AI as part of modern reproducible research workflows"
  }
]