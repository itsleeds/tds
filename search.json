[
  {
    "objectID": "s3/index.html",
    "href": "s3/index.html",
    "title": "Session 3: Geographic and origin-destination data",
    "section": "",
    "text": "In this session, we will learn how to use geogarphic and origin-destination data. The contents of the session are as follows:\n\nWe will start with reviewing the homework from the previous session\nA short lecture on geographic and origin-destination data (see slides)\nSession activities: working with various data, including analysing origin-destination trip flows in London Cycle Hire System.\nBonus: Geometry operations and spatial analysis\nHomework and next session"
  },
  {
    "objectID": "s3/index.html#pre-requisites",
    "href": "s3/index.html#pre-requisites",
    "title": "Session 3: Geographic and origin-destination data",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nYou need to have a number of packages installed and loaded. Install the packages by typing in the following commands into RStudio (you do not need to add the comments after the # symbol)\nIf you need to install any of these packages use:\n\nRPython\n\n\n\nif (!require(\"pak\")) install.packages(\"pak\")\npak::pkg_install(c(\"sf\", \"tidyverse\", \"remotes\", \"ggspatial\"))\n# GitHub pkgs\npak::pkg_install(\"Nowosad/spDataLarge\")\n\n\nlibrary(sf)        # vector data package\nlibrary(tidyverse) # tidyverse packages\nlibrary(ggspatial) # ggspatial package\nlibrary(spData)    # spatial data package\n\n\n\n\n# Install necessary packages (uncomment if not already installed)\n# !pip install geopandas pandas matplotlib seaborn\n\nimport geopandas as gpd       # vector data package\nimport pandas as pd           # data manipulation\nimport matplotlib.pyplot as plt  # plotting\nimport seaborn as sns            # advanced plotting\n# For spatial data, geopandas comes with sample datasets\n# Alternatively, we can use the naturalearth datasets\nimport geopandas.datasets\n\n\n\n\n\nCheck your packages are up-to-date with update.packages() in R (or equivalent in Python)\nCreate a project folder with an appropriate name for this session (e.g. session3)\nCreate appropriate folders for code, data and anything else (e.g. images)\nCreate a script called learning-OD.R, e.g. with the following command:\n\nmkdir code\ncode code/learning-OD.R # for R\ncode code/learning-OD.py # for Python"
  },
  {
    "objectID": "s3/index.html#basic-sf-operations",
    "href": "s3/index.html#basic-sf-operations",
    "title": "Session 3: Geographic and origin-destination data",
    "section": "3.1 Basic sf operations",
    "text": "3.1 Basic sf operations\nWe will start with a simple map of the world. Load the world object from the spData package. Notice the use of :: to say that you want the world object from the spData package.\n\nRPython\n\n\n\nworld = spData::world\n\n\n\n\nworld = gpd.read_file(\n    'https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip'\n)\n\n\n\n\nUse some basic R functions to explore the world object. e.g. class(world), dim(world), head(world), summary(world). Also view the world object by clicking on it in the Environment panel.\nsf objects can be plotted with plot().\n\nRPython\n\n\n\nplot(world)\n\n\n\n\nprint(type(world))       # Equivalent to class(world)\nprint(world.shape)       # Equivalent to dim(world)\nprint(world.head())      # Equivalent to head(world)\nprint(world.describe())  # Equivalent to summary(world)\n\n# Plotting the world GeoDataFrame\nworld.plot(figsize=(12, 8))\nplt.title('World Map')\nplt.show()\n\n\n\n\nNote that this makes a map of each column in the data frame. Try some other plotting options\n\nRPython\n\n\n\nplot(world[3:6])\nplot(world[\"pop\"])\n\n\n\n\n# Since world is a GeoDataFrame, we can select columns by position\n# However, GeoPandas plots the geometry, so we need to specify columns\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nworld.plot(column='POP_EST', ax=axes[0])\nworld.plot(column='GDP_YEAR', ax=axes[1])\nworld.plot(column='CONTINENT', ax=axes[2])\nplt.show()"
  },
  {
    "objectID": "s3/index.html#basic-spatial-operations",
    "href": "s3/index.html#basic-spatial-operations",
    "title": "Session 3: Geographic and origin-destination data",
    "section": "3.2 Basic spatial operations",
    "text": "3.2 Basic spatial operations\nLoad the nz and nz_height datasets from the spData package.\n\nRPython\n\n\n\nnz = spData::nz\nnz_height = spData::nz_height\n\n\n\n\nnz = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz.gpkg\")\nnz_height = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz_height.gpkg\")\n\n\n\n\nWe can use tidyverse functions like filter and select on sf objects in the same way you did in Session 1.\n\nRPython\n\n\n\ncanterbury = nz |&gt; filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\n\n\n\n\ncanterbury = nz[nz['Name'] == 'Canterbury']\n\n\n\n\nIn this case we filtered the nz object to only include places called Canterbury and then did and intersection to find objects in the nz_height object that are in Canterbury.\nThis syntax is not very clear. But is the equivalent to\n\nRPython\n\n\n\ncanterbury_height = nz_height[canterbury, , op = st_intersects]\n\n\n\n\ncanterbury_height = gpd.overlay(nz_height, canterbury, how='intersection')\n\n\n\n\nThere are many different types of relationships you can use with op. Try ?st_intersects() to see more. For example this would give all the places not in Canterbury\n\nRPython\n\n\n\nnz_height[canterbury, , op = st_disjoint]\n\n\n\n\ncanterbury_height = gpd.sjoin(nz_height, canterbury, op='intersects')\n\n\n\n\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue. The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string."
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Reading List",
    "section": "",
    "text": "This reading list contains key resources for the Transport Data Science module, organized by topic.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#key-skills",
    "href": "reading.html#key-skills",
    "title": "Reading List",
    "section": "2.1 Key Skills",
    "text": "2.1 Key Skills\n\nQuarto documentation (Allaire et al., 2024)\n\n\nThe software used to create the Transport Data Science course materials and numerous websites, presentations, dashboards, and books, Quarto is a powerful tool for creating reproducible documents with code and data.\nSee the technical writing page of Quarto’s documentation for key information on how to add references, figure captions, and more.\n\nIntroduction to GitHub (Heis, 2025)\n\nA good starting point for learning how to use GitHub for version control and collaboration. \nSee also their introduction to Devcontainers at docs.github.com/en/codespaces/",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#python",
    "href": "reading.html#python",
    "title": "Reading List",
    "section": "2.2 Python",
    "text": "2.2 Python\n\nCourse Materials for: Geospatial Data Science (Szell, 2025)\n\nCourse materials covering various aspects of geospatial data science, including data analysis, visualization, and working with street networks using Python.\n\nModern Polars (Heavey, n.d.)\n\nA side-by-side comparison of the Polars and Pandas libraries. \n\nA course on Geographic Data Science (Arribas-Bel, 2019)\n\nFree and open source online book on using GeoPandas and other Python libraries for geographic data analysis.\n\nPython for Data Analysis (McKinney, 2022)\n\nDta wrangling with Pandas, NumPy, and Jupyter, written by the creator of the Pandas library.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#r",
    "href": "reading.html#r",
    "title": "Reading List",
    "section": "2.3 R",
    "text": "2.3 R\n\nAdvanced R\n\nA comprehensive guide to advanced programming in R, covering topics such as functional programming and object-oriented programming.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#miscellaneous",
    "href": "reading.html#miscellaneous",
    "title": "Reading List",
    "section": "5.1 Miscellaneous",
    "text": "5.1 Miscellaneous\n\nData Science for Transport: A Self-Study Guide with Computer Exercises (Fox, 2018)\n\nAn introduction to transport data science with hands-on examples, slightly out of date as of 2025.\n\nReproducible Road Safety Research with R (Lovelace, 2020)\n\nIntroductory guide for analyzing road safety data in R\n\nOpen source tools for geographic analysis in transport planning (Lovelace, 2021)\n\nReview of open source tools available for transport planning and analysis.\n\nPython for Data Science (Turrell et al., 2025)\n\nA modern guide to data science using Python based on R for Data Science, with practical examples and clear explanations.\n\nThe Geography of Transport Systems (Rodrigue et al., 2013)\n\nComprehensive textbook on transport geography and systems\n\nModelling Transport (Ortúzar S. and Willumsen, 2001)\n\nFoundational text on transport modeling methods\n\nBuilding Reproducible Analytical Pipelines with R (rodrigues_building?)\n\nA guide to the data engineering side of data science, with a focus on reproducibility and automation.\n\nPapers investigating the relationships between new contraflow interventions and traffic levels and collision rates in London (Tait et al., 2024, 2023)\n\nSee the full bibliography on Zotero for more resources, and feel free to suggest additions by opening an issue in the tds issue tracker.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The timetable below is a user-friently representation of the timetable for the module (see github for .csv and .ics versions). See timetable.leeds.ac.uk and mytimetable.eeds.ac.uk for the official timetable. If you spot any discrepancies, please let us know.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "d3/report-structure.html",
    "href": "d3/report-structure.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Your report should have a logical structure and clear headings which could include:\n\nIntroduction\n\nClear research question\nContext and motivation\nReference to relevant literature\n\nInput Data and Data Cleaning\n\nDescription of datasets\nData quality considerations\nProcessing steps\n\nExploratory Data Analysis\n\nInitial visualization\nKey patterns\nStatistical summaries\n\nAnalysis and Results\n\nDetailed analysis\nClear presentation\nSupporting visualizations\n\nDiscussion and conclusions\n\nResult, key findings, interpretation\nPolicy implications/recommendations\nStrengths and limitations\nFuture directions\n\nReferences\n\nProperly formatted citations\nMix of academic and technical/policy/other sources\nRecommendation: generate these with Quarto (see Quarto Citation Guide)"
  },
  {
    "objectID": "d2/assessment-brief.html",
    "href": "d2/assessment-brief.html",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "TRAN5340M - Transport Data Science\n\n\n\nFormative Coursework: Portfolio Plan and Reproducible Code\n\n\n\nPortfolio Plan and Reproducible Code Submission\n\n\n\n\nTo develop a clear plan for applying data science techniques to a transport problem.\nTo demonstrate the ability to work with datasets and produce reproducible code.\nTo critically engage with academic literature and formulate research questions.\n\nNote: this is formative and not formally assessed, but feedback will be provided.\n\n\n\nNon-assessed submission deadline: 28th February 2025, 13:59.\n\n\n\nFeedback will be provided within 15 working days of submission. Written feedback will be provided alongside guidance on how to proceed with the final coursework.\n\n\n\nProfessor Robin Lovelace\nr dot lovelace [at] leeds.ac.uk\n\n\n\n\nThis formative coursework is designed to help you plan and receive feedback on your final coursework. You will submit a .zip file containing a PDF document and reproducible code. The document should outline your topic, datasets, research questions, and analysis plan. Feedback will be provided to guide your final submission.\nThe purpose of this formative assessment is:\n\nTo allow you to ask questions to the course team (e.g., “Does this sound like a reasonable input dataset and topic?”).\nTo describe progress on reading input datasets and the analysis plan.\nTo receive feedback on your approach before the final submission."
  },
  {
    "objectID": "d2/assessment-brief.html#module-code-and-title",
    "href": "d2/assessment-brief.html#module-code-and-title",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "TRAN5340M - Transport Data Science"
  },
  {
    "objectID": "d2/assessment-brief.html#assessment-title",
    "href": "d2/assessment-brief.html#assessment-title",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "Formative Coursework: Portfolio Plan and Reproducible Code"
  },
  {
    "objectID": "d2/assessment-brief.html#assessment-type",
    "href": "d2/assessment-brief.html#assessment-type",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "Portfolio Plan and Reproducible Code Submission"
  },
  {
    "objectID": "d2/assessment-brief.html#learning-outcomes",
    "href": "d2/assessment-brief.html#learning-outcomes",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "To develop a clear plan for applying data science techniques to a transport problem.\nTo demonstrate the ability to work with datasets and produce reproducible code.\nTo critically engage with academic literature and formulate research questions.\n\nNote: this is formative and not formally assessed, but feedback will be provided."
  },
  {
    "objectID": "d2/assessment-brief.html#deadline",
    "href": "d2/assessment-brief.html#deadline",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "Non-assessed submission deadline: 28th February 2025, 13:59."
  },
  {
    "objectID": "d2/assessment-brief.html#feedback",
    "href": "d2/assessment-brief.html#feedback",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "Feedback will be provided within 15 working days of submission. Written feedback will be provided alongside guidance on how to proceed with the final coursework."
  },
  {
    "objectID": "d2/assessment-brief.html#contact",
    "href": "d2/assessment-brief.html#contact",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "Professor Robin Lovelace\nr dot lovelace [at] leeds.ac.uk"
  },
  {
    "objectID": "d2/assessment-brief.html#assessment-summary",
    "href": "d2/assessment-brief.html#assessment-summary",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "",
    "text": "This formative coursework is designed to help you plan and receive feedback on your final coursework. You will submit a .zip file containing a PDF document and reproducible code. The document should outline your topic, datasets, research questions, and analysis plan. Feedback will be provided to guide your final submission.\nThe purpose of this formative assessment is:\n\nTo allow you to ask questions to the course team (e.g., “Does this sound like a reasonable input dataset and topic?”).\nTo describe progress on reading input datasets and the analysis plan.\nTo receive feedback on your approach before the final submission."
  },
  {
    "objectID": "d2/assessment-brief.html#rendering",
    "href": "d2/assessment-brief.html#rendering",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "3.1 Rendering",
    "text": "3.1 Rendering\n\nIf you cannot render to PDF directly, render to HTML and convert to PDF by printing to PDF from your browser."
  },
  {
    "objectID": "d2/assessment-brief.html#presentation",
    "href": "d2/assessment-brief.html#presentation",
    "title": "Formative assessment brief: portfolio plan and reproducible data science code",
    "section": "3.2 Presentation",
    "text": "3.2 Presentation\nYou must appropriately cite all supporting evidence using appropriate references and a consistent, professional style.\nSee the authoring tutorial with RStudio at quarto.org for guidance on how to add citations to your document in RStudio’s Visual Editor mode."
  },
  {
    "objectID": "d2/index.html",
    "href": "d2/index.html",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "",
    "text": "This is a formative (non-assessed but required) submission that will help you develop your final coursework. The deadline is 28th February 2025, 13:59."
  },
  {
    "objectID": "d2/index.html#what-to-submit",
    "href": "d2/index.html#what-to-submit",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "What to Submit",
    "text": "What to Submit\nSubmit a .zip file containing two key items:\n\nA concise PDF document (recommended length: 2 pages, absolute maximum: 5 pages) outlining:\n\nYour chosen transport-related topic\nThe main dataset(s) you plan to use\nYour research question\nAt least 2 academic references (see Quarto Citation Guide for details)\nAny initial analysis or questions you have\n\nReproducible code as a .qmd file showing how you accessed and processed your data"
  },
  {
    "objectID": "d2/index.html#template-and-example-submission",
    "href": "d2/index.html#template-and-example-submission",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Template and example submission",
    "text": "Template and example submission\nSee the template.qmd file (and rendered result) for guidance on the structure of your submission. An example submission is available in the d2/example.qmd file (rendered here).\nSee an example .zip file with the files needed to reproduce this analysis at gitub.com/itsleeds/tds/releases/.\nSee the source code of these files, including the .bib files for creating references, in the course repository: github.com/itsleeds/tds/tree/main/d2."
  },
  {
    "objectID": "d2/index.html#key-requirements",
    "href": "d2/index.html#key-requirements",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Key Requirements",
    "text": "Key Requirements\n\nMaximum .zip file size: 30 MB\nSubmit via Turnitin\nAI tools can be used in an assistive role (must be acknowledged)\nUse the default quarto referencing style"
  },
  {
    "objectID": "d2/index.html#writing-tips",
    "href": "d2/index.html#writing-tips",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Writing tips",
    "text": "Writing tips\nSee documentation on figures, technical writing and the visual editor mode from quarto.org for help with creating figures and citations."
  },
  {
    "objectID": "d2/index.html#topics-and-datasets",
    "href": "d2/index.html#topics-and-datasets",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Topics and Datasets",
    "text": "Topics and Datasets\nSome suggested areas include:\n\nRoad safety analysis\nInfrastructure and travel behavior\nTraffic congestion patterns\nPublic transport accessibility\nActive travel infrastructure\nTransport equity studies\nOther transport-related topics are encouraged\n\nSpecific examples could include:\n\nWhat is the relationship between travel behaviour (e.g. as manifested in origin-destination data represented as desire lines, routes and route networks) and road traffic casualties in a transport region (e.g. London, West Midlands and other regions in the pct::pct_regions$region_name data)\nAnalysis of a large transport dataset, e.g. https://www.nature.com/articles/sdata201889\nInfrastructure and travel behaviour\n\nWhat are the relationships between specific types of infrastructure and travel, e.g. between fast roads and walking?\nHow do official sources of infrastructure data (e.g. the CID) compare with crowd-sourced datasets such as OpenStreetMap (which can be accessed with the new osmextract R package)\nUsing new data sources to support transport planning, e.g. using data from https://telraam.net/ or https://dataforgood.facebook.com/dfg/tools/high-resolution-population-density-maps\n\nChanging transport systems\n\nModelling change in transport systems, e.g. by comparing before/after data for different countries/cities, which countries had the hardest lockdowns and where have changes been longer term? - see here for open data: https://github.com/ActiveConclusion/COVID19_mobility\nHow have movement patterns changed during the Coronavirus pandemic and what impact is that likely to have long term (see here for some graphics on this)\n\nSoftware / web development\n\nCreating a package to make a particular data source more accessible, see https://github.com/ropensci/stats19 and https://github.com/elipousson/crashapi examples\nDevelopment of a data dashboard, e.g. using Quarto Dashboards\nDevelopment of a web app, e.g. using the shiny package\n\nRoad safety - how can we makes roads and transport systems in general safer?\n\nInfluence of Road Infrastructure:\n\nAssessing the role of well-designed pedestrian crossings, roundabouts, and traffic calming measures in preventing road accidents.\nInvestigating the correlation between road surface quality (e.g., potholes, uneven surfaces) and the frequency of accidents.\n\nInfluence of Traffic Management:\n\nAssessing the role of traffic lights and speed cameras in preventing road accidents.\nInvestigating the correlation between the frequency of accidents and the presence of traffic calming measures (e.g., speed bumps, chicanes, road narrowing, etc.).\n\nLegislation and Enforcement:\n\n\nAssessing the role of speed limits in preventing road accidents.\n\n\n\nTraffic congestion - how can we reduce congestion?\n\nData Collection and Analysis:\n\nUtilizing real-time traffic data from platforms like Waze and Google Maps to forecast congestion patterns.\nAnalyzing historical traffic data to identify recurring congestion patterns and anticipate future traffic bottlenecks.\n\nMachine Learning and Predictive Modeling:\n\nDesigning machine learning models that use past and current traffic data to predict future congestion levels."
  },
  {
    "objectID": "d2/index.html#support-and-feedback",
    "href": "d2/index.html#support-and-feedback",
    "title": "Coursework submission 1: Data science project plan and reproducible code",
    "section": "Support and Feedback",
    "text": "Support and Feedback\n\nFeedback will be provided within 15 working days\n\nFor full details including assessment criteria, formatting guidelines, and academic integrity requirements, see the assessment brief."
  },
  {
    "objectID": "marking-criteria.html",
    "href": "marking-criteria.html",
    "title": "Marking Criteria",
    "section": "",
    "text": "The report should be written as a Quarto document (.qmd file) and submitted as a .zip file containing the qmd file and rendered PDF file. See the template in the course GitHub repository at github.com/itsleeds/tds in folder/file d2/template.qmd for an example.\nMarks for the submitted report, are awarded in 4 categories, accounting for the following criteria:\n\n0.1 Data processing: 20%\n\nThe selection and effective use of input datasets that are large (e.g. covering multiple years), complex (e.g. containing multiple variables) and/or diverse (e.g. input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g. by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g. from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n0.2 Visualization and report: 20%\n\nCreation of figures that are readable and well-described (e.g. with captions and description)\nHigh quality, attractive or advanced techniques (e.g. multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g. mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g. with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n0.3 Code quality, efficiency and reproducibility: 20%\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n0.4 Understanding the data science process, including choice of topic and impact: 40%\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "s4/dodgr-install.html",
    "href": "s4/dodgr-install.html",
    "title": "1 Local Install of dodgr",
    "section": "",
    "text": "1 Local Install of dodgr\nMalcolm Morgan University of Leeds, 2020-02-11\nOnly if you cannot run the dodgr examples.\nThis will locally compile the latest version of dodgr on you computer.\n\nSave your work in progress and close RStudio\nGo to https://github.com/ATFutures/dodgr/ and click Clone or download\nChoose download zip\nUnzip the folder\nIn the unziped folder find and open dodgr.Rproj a new Rstudio session will open.\nRun this code.\n\n\nremove.packages(\"dodgr\")\nif(!\"devtools\" %in% installed.packages()[,1]){\n  install.packages(\"devtools\")\n}\ndevtools::install(\".\", export_all = TRUE, upgrade = \"never\")\nlibrary(dodgr)\n\nClose RStudio and open a new Rstudio session with your TDS work.\nDodgr should now work"
  },
  {
    "objectID": "s2/demo.html",
    "href": "s2/demo.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "library(tidyverse)\nlibrary(osmextract)\n\nregion_name = \"lund\"\nosm_data_raw = oe_get(place = region_name)\nosm_data_shops = oe_get(\n  place = region_name,\n  query = \"\n  SELECT * \n  FROM 'points' \n  WHERE shop = 'supermarket'\",\n  extra_tags = c(\"shop\")\n)\ndim(osm_data_shops)\nplot(osm_data_shops$geometry)\n\nlund_region = zonebuilder::zb_zone(\"Lund, sweden\")\nlibrary(tmap)\ntmap_mode(\"view\")\nqtm(lund_region)\nlund_6km = lund_region |&gt;\n  filter(circle_id &lt;= 3)\nlund_6km_boundary = sf::st_union(lund_6km)\nqtm(lund_6km_boundary)\n\nosm_data_lund1 = osm_data_shops |&gt;\n  sf::st_filter(lund_6km_boundary)\nnrow(osm_data_lund1)\nqtm(osm_data_lund1)\n\nosm_data_lund2 = oe_get(\n  place = region_name,\n  query = \"\n  SELECT * \n  FROM 'points' \n  WHERE shop = 'supermarket'\",\n  extra_tags = c(\"shop\"),\n  boundary = lund_6km_boundary,\n  boundary_type = \"clipsrc\"\n)\n\nnrow(osm_data_lund2)"
  },
  {
    "objectID": "s2/index.html",
    "href": "s2/index.html",
    "title": "Session 2: Getting transport datasets with R",
    "section": "",
    "text": "In this session, we will learn how to get transport datasets using R. The contents of the session are as follows:\n\nWe’ll start with a short lecture on data sources and ways of classifying transport datasets (see the slides)\nReviewing the homework from the previous session\nSession activities: importing and exploring a range of transport datasets in your own time\nBonus: exploring the Cadence platform\nHomework for the next session\n\n\n\nYou should now be familiar with the basics of R, Quarto and the structure of transport datasets, having completed the homework from the previous session.\nWe will do a demo of trying to reproduce the demo from last week and discuss any issues you had running the code in Chapter 13 of Geocomputation with R.\n\n\n\nNote: you may need to install the pct package as follows:\nremotes::install_github(\"ITSLeeds/pct\")\nWe will also load the following packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nNote that this session uses imports and uses geographic data with the sf package. Read more about the package in chapters 2 onwards in Geocomputation with R and in the sf package documentation."
  },
  {
    "objectID": "s2/index.html#review-homework",
    "href": "s2/index.html#review-homework",
    "title": "Session 2: Getting transport datasets with R",
    "section": "",
    "text": "You should now be familiar with the basics of R, Quarto and the structure of transport datasets, having completed the homework from the previous session.\nWe will do a demo of trying to reproduce the demo from last week and discuss any issues you had running the code in Chapter 13 of Geocomputation with R."
  },
  {
    "objectID": "s2/index.html#prerequisites",
    "href": "s2/index.html#prerequisites",
    "title": "Session 2: Getting transport datasets with R",
    "section": "",
    "text": "Note: you may need to install the pct package as follows:\nremotes::install_github(\"ITSLeeds/pct\")\nWe will also load the following packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nNote that this session uses imports and uses geographic data with the sf package. Read more about the package in chapters 2 onwards in Geocomputation with R and in the sf package documentation."
  },
  {
    "objectID": "s2/index.html#bonus-exercises",
    "href": "s2/index.html#bonus-exercises",
    "title": "Session 2: Getting transport datasets with R",
    "section": "2.1 Bonus exercises",
    "text": "2.1 Bonus exercises\n\nReproduce the examples\nGet all supermarkets in OSM for West Yorkshire\nIdentify all cycleways in West Yorkshire and, using the stats19 data you have already downloaded, identify all crashes that happened near them.\n\nImport and visualise a dataset with supermarket names and locations with the following code (see the source code of the session to see how the supermarket data was obtained with osmextract):\n\nsupermarkets = sf::read_sf(\"https://github.com/ITSLeeds/tds/releases/download/2025/supermarkets_points_cleaned.geojson\")\nlibrary(tmap)\ntmap_mode(\"view\")\ntm_shape(supermarkets) +\n  tm_dots(\"name_simplified\")"
  },
  {
    "objectID": "s2/index.html#the-ons-create-a-custom-dataset-tool",
    "href": "s2/index.html#the-ons-create-a-custom-dataset-tool",
    "title": "Session 2: Getting transport datasets with R",
    "section": "5.1 The ONS “create a custom dataset” tool",
    "text": "5.1 The ONS “create a custom dataset” tool\nThe Office for National Statistics (ONS) provides a tool to create custom datasets. The tool is flexible and provides datasets in a variety of formats, including CSV. Give the tool a try at www.ons.gov.uk/datasets/create. To test the tool, try to get data on travel to work patterns for all usual residents in England and Wales at the local authority level (note: you may need to change the file name to match the one you downloaded).\n\nres_sf = sf::st_read(\"lad_boundaries_2023.geojson\")\n\nReading layer `lad_boundaries_2023' from data source \n  `/home/runner/work/tds/tds/s2/lad_boundaries_2023.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 361 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -116.1928 ymin: 7054.1 xmax: 655653.9 ymax: 1220310\nProjected CRS: OSGB36 / British National Grid\n\ntravel_to_work_lad = readr::read_csv(\"custom-filtered-2025-02-04T00_06_30Z.csv\")\n# names(travel_to_work_lad)\n# [1] \"Lower tier local authorities Code\"                      \n# [2] \"Lower tier local authorities\"                           \n# [3] \"Distance travelled to work (8 categories) Code\"         \n# [4] \"Distance travelled to work (8 categories)\"              \n# [5] \"Method used to travel to workplace (12 categories) Code\"\n# [6] \"Method used to travel to workplace (12 categories)\"     \n# [7] \"Observation\" \ntravel_to_work_updated = travel_to_work_lad |&gt;\n  select(\n    LAD23CD = `2023 Lower tier local authorities Code`,\n    Mode = `Method used to travel to workplace (12 categories)`,\n    Distance = `Distance travelled to work (8 categories)`,\n    Observation = Observation\n  )\n# Pivot wider:\nttw_wide = travel_to_work_updated |&gt;\n  pivot_wider(names_from = c(Distance, Mode), values_from = Observation)\nsummary(res_sf[[\"LAD23CD\"]] %in% travel_to_work_lad[[1]]) \n\n   Mode   FALSE    TRUE \nlogical      44     317 \n\n# Other way around:\nsummary(travel_to_work_lad[[1]] %in% res_sf[[\"LAD23CD\"]])\n\n   Mode    TRUE \nlogical   30432 \n\n# names(ttw_wide)"
  },
  {
    "objectID": "s5/demo-references.html",
    "href": "s5/demo-references.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "You can cite packages as follows:\n\ncitation(\"osmextract\")\n\nOpen up the .bib file, e.g. with\n\nfile.edit(\"s5/demo-references.bib\")\n\nI used the osmextract package (Gilardi and Lovelace, 2024).\n(Ferster et al., 2019)\n(Kaiser et al., 2024)\n\n\n\n\n\n\n\n1 References\n\nFerster, C., Fischer, J., Manaugh, K., Nelson, T., Winters, M., 2019. Using OpenStreetMap to inventory bicycle infrastructure: A comparison with open data from cities. International Journal of Sustainable Transportation 14, 64–73. https://doi.org/10.1080/15568318.2018.1519746\n\n\nGilardi, A., Lovelace, R., 2024. Osmextract: Download and import open street map data extracts.\n\n\nKaiser, S.K., Klein, N., Kaack, L.H., 2024. From counting stations to city-wide estimates: Data-driven bicycle volume extrapolation. arXiv. https://doi.org/10.48550/ARXIV.2406.18454"
  },
  {
    "objectID": "s5/slides.html#a-brief-history-of-geographic-vizualisation",
    "href": "s5/slides.html#a-brief-history-of-geographic-vizualisation",
    "title": "Visualising transport data",
    "section": "A brief history of geographic vizualisation",
    "text": "A brief history of geographic vizualisation\n\nHumboldt’s Naturgemälde (1807, Geography of Plants)"
  },
  {
    "objectID": "s5/slides.html#good-bad-ugly-wrong",
    "href": "s5/slides.html#good-bad-ugly-wrong",
    "title": "Visualising transport data",
    "section": "Good, bad, ugly, wrong",
    "text": "Good, bad, ugly, wrong\n\nAim must be good graphics, but first it’s important to avoid pitfalls\nSource: Free and open book Data Visualisation"
  },
  {
    "objectID": "s5/slides.html#what-is-data-visualisation",
    "href": "s5/slides.html#what-is-data-visualisation",
    "title": "Visualising transport data",
    "section": "What is data visualisation?",
    "text": "What is data visualisation?\n\nData visualization is part art and part science. The challenge is to get the art right without getting the science wrong and vice versa. A data visualization first and foremost has to accurately convey the data.\nIn my experience, scientists frequently (though not always!) know how to visualize data without being grossly misleading. However, they may not have a well developed sense of visual aesthetics, and they may inadvertantly make visual choices that detract from their desired message. Designers, on the other hand, may prepare visualizations that look beautiful but play fast and loose with the data.\n\nSource: Fundamentals of Data Vizualisation"
  },
  {
    "objectID": "s5/slides.html#viz-4-policy",
    "href": "s5/slides.html#viz-4-policy",
    "title": "Visualising transport data",
    "section": "Viz 4 policy",
    "text": "Viz 4 policy\n\n[visualisations] are also often the best way to present the findings of [transport] research in a way that is accessible. [visualisation] making is therefore a critical part of [transport] and its emphasis not only on describing, but also changing the world.\n\nSource: Geocomputation with R"
  },
  {
    "objectID": "s5/slides.html#illustration-of-policy-impact",
    "href": "s5/slides.html#illustration-of-policy-impact",
    "title": "Visualising transport data",
    "section": "Illustration of policy impact",
    "text": "Illustration of policy impact\n\n\nVisualisations can make findings become ‘real’"
  },
  {
    "objectID": "s5/slides.html#a-brief-history-of-geographic-data-viz-in-r",
    "href": "s5/slides.html#a-brief-history-of-geographic-data-viz-in-r",
    "title": "Visualising transport data",
    "section": "A brief history of geographic data viz in R",
    "text": "A brief history of geographic data viz in R\n“The core R engine was not designed specifically for the display and analysis of maps, and the limited interactive facilities it offers have drawbacks in this area” (Bivand, Pebesma, and G’omez-Rubio 2013).\nFive years later…\n“An example showing R’s flexibility and evolving geographic capabilities is leaflet (Cheng, Karambelkar, and Xie 2018), a package for making interactive maps that has been extended by the R community, as we’ll see in Chapter 9” (Lovelace, Nowosad, and Meunchow 2018)."
  },
  {
    "objectID": "s5/slides.html#base-r-graphics-sf",
    "href": "s5/slides.html#base-r-graphics-sf",
    "title": "Visualising transport data",
    "section": "Base R graphics: sf",
    "text": "Base R graphics: sf"
  },
  {
    "objectID": "s5/slides.html#base-r-graphics-sf-ii",
    "href": "s5/slides.html#base-r-graphics-sf-ii",
    "title": "Visualising transport data",
    "section": "Base R graphics: sf II",
    "text": "Base R graphics: sf II"
  },
  {
    "objectID": "s5/slides.html#sf-graphics-code",
    "href": "s5/slides.html#sf-graphics-code",
    "title": "Visualising transport data",
    "section": "sf graphics: code",
    "text": "sf graphics: code\n\n# facet plots by default\nplot(nz) \n# plot just geometry, ready for new layers:\nplot(st_geometry(nz), reset = FALSE)\n# addition of new layers\nplot(nz_height, add = TRUE)\n# transparency\nsf_cols = sf.colors(n = 2, alpha = 0.2)\nnz$col = factor(x = nz$Island, labels = sf_cols)\nplot(st_geometry(nz), col = as.character(nz$col))\n#  see ?plot.sf for more"
  },
  {
    "objectID": "s5/slides.html#observations",
    "href": "s5/slides.html#observations",
    "title": "Visualising transport data",
    "section": "Observations",
    "text": "Observations\n\nFacets by default: useful for seeing patterns.\nTransparency new, add = ... argument the same\nYou can go far with base R graphics (Murrell 2016)."
  },
  {
    "objectID": "s5/slides.html#tmap",
    "href": "s5/slides.html#tmap",
    "title": "Visualising transport data",
    "section": "tmap",
    "text": "tmap\n\nA diverse dedicated mapping R package\n\n\nlibrary(tmap)\ntmap_mode(\"plot\")\ntm_shape(nz) +\n  tm_polygons(\"Median_income\", palette = \"RdYlBu\")"
  },
  {
    "objectID": "s5/slides.html#why-tmap",
    "href": "s5/slides.html#why-tmap",
    "title": "Visualising transport data",
    "section": "Why tmap?",
    "text": "Why tmap?\n\nIt is powerful and flexible.\nConcise syntax, attractive maps with minimal code, familiar to ggplot2 users.\nUnique capability: same code -&gt; static + interactive maps with switch tmap_mode().\nWide range of spatial classes (including raster objects) supported.\nWell documented + developed — see tmap-nutshell and JSS paper (Tennekes 2018)."
  },
  {
    "objectID": "s5/slides.html#tmap-basics",
    "href": "s5/slides.html#tmap-basics",
    "title": "Visualising transport data",
    "section": "tmap basics",
    "text": "tmap basics\n# Add fill layer#|  to nz shape\ntm_shape(nz) + tm_fill() \n# Add border layer to nz shape\ntm_shape(nz) + tm_borders() \n# Add fill and border layers to nz shape\ntm_shape(nz) + tm_fill() + tm_borders()"
  },
  {
    "objectID": "s5/slides.html#animations",
    "href": "s5/slides.html#animations",
    "title": "Visualising transport data",
    "section": "Animations",
    "text": "Animations\n\nAre easy with tmap (section 9.3 of geocompr)"
  },
  {
    "objectID": "s5/slides.html#interactive-maps-with-mapview",
    "href": "s5/slides.html#interactive-maps-with-mapview",
    "title": "Visualising transport data",
    "section": "Interactive maps with mapview",
    "text": "Interactive maps with mapview"
  },
  {
    "objectID": "s5/slides.html#web-mapping-applications",
    "href": "s5/slides.html#web-mapping-applications",
    "title": "Visualising transport data",
    "section": "Web mapping applications",
    "text": "Web mapping applications\n\nLeaflet integrates with shiny via leaflet::leafletOutput(), enabling web mapping applications built on R\nThese can be set-up to scale nationally, as illustrated by pct.bike (Lovelace et al. 2017)."
  },
  {
    "objectID": "s5/slides.html#summary",
    "href": "s5/slides.html#summary",
    "title": "Visualising transport data",
    "section": "Summary",
    "text": "Summary\n\nVisualisation is an important skill in data science\nVisualisation is particularly valuable for evidence-based decision-making and policy\nOpen source command software like R provides powerful tools for data visualisation\nGeographic data visualisation is possible with R using packages such as sf and tmap\nThese visualisation skills will be useful in the lecture next week on project work"
  },
  {
    "objectID": "s5/slides.html#exercise-with-desire-line-data-in-stplanr",
    "href": "s5/slides.html#exercise-with-desire-line-data-in-stplanr",
    "title": "Visualising transport data",
    "section": "Exercise with desire line data in stplanr",
    "text": "Exercise with desire line data in stplanr\n\nCreate a map showing the number of people walking and cycling in the stplanr dataset flowlines_sf using: -base graphics (hint: use plot()) and -tmap (hint: use tm_shape(flowlines_sf) + ..., palette = \"viridis\" and other options shown in tmaptools::palette_explorer() give different colourschemes).\n\nName two advantages of each approach\nBonus: Other mapping packages could be used to show the same data?\n\n\nFor more information on plotting OD data, see the stplanr-od vignetted, e.g. with\nvignette(\"stplanr-od\")"
  },
  {
    "objectID": "s5/slides.html#result-base-graphics",
    "href": "s5/slides.html#result-base-graphics",
    "title": "Visualising transport data",
    "section": "Result: base graphics",
    "text": "Result: base graphics\n\nlibrary(stplanr)\nlwd = flowlines_sf$All / mean(flowlines_sf$All) * 3\nplot(flowlines_sf[\"Taxi\"], lwd = lwd)"
  },
  {
    "objectID": "s5/slides.html#bonus-exercise",
    "href": "s5/slides.html#bonus-exercise",
    "title": "Visualising transport data",
    "section": "Bonus exercise",
    "text": "Bonus exercise\n\nBased on the routes_fast_sf data in the stplanr package, identify roads where investment in cycling is likely to be effective.\n\nStarting point:\n\n\n[1] 42\n\n\n[1] 42"
  },
  {
    "objectID": "s5/slides.html#sf-results",
    "href": "s5/slides.html#sf-results",
    "title": "Visualising transport data",
    "section": "sf results",
    "text": "sf results"
  },
  {
    "objectID": "s5/slides.html#tmap-view-mode-results",
    "href": "s5/slides.html#tmap-view-mode-results",
    "title": "Visualising transport data",
    "section": "tmap ‘view mode’ results",
    "text": "tmap ‘view mode’ results\n See here for result"
  },
  {
    "objectID": "s5/slides.html#exercises-bonus-optional",
    "href": "s5/slides.html#exercises-bonus-optional",
    "title": "Visualising transport data",
    "section": "Exercises bonus (optional)",
    "text": "Exercises bonus (optional)\n\nUsing data in the pct github package, estimate cycling potential in a city of your choice in the UK, and show the results\nSee the pct_training vignette for further information\nWork on your portfolios, adding a new visualisation"
  },
  {
    "objectID": "s5/slides.html#references",
    "href": "s5/slides.html#references",
    "title": "Visualising transport data",
    "section": "References",
    "text": "References\n\n\n\n\nBivand, Roger S., Edzer Pebesma, and Virgilio G’omez-Rubio. 2013. Applied Spatial Data Analysis with R. 2nd ed. 2013 edition. New York: Springer.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2018. Leaflet: Create Interactive Web Maps with the JavaScript ’Leaflet’ Library. https://CRAN.R-project.org/package=leaflet.\n\n\nLovelace, Robin, Anna Goodman, Rachel Aldred, Nikolai Berkoff, Ali Abbas, and James Woodcock. 2017. “The Propensity to Cycle Tool: An Open Source Online System for Sustainable Transport Planning.” Journal of Transport and Land Use 10 (1). https://doi.org/10.5198/jtlu.2016.862.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Meunchow. 2018. Geocomputation with R. CRC Press. http://robinlovelace.net/geocompr.\n\n\nMurrell, Paul. 2016. R Graphics, Second Edition. CRC Press.\n\n\nTennekes, Martijn. 2018. “Tmap: Thematic Maps in R.” Journal of Statistical Software, Articles 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06."
  },
  {
    "objectID": "slides/road-safety.html#welcome",
    "href": "slides/road-safety.html#welcome",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Welcome!",
    "text": "Welcome!\nReproducible data science for road safety research\nRS5C Pre-conference Workshop\n2nd September 2025"
  },
  {
    "objectID": "slides/road-safety.html#agenda",
    "href": "slides/road-safety.html#agenda",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Agenda",
    "text": "Agenda\n\n13:00-14:00 Networking lunch\n14:00-14:15 Introduction to reproducible research\n14:15-14:30 New datasets and tools (Richard Owen, Agilysis)\n14:30-16:00 Workshop\n16:00 onwards Networking and walk to RS5C reception"
  },
  {
    "objectID": "slides/road-safety.html#prerequisites",
    "href": "slides/road-safety.html#prerequisites",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo run the code\n\nComputer to run the code\n\nEither: A laptop with R, RStudio or VS Code and Docker or similar installed to run the code locally\nOr: Access to a cloud-based environment for data science (e.g., GitHub Codespaces or Posit Cloud)"
  },
  {
    "objectID": "slides/road-safety.html#learn-and-share",
    "href": "slides/road-safety.html#learn-and-share",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Learn and share",
    "text": "Learn and share\nThe following will help:\n\nAn interest in road safety and knowledge of road traffic casualty datasets\nA willingness to learn and share (LinkedIn, BlueSky, etc)\nA GitHub account (to ask questions on the Discussions page and share your own code)\nFamiliarity with data science tools, e.g. R, Python, RStudio, VS Code"
  },
  {
    "objectID": "slides/road-safety.html#housekeeping",
    "href": "slides/road-safety.html#housekeeping",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nConnect to the UoL-Guest Wi-Fi network and enter your details.\nGitHub account sign-up if not done already.\nR and RStudio installation check, locally or in cloud environment."
  },
  {
    "objectID": "slides/road-safety.html#wifi",
    "href": "slides/road-safety.html#wifi",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "WiFi",
    "text": "WiFi"
  },
  {
    "objectID": "slides/road-safety.html#setup-check",
    "href": "slides/road-safety.html#setup-check",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Setup check",
    "text": "Setup check\nTo check you have the necessary software installed, try running the following code.\n\npkgs = c(\"tidyverse\", \"stats19\")\nif (!requireNamespace(\"pak\", quietly = TRUE)) install.packages(\"pak\")\npak::pkg_install(pkgs)\n\nYou should be able to generate the map on the next slide."
  },
  {
    "objectID": "slides/road-safety.html#setup-check-the-result",
    "href": "slides/road-safety.html#setup-check-the-result",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Setup check: The result",
    "text": "Setup check: The result"
  },
  {
    "objectID": "slides/road-safety.html#why-are-we-doing-this",
    "href": "slides/road-safety.html#why-are-we-doing-this",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?"
  },
  {
    "objectID": "slides/road-safety.html#workshop-tasks",
    "href": "slides/road-safety.html#workshop-tasks",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Workshop Tasks",
    "text": "Workshop Tasks\n\nImporting collision, casualty and vehicle tables (20 min)\nTemporal visualisation and aggregation (20 min)\nSpatial visualisation and aggregation (30 min)\nJoining STATS19 tables (20 min)"
  },
  {
    "objectID": "slides/road-safety.html#task-1-importing-tables-20-min",
    "href": "slides/road-safety.html#task-1-importing-tables-20-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 1: Importing tables (20 min)",
    "text": "Task 1: Importing tables (20 min)\n\nLearn how to load the main STATS19 tables (collision, casualty, vehicle) using the stats19 R package.\nExplore the structure and key variables in each table.\n\nSee Chapter 4 and Chapter 8 for details."
  },
  {
    "objectID": "slides/road-safety.html#task-2-temporal-visualisation-20-min",
    "href": "slides/road-safety.html#task-2-temporal-visualisation-20-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 2: Temporal visualisation (20 min)",
    "text": "Task 2: Temporal visualisation (20 min)\n\nAggregate collision data by time (e.g., by month or day of week).\nCreate time series plots to identify trends and patterns.\n\nSee Chapter 6 on temporal data."
  },
  {
    "objectID": "slides/road-safety.html#task-3-spatial-visualisation-30-min",
    "href": "slides/road-safety.html#task-3-spatial-visualisation-30-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 3: Spatial visualisation (30 min)",
    "text": "Task 3: Spatial visualisation (30 min)\n\nConvert collision data to spatial format and plot on a map.\nAggregate collisions by area (e.g., by local authority).\nCreate maps to visualise spatial patterns.\n\nSee Chapter 7 on spatial data."
  },
  {
    "objectID": "slides/road-safety.html#task-4-joining-tables-20-min",
    "href": "slides/road-safety.html#task-4-joining-tables-20-min",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Task 4: Joining tables (20 min)",
    "text": "Task 4: Joining tables (20 min)\n\nJoin collision, casualty, and vehicle tables to enrich your analysis.\nExplore relationships between different aspects of road traffic incidents.\n\nSee Chapter 8 on joining tables."
  },
  {
    "objectID": "slides/road-safety.html#bonus-tasks",
    "href": "slides/road-safety.html#bonus-tasks",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Bonus tasks",
    "text": "Bonus tasks\nFor fast finishers or anyone wanting to go the extra mile:\n\nCreate a repo and share your work on GitHub.\nReproduce a map used in a Leeds City Council consultation.\nAnalyse data to answer a new research question.\nContribute upstream to an open source road safety project."
  },
  {
    "objectID": "slides/road-safety.html#output-youll-be-making-in-leeds-focussed-bonus-task",
    "href": "slides/road-safety.html#output-youll-be-making-in-leeds-focussed-bonus-task",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Output you’ll be making in Leeds-focussed bonus task",
    "text": "Output you’ll be making in Leeds-focussed bonus task"
  },
  {
    "objectID": "slides/road-safety.html#the-logic-of-scientific-discovery",
    "href": "slides/road-safety.html#the-logic-of-scientific-discovery",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "The Logic of Scientific Discovery",
    "text": "The Logic of Scientific Discovery\n\nCore Idea: Science can’t prove theories true, only prove them false.\nFalsifiability: A scientific theory must be testable and able to be disproven.\nHow Science Progresses: Through conjecture (proposing theories) and refutation (trying to prove them wrong).\n\nGoal: Not to find “truth,” but to eliminate “untruth.”"
  },
  {
    "objectID": "slides/road-safety.html#why-reproducibility-is-needed-for-your-work-to-be-scientific",
    "href": "slides/road-safety.html#why-reproducibility-is-needed-for-your-work-to-be-scientific",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Why reproducibility is needed for your work to be scientific",
    "text": "Why reproducibility is needed for your work to be scientific\n\nThe Test of Falsification: For a finding to be truly scientific, it must be reproducible.\nRole of Replication: A failed replication is not a failure of the researcher, but a successful attempt at falsification.\nOpen Science: Popper’s ideas champion the need for transparency (open methods, data, code) so others can perform the critical tests needed to advance science."
  },
  {
    "objectID": "slides/road-safety.html#stages-of-open-and-reproducible-science",
    "href": "slides/road-safety.html#stages-of-open-and-reproducible-science",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Stages of open and reproducible science",
    "text": "Stages of open and reproducible science\n\n\n\nOpen access to the publications\nOpen access to sample (synthetic if sensitive) data\nOpen access to the code\nFully reproducible paper published with documentation\nProject deployed in tool for non-specialist use"
  },
  {
    "objectID": "slides/road-safety.html#what-can-you-do-with-reproducible-research",
    "href": "slides/road-safety.html#what-can-you-do-with-reproducible-research",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "What can you do with reproducible research?",
    "text": "What can you do with reproducible research?\n\nValidate Findings: Others can replicate your work to confirm results.\nBuild on Existing Work: Researchers can use your methods and data to explore new questions.\nIncrease Trust: Transparency in research enhances credibility and trustworthiness.\nFacilitate Collaboration: Openly shared resources promote teamwork and knowledge exchange."
  },
  {
    "objectID": "slides/road-safety.html#example-1-increasing-inequalities-in-cycling-casualties",
    "href": "slides/road-safety.html#example-1-increasing-inequalities-in-cycling-casualties",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Example 1: Increasing inequalities in cycling casualties",
    "text": "Example 1: Increasing inequalities in cycling casualties\n\n(Vidal Tortosa et al. 2021)"
  },
  {
    "objectID": "slides/road-safety.html#thank-you",
    "href": "slides/road-safety.html#thank-you",
    "title": "Reproducible Data Science for Road Safety Research",
    "section": "Thank you!",
    "text": "Thank you!\n\nQuestions? Get in touch via email or GitHub Discussions.\nSee the course homepage at https://itsleeds.github.io/tds/reproducible-road-safety-workshop\nInterested in more teaching/research opportunities? See the upcoming course on 18th to 19th September and get in touch!\nOver to Richard Owen (Agylisis) for next presentation before practical\n\n\n\n\n\nTait, Caroline, Roger Beecham, Robin Lovelace, and Stuart Barber. 2023. “Contraflows and Cycling Safety: Evidence from 22 Years of Data Involving 508 One-Way Streets.” Accident Analysis & Prevention 179 (January): 106895. https://doi.org/10.1016/j.aap.2022.106895.\n\n\nVidal Tortosa, Eugeni, Robin Lovelace, Eva Heinen, and Richard P. Mann. 2021. “Socioeconomic Inequalities in Cycling Safety: An Analysis of Cycling Injury Risk by Residential Deprivation Level in England.” Journal of Transport & Health 23 (December): 101291. https://doi.org/10.1016/j.jth.2021.101291."
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Introduction to Transport Data Science"
  },
  {
    "objectID": "minihack-transport-data.html",
    "href": "minihack-transport-data.html",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "This event is designed to build data, coding and reproducible research skills for Institute for Transport Studies staff and students. Please contact the organisers (Robin Lovelace) if you are not based at the University of Leeds and would like to join in. It will take place on the 8th May as a pre-event before a lecture on data science and is open to staff and students at the University of Leeds and Transport professions. See here to sign-up.\n\n\n\nReactivate the Transport Data Science Hackathons\nFacilitate learning and collaboration among participants\nOutcomes for participants:\n\nLearning basics of packaging and modular coding\nData wrangling with tidyverse\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nDemonstrate the potential of open data (transparency, participation, research) and reproducible/open work-flows.\n\n\n\n\n\n\nNone: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details\n\n\n\n\n\n13:00 - 13:30: Presentation of the challenges\n\nTransmilenio: Victor Cantillo García\nBring your own data (BYD)\n\n5 minute pitches by anyone who wants to work on their own data challenge\n\n\n13:30 - 14:00: Importing the data\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: The hackathon\n15:15 - 15:45: Presentation of the results\n15:45 onwards: Networking and lecture (optional, see ticketsource.us for tickets)\n\n\n\n\nThe prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#objectives",
    "href": "minihack-transport-data.html#objectives",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "Reactivate the Transport Data Science Hackathons\nFacilitate learning and collaboration among participants\nOutcomes for participants:\n\nLearning basics of packaging and modular coding\nData wrangling with tidyverse\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nDemonstrate the potential of open data (transparency, participation, research) and reproducible/open work-flows.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#prerequisites",
    "href": "minihack-transport-data.html#prerequisites",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "None: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#schedule",
    "href": "minihack-transport-data.html#schedule",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "13:00 - 13:30: Presentation of the challenges\n\nTransmilenio: Victor Cantillo García\nBring your own data (BYD)\n\n5 minute pitches by anyone who wants to work on their own data challenge\n\n\n13:30 - 14:00: Importing the data\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: The hackathon\n15:15 - 15:45: Presentation of the results\n15:45 onwards: Networking and lecture (optional, see ticketsource.us for tickets)",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#prizes",
    "href": "minihack-transport-data.html#prizes",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "",
    "text": "The prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#transmilenio-data",
    "href": "minihack-transport-data.html#transmilenio-data",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "2.1 Transmilenio data",
    "text": "2.1 Transmilenio data\n\nTransMilenio (TM) is the organisation in charge of managing all components of Bogotá’s integrated public transport system.\nTM publishes a lot of their data for public use, in line with the open data policy of Bogotá.\nData includes:\n\nSpatial: GTFS, station location and lines of BRT, regular buses, and cable lines.\nCounts: Raw daily tap-in records, and aggregated boarding / alighting and exit counts by station and 15 minutes interval.\n\n\n\n2.1.1 Motivation:\n\nTM published some useful maps but they are not easily reproducible.\nAccessing the data is not straightforward as the count information is saved in individual .csv files by day.\n\n\n\n2.1.2 Goal:\n\nDevelop a set of functions that can be integrated into an R library to access and analyse the open data published by TM.",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#origin-destination-data",
    "href": "minihack-transport-data.html#origin-destination-data",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "2.2 Origin destination data",
    "text": "2.2 Origin destination data\n\nSee https://github.com/itsleeds/2021-census-od-data for 2021 OD data from the Census",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "minihack-transport-data.html#bring-your-own-data",
    "href": "minihack-transport-data.html#bring-your-own-data",
    "title": "Transport Data Minihack: Data Challenges",
    "section": "2.3 Bring your own data",
    "text": "2.3 Bring your own data\nParticipants are welcome to bring their own data to the event. Please mention the dataset in the sign-up form (see link above).",
    "crumbs": [
      "Transport Data Minihack 8th May"
    ]
  },
  {
    "objectID": "sem2/index.html",
    "href": "sem2/index.html",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The slides and high-resolution versions of some images from the talk are available here\n\n\n\n\nThe railway is international, noting the location of the United Kingdom\n\n\n\n\nThe operation of railway infrastructure in Britain is organised into thirteen routes and six regions.\n\nThis diagram show four routes that make up the Eastern region."
  },
  {
    "objectID": "sem2/index.html#make-things-as-simple-as-possiblebut-no-simpler",
    "href": "sem2/index.html#make-things-as-simple-as-possiblebut-no-simpler",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The slides and high-resolution versions of some images from the talk are available here"
  },
  {
    "objectID": "sem2/index.html#the-global-railway",
    "href": "sem2/index.html#the-global-railway",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The railway is international, noting the location of the United Kingdom"
  },
  {
    "objectID": "sem2/index.html#the-british-railway",
    "href": "sem2/index.html#the-british-railway",
    "title": "Seminar 2 -",
    "section": "",
    "text": "The operation of railway infrastructure in Britain is organised into thirteen routes and six regions.\n\nThis diagram show four routes that make up the Eastern region."
  },
  {
    "objectID": "sem2/index.html#logical-network-model",
    "href": "sem2/index.html#logical-network-model",
    "title": "Seminar 2 -",
    "section": "2.1 Logical Network Model",
    "text": "2.1 Logical Network Model\nThe timetable production process can be modeled as a series of processes and interactions between railway operators and infrastructure manager."
  },
  {
    "objectID": "sem2/index.html#logical-operations-model",
    "href": "sem2/index.html#logical-operations-model",
    "title": "Seminar 2 -",
    "section": "2.2 Logical Operations Model",
    "text": "2.2 Logical Operations Model\nThe operation production process can be modeled as series of processes and data exchange."
  },
  {
    "objectID": "sem2/index.html#green-house-gases-co2",
    "href": "sem2/index.html#green-house-gases-co2",
    "title": "Seminar 2 -",
    "section": "3.1 Green House Gases (CO2)",
    "text": "3.1 Green House Gases (CO2)\nUK produced 455 million tonnes CO2 equivalent (mtCO2e) in 2019\nTransport accounted for 122 mtCO2e (26.8%) of which\n\nHGV 19.5 mtCO2e (16.0%/4.2%)\nAll rail 1.7 mtCO2e (1.4%/0.04%)\nA freight train removes up to 76 HGVs from our roads"
  },
  {
    "objectID": "sem2/index.html#national-greenhouse-emissions-1990-2020",
    "href": "sem2/index.html#national-greenhouse-emissions-1990-2020",
    "title": "Seminar 2 -",
    "section": "3.2 National Greenhouse Emissions 1990-2020",
    "text": "3.2 National Greenhouse Emissions 1990-2020"
  },
  {
    "objectID": "sem2/index.html#transport-greenhouse-emissions-1990-2020",
    "href": "sem2/index.html#transport-greenhouse-emissions-1990-2020",
    "title": "Seminar 2 -",
    "section": "3.3 Transport Greenhouse Emissions 1990-2020",
    "text": "3.3 Transport Greenhouse Emissions 1990-2020\n\n(from DfT Transport Statistics (2021))"
  },
  {
    "objectID": "sem2/index.html#system-visualisation",
    "href": "sem2/index.html#system-visualisation",
    "title": "Seminar 2 -",
    "section": "4.1 System Visualisation",
    "text": "4.1 System Visualisation\nAn Open-TrainTimes (OTT) schematic view of Leeds Stations\n\n\nOpenTrainTimes: here\nNational Rail Enquiries here\nRealtime Trains here"
  },
  {
    "objectID": "sem2/index.html#operation-railway",
    "href": "sem2/index.html#operation-railway",
    "title": "Seminar 2 -",
    "section": "4.2 Operation Railway",
    "text": "4.2 Operation Railway\nThe view of live operational railway systems."
  },
  {
    "objectID": "sem2/index.html#the-power-of-social-media",
    "href": "sem2/index.html#the-power-of-social-media",
    "title": "Seminar 2 -",
    "section": "6.1 The Power of Social Media",
    "text": "6.1 The Power of Social Media\nThe benefit of positive social media interaction."
  },
  {
    "objectID": "sem2/index.html#the-power-of-open-data",
    "href": "sem2/index.html#the-power-of-open-data",
    "title": "Seminar 2 -",
    "section": "6.2 The Power of Open Data",
    "text": "6.2 The Power of Open Data\nAll data used is on the basis that it under open or permissive license.\n\nThe base map of mainland Britain is derived from the WorldPop base maps under CC 4.0 by deed retrieved 2023-09-07.\nThe centre-line track-model is hosted by OpenRailData under the Open Government License(OGL) by Network Rail, retrieved 2023-07-11.\nThe Origin Destination Matrix data, for example ODM 2022-23, were published by the Office of Road and Rail on the Rail Development Group Rail Data Marketplace, under the OGL. Retrieved 2024-02-18.\nThe Station Attributes for All-Mainline Stations published by the Office of Road and Rail under the OGL. Retrieved 2024-02-18.\nThe Network Rail CORPUS dataset is an open data feed which is released under the OGL. Retrieved 2023-11-29 as a local copy.\nThe National Public Transport Access Network (NaPTAN) under the OGL and is updated each time the scripts are run.\nWhile this implementation now uses NaPTAN and CORPUS to validate and identify six closed stations, the Isle of Wight ferry-link continues to use OpenStreetMap data, licensed under CC-BY-SA 2.0 through the OverPassAPI Turbo service, and is updated each time the scripts are run."
  },
  {
    "objectID": "sem2/index.html#seconds-of-fame",
    "href": "sem2/index.html#seconds-of-fame",
    "title": "Seminar 2 -",
    "section": "6.3 15 Seconds of Fame",
    "text": "6.3 15 Seconds of Fame\nThe original 2018/19 visualisation recieved 500k X/Twitter views within a week, as well as an OpenInnovations blog post and a Bloomberg article Nine Maps Show How Britain Is on the Move.\nAll code and data is published on GitHub under my anisotropi4/kingfisher repository and updated with the ORR publication of the additional 2020-2024 financial year data."
  },
  {
    "objectID": "sem2/index.html#simple-visualisations",
    "href": "sem2/index.html#simple-visualisations",
    "title": "Seminar 2 -",
    "section": "6.4 Simple Visualisations",
    "text": "6.4 Simple Visualisations\nThe squares and coolest countries in Europe."
  },
  {
    "objectID": "sem2/index.html#shortest-path-census-flow",
    "href": "sem2/index.html#shortest-path-census-flow",
    "title": "Seminar 2 -",
    "section": "7.1 Shortest path census flow",
    "text": "7.1 Shortest path census flow"
  },
  {
    "objectID": "sem2/index.html#output-area-oa-census-2011-centroid",
    "href": "sem2/index.html#output-area-oa-census-2011-centroid",
    "title": "Seminar 2 -",
    "section": "7.2 Output Area (OA) Census 2011 centroid",
    "text": "7.2 Output Area (OA) Census 2011 centroid"
  },
  {
    "objectID": "sem2/index.html#full-oa-2011-delaunay-mesh",
    "href": "sem2/index.html#full-oa-2011-delaunay-mesh",
    "title": "Seminar 2 -",
    "section": "7.3 Full OA 2011 Delaunay mesh",
    "text": "7.3 Full OA 2011 Delaunay mesh"
  },
  {
    "objectID": "sem2/index.html#oa-2011-odm-mesh",
    "href": "sem2/index.html#oa-2011-odm-mesh",
    "title": "Seminar 2 -",
    "section": "7.4 OA 2011 ODM mesh",
    "text": "7.4 OA 2011 ODM mesh"
  },
  {
    "objectID": "sem2/index.html#network-simplification-and-parenx",
    "href": "sem2/index.html#network-simplification-and-parenx",
    "title": "Seminar 2 -",
    "section": "7.5 Network simplification and parenx",
    "text": "7.5 Network simplification and parenx\nThe network merge paper is here.\n\nThe parenx PyPi module.\n\n7.5.1 parenx cookbook\nThe parenx cookbook shows examples of how to use the parenx library."
  },
  {
    "objectID": "sem2/index.html#make-it-look-good.",
    "href": "sem2/index.html#make-it-look-good.",
    "title": "Seminar 2 -",
    "section": "8.1 Make it look good.",
    "text": "8.1 Make it look good.\n\nArt E-coli statue."
  },
  {
    "objectID": "sem2/index.html#the-railway",
    "href": "sem2/index.html#the-railway",
    "title": "Seminar 2 -",
    "section": "8.2 The Railway",
    "text": "8.2 The Railway\nBeeching 1963 and today."
  },
  {
    "objectID": "s1/index.html",
    "href": "s1/index.html",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "",
    "text": "Lecture: an introduction to Transport Data Science (30 min)\n\nSee the slides\n\nQ&A (15 min) \nBreak and networking (15 min) \nData science and a good research question (30 min)\nData science foundations (guided): Project set-up and using RStudio or VS Code as an integrated development environment (30 min)\nFocussed work (1 hr)"
  },
  {
    "objectID": "s1/index.html#agenda",
    "href": "s1/index.html#agenda",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "",
    "text": "Lecture: an introduction to Transport Data Science (30 min)\n\nSee the slides\n\nQ&A (15 min) \nBreak and networking (15 min) \nData science and a good research question (30 min)\nData science foundations (guided): Project set-up and using RStudio or VS Code as an integrated development environment (30 min)\nFocussed work (1 hr)"
  },
  {
    "objectID": "s1/index.html#how-to-come-up-with-a-good-research-question",
    "href": "s1/index.html#how-to-come-up-with-a-good-research-question",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "How to come up with a good research question",
    "text": "How to come up with a good research question\n\nThink about the data you have access to\nThink about the problems you want to solve\nThink about the methods you want to use and skills you want to learn\nThink about how the final report will look and hold-together\n\n\nHow much potential is there for cycling across the transport network?\n\n\n\nHow can travel to schools be made safer?\n\n\nHow can hospitals encourage visitors to get there safely?\n\n\nWhere’s the best place to build electric car charging points?\nSee openstreetmap.org or search for other open access datasets for more ideas"
  },
  {
    "objectID": "s1/index.html#data-object-manipulation-basics",
    "href": "s1/index.html#data-object-manipulation-basics",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "1.1 Data object manipulation basics",
    "text": "1.1 Data object manipulation basics\n\nUse the $ operator to print the vehicle_type column of crashes.\n\n\nIn R the $ symbol is used to refer to elemements of a list. So the answer is simply:\n\ncrashes$vehicle_type\n\n[1] \"car\"  \"bus\"  \"tank\"\n\n\n\n\nSubset the crashes with the [,] syntax\n\n\nTry out different combinations on the dataframe crashes to see what happens. For example, try:\n\ncrashes[1,]\n\n  casualty_type casualty_age vehicle_type\n1    pedestrian           20          car\n\ncrashes[,1]\n\n[1] \"pedestrian\" \"cyclist\"    \"cat\"       \n\ncrashes[1,1]\n\n[1] \"pedestrian\"\n\n\n\nSubset the object with the [[ syntax.\n\n\nThe [[ operator is used to extract elements from a list. Try:\n\ncrashes[[1]]\n\n[1] \"pedestrian\" \"cyclist\"    \"cat\"       \n\ncrashes[[2]]\n\n[1] 20 40 60\n\n\n\n\nBonus: what is the class() of the objects created by each of the previous exercises?\n\n\nExplore how many R classes you can find\n\n\nBonus (advanced): reproduce the above with Python using the pandas or polars package"
  },
  {
    "objectID": "s1/index.html#data-science-on-real-data",
    "href": "s1/index.html#data-science-on-real-data",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "1.2 Data science on real data",
    "text": "1.2 Data science on real data\nTo get some larger datasets, try the following (from Chapter 8 of RSRR)\n\nRPython\n\n\n\nremotes::install_cran(\"stats19\")\nlibrary(stats19)\nac = get_stats19(year = 2020, type = \"collision\")\nca = get_stats19(year = 2020, type = \"cas\")\nve = get_stats19(year = 2020, type = \"veh\")\n# population hurt by road traffic collisions in 2020:\n(nrow(ca) / 67e6) * 100\n\n[1] 0.1725134\n\n\n\n\nChallenge: reproduce the above code in Python using the pystats19 package\n# Install the package, e.g. with pip\n!pip install pystats19\nimport pystats19\n# See the documentation at https://github.com/Mayazure/py-stats19\n\n\n\nLet’s go through these exercises together:\n\nSubset the casualty_age object using the inequality (&lt;) so that only elements less than 50 are returned.\nSubset the crashes data frame so that only tanks are returned using the == operator.\nBonus: assign the age of all tanks to 61.\n\n\nTry running the subsetting code on a larger dataset, e.g. the ac object created previously\n\n\nCoerce the vehicle_type column of crashes to the class character.\nCoerce the crashes object into a matrix. What happened to the values?\nBonus: What is the difference between the output of summary() on character and factor variables?\n\n\nWe’ll explore this together"
  },
  {
    "objectID": "s1/index.html#bonus-data-science-and-transport",
    "href": "s1/index.html#bonus-data-science-and-transport",
    "title": "Session 1: Introduction to Transport Data Science",
    "section": "2.1 Bonus: data science and transport",
    "text": "2.1 Bonus: data science and transport\n\nWork through Chapter 13 of the book Geocomputation with R, taking care to ask questions about any aspects that you don’t understand (your homework will be to complete and make notes on the chapter, including reproducible code)."
  },
  {
    "objectID": "sem1/index.html",
    "href": "sem1/index.html",
    "title": "Seminar 1 - Mini-workshop",
    "section": "",
    "text": "The best way to learn is by exploring data and answering your own questions. Here are some datasets that can help you investigate questions like:"
  },
  {
    "objectID": "sem1/index.html#motorised-vehicles-counts-leeds",
    "href": "sem1/index.html#motorised-vehicles-counts-leeds",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.1 Motorised vehicles counts: Leeds",
    "text": "1.1 Motorised vehicles counts: Leeds\nMany cities/countries publish data from permanent traffic counters e.g. ANPR cameras, induction loops or low-cost sensors. We are going to use data from the sensors in Leeds (available in Data Mill North)\n\nleeds_car_location &lt;- read_csv(\n  \"https://datamillnorth.org/download/e6q0n/9bc51361-d98e-47d3-9963-aeeca3fa0afc/Camera%20Locations.csv\"\n  ) \n\nleeds_car_location_sf &lt;- leeds_car_location |&gt; \n  st_as_sf(coords = c(\"X\",\"Y\"),\n           crs = 27700)\n\n\nleeds_car_2019 &lt;- read_csv(\n  \"https://datamillnorth.org/download/e6q0n/9e62c1e5-8ba5-4369-9d81-a46c4e23b9fb/Data%202019.csv\"\n  )\n\nIf you are interested in open traffic count datasets see this\n\n1.1.0.1 code\n\nleeds_car_2019 |&gt; \n  group_by(Cosit) |&gt; \n  summarise(mean(Volume))\n\n\nmean_daily_volumes &lt;- leeds_car_2019 |&gt;\n  # converting cosit to numeric\n  mutate(Cosit = as.numeric(Cosit)) |&gt; \n  # extracting the date\n  mutate(time_date = dmy_hm(Sdate),\n         # extracts the day\n         date = date(time_date)) |&gt; \n  # calculating the total flows for each day\n  summarise(Volume = sum(Volume,rm.na = T),\n            .by = c(date,Cosit)) |&gt; \n  # Calculating the daily mean \n  summarise(daily_volume = mean(Volume,rm.na = T),\n            .by = Cosit) \n\n\ndaily_volumes &lt;- leeds_car_2019 |&gt; \n    # converting cosit to numeric\n  mutate(Cosit = as.numeric(Cosit)) |&gt;\n  # extracting the date\n  mutate(time_date = dmy_hm(Sdate),\n         # extracts the day\n         date = date(time_date)) |&gt; # calculating the total flows for each day\n  summarise(mean_volume = sum(Volume,rm.na = T),\n          .by = c(date,Cosit))\n\ndaily_volumes |&gt; \n  mutate(Cosit = as.numeric(Cosit)) |&gt; \n  filter(Cosit == 90201)|&gt; \n  ggplot(aes(x = date,y = mean_volume))+\n  geom_line()\n  \n\nmean_daily_volumes |&gt; \n  ggplot(aes(daily_volume))+\n  geom_histogram()\n\nleeds_car_location_sf |&gt;\n  left_join(mean_daily_volumes,by = c(\"Site ID\"=\"Cosit\")) |&gt; \n  tm_shape()+\n  tm_dots(\"daily_volume\",size = \"daily_volume\")"
  },
  {
    "objectID": "sem1/index.html#cycle-counts-for-west-yorkshire",
    "href": "sem1/index.html#cycle-counts-for-west-yorkshire",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.2 Cycle counts for West Yorkshire",
    "text": "1.2 Cycle counts for West Yorkshire\nSome cities would have some dedicated infrastructure to count the number of people using bikes at strategic points of the city. We are going to use some cycle counters from West Yorkshire that you can find here:\n\nleeds_bike_location &lt;- read_csv(\n  \"https://datamillnorth.org/download/e1dmk/a8c8a11e-1616-4915-a897-9ca5ab4e03b8/Cycle%20Counter%20Locations.csv\",skip = 1\n  ) \n\nleeds_bike_location_sf &lt;- leeds_bike_location |&gt;\n  drop_na(Latitude,Longitude) |&gt; \n  st_as_sf(coords = c(\"Longitude\",\"Latitude\"),\n           crs = 4326) |&gt; \n  st_transform(27700)\n\nThe data for 2019:\n\nleeds_bike_2019 &lt;- read_csv(\n  \"https://datamillnorth.org/download/e1dmk/f13f5d49-6128-4619-a3ff-e6e12f88a71f/Cycle%20Data%202019.csv\"\n  )\n\nOther interesting datasets for you to explore are Paris cycling counters or Scotland."
  },
  {
    "objectID": "sem1/index.html#pedestrian-counts-melbourne",
    "href": "sem1/index.html#pedestrian-counts-melbourne",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.3 Pedestrian Counts: Melbourne",
    "text": "1.3 Pedestrian Counts: Melbourne\nCities also monitor the number pedestrians in key locations. We can use data from the sensors in Melbourne accessible here:\n\nmelbourne_locations_sf &lt;- st_read(\"https://data.melbourne.vic.gov.au/api/explore/v2.1/catalog/datasets/pedestrian-counting-system-sensor-locations/exports/geojson?lang=en&timezone=Europe%2FLondon\")\n\nWe will extract\n\nmelbourne_dec2024 &lt;- read_csv(\"https://data.melbourne.vic.gov.au/api/explore/v2.1/catalog/datasets/pedestrian-counting-system-monthly-counts-per-hour/exports/csv?lang=en&refine=sensing_date%3A%222024%2F12%22&timezone=Australia%2FMelbourne&use_labels=true&delimiter=%2C\")"
  },
  {
    "objectID": "sem1/index.html#public-transport-tap-in-data-bogotá",
    "href": "sem1/index.html#public-transport-tap-in-data-bogotá",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.4 Public transport tap-in data: Bogotá",
    "text": "1.4 Public transport tap-in data: Bogotá\nPublic transport ridership data can be difficult to obtain. Fortunately, some cities which have systems managed by a public organisation make this data available for the public. Bogotá’s integrated transport system publishes the tap-in data for the BRT system (see this). We will use one of the daily reports.\n\ntm_stations_sf &lt;- st_read(\"Estaciones_Troncales_de_TRANSMILENIO.geojson\")\n\nMonthly boarding data can be manually obtained in the open data portal of TransMilenio here\n\nurl_tm &lt;- \"https://storage.googleapis.com/validaciones_tmsa/ValidacionTroncal/2024/consolidado_2024.zip\"\nu_bn &lt;- basename(url_tm)\n\n\nif(!file.exists(u_bn)){\n  download.file(url = url_tm,\n                destfile = u_bn,\n                mode = \"wb\")\n}\n\n\n\nurl_tm &lt;- \"https://storage.googleapis.com/validaciones_tmsa/ValidacionTroncal/2024/consolidado_2024.zip\"\n\n\ntm_brt_2024 &lt;- read_csv(unz(u_bn,\"troncal_2024.csv\"))\n\nTfL’s crowding data is also a great source of ridership data. See this.\n\n1.4.1 code\n\ndaily_tapins &lt;- tm_brt_2024 |&gt; \n  summarise(validaciones = sum(validaciones),\n            .by = c(Estacion_Parada,fecha)) |&gt; \n  summarise(validaciones = mean(validaciones),\n            .by = Estacion_Parada) |&gt; \n  mutate(numero_estacion = str_extract(Estacion_Parada,\"\\\\(\\\\d*\\\\)\") |&gt; \n           str_remove_all(\"(\\\\(|\\\\))\")) \n  \ntm_stations_sf |&gt; \n  left_join(daily_tapins,by = \"numero_estacion\") |&gt; \n  tm_shape()+\n  tm_dots(\"validaciones\",size = \"validaciones\")"
  },
  {
    "objectID": "sem1/index.html#network-data-from-osm",
    "href": "sem1/index.html#network-data-from-osm",
    "title": "Seminar 1 - Mini-workshop",
    "section": "1.5 Network data from OSM",
    "text": "1.5 Network data from OSM\nYou may be already familiar with getting and using OSM data. This an example of how to obtain the network that can be used for pedestrians.\n\nmy_coordinates &lt;- c(-76.78893552474851,18.01206727612776)\nsf_point &lt;- st_point(my_coordinates) |&gt; st_sfc(crs = 4326)\nsf_buffer &lt;- st_buffer(sf_point,dist = 15e3)\n\n\ntm_basemap(\"OpenStreetMap\")+\n  tm_shape(sf_buffer)+\n  tm_borders()\n\n\nmy_network &lt;- oe_get_network(sf_buffer, mode = \"walking\")\n\n\ntm_shape(my_network)+\n  tm_lines(\"highway\")\n\nNote: you can access a simplified network dataset from Ordnance Survey’s OpenRoads dataset."
  },
  {
    "objectID": "s6/index.html",
    "href": "s6/index.html",
    "title": "Session 6: Joins and aggregations",
    "section": "",
    "text": "In this session, we will explore techniques for joining, combining and aggregating datasets in R, particularly for spatial data. We’ll work with road crash data (STATS19), Lower Super Output Area (LSOA) boundaries, and census population data for West Yorkshire. Through this session, you’ll learn:\n\nHow to perform spatial joins between point and polygon data\nHow to join tables using common identifiers (key-based joins)\nHow to calculate and visualize derived metrics from joined datasets\n\nThe skills developed in this session are essential for transport data scientists who often need to combine data from various sources to gain comprehensive insights.\n\n\nFirst, let’s load the libraries we’ll need for this session:\n\n# Load required libraries\nlibrary(tidyverse)  # Data manipulation and visualisation\nlibrary(sf)         # Simple features for spatial data\nlibrary(stats19)    # Package for road crash data\nlibrary(tmap)       # Thematic mapping"
  },
  {
    "objectID": "s6/index.html#required-libraries",
    "href": "s6/index.html#required-libraries",
    "title": "Session 6: Joins and aggregations",
    "section": "",
    "text": "First, let’s load the libraries we’ll need for this session:\n\n# Load required libraries\nlibrary(tidyverse)  # Data manipulation and visualisation\nlibrary(sf)         # Simple features for spatial data\nlibrary(stats19)    # Package for road crash data\nlibrary(tmap)       # Thematic mapping"
  },
  {
    "objectID": "s6/index.html#downloading-stats19-crash-data",
    "href": "s6/index.html#downloading-stats19-crash-data",
    "title": "Session 6: Joins and aggregations",
    "section": "2.1 Downloading STATS19 crash data",
    "text": "2.1 Downloading STATS19 crash data\nWe’ll begin by downloading road crash data for four years (2019-2022) using the stats19 package:\n\n# Download STATS19 crash data for 2019-2023\n# We're downloading data for multiple years to have a robust dataset\ncrashes_2019 = get_stats19(year = 2019, type = \"accidents\", ask = FALSE)\ncrashes_2020 = get_stats19(year = 2020, type = \"accidents\", ask = FALSE)\ncrashes_2021 = get_stats19(year = 2021, type = \"accidents\", ask = FALSE)\ncrashes_2022 = get_stats19(year = 2022, type = \"accidents\", ask = FALSE)\ncrashes_2023 = get_stats19(year = 2023, type = \"accidents\", ask = FALSE)\n\nThe get_stats19() function downloads the crash data. The parameters used are:\n\nyear: Specifies which year’s data to download\ntype: Selects the type of data (accidents, vehicles, or casualties)\nask = FALSE: Automatically downloads without prompting for confirmation"
  },
  {
    "objectID": "s6/index.html#combining-multiple-years-of-data",
    "href": "s6/index.html#combining-multiple-years-of-data",
    "title": "Session 6: Joins and aggregations",
    "section": "2.2 Combining multiple years of data",
    "text": "2.2 Combining multiple years of data\nOnce we have the individual year datasets, we can combine them into a single dataframe using bind_rows():\n\n# Combine all years into one dataset\n# This creates a unified dataset for analysis across the full time period\ncrashes = bind_rows(crashes_2019, crashes_2020, crashes_2021, crashes_2022, crashes_2023)\n\n# Let's check the dimensions of our combined dataset\ndim(crashes)\n\nbind_rows() from dplyr appends the rows from each dataset, creating a unified dataset spanning all four years.\n\n\n\n\n\n\nNotebind_rows() vs rbind() in R (click to expand)\n\n\n\n\n\nBoth bind_rows() and rbind() combine data frames row-wise, but they differ in behaviour and flexibility.\n\nrbind() (Base R)\n\nComes from base R\nRequires exactly matching column names and types\nWill throw an error if the data frames don’t align perfectly\n\ndf1 = data.frame(a = 1:2, b = c(\"x\", \"y\"))\ndf2 = data.frame(a = 3:4, b = c(\"z\", \"w\"))\nrbind(df1, df2)  # ✅ Works\ndf3 = data.frame(a = 5:6, c = c(\"a\", \"b\"))\nrbind(df1, df3)  # ❌ Error: column names do not match\n\nbind_rows() (from dplyr)\n\nPart of the tidyverse\nMore flexible than rbind()\nAutomatically fills in missing columns with NAs\n\nlibrary(dplyr)\n\ndf1 = data.frame(a = 1:2, b = c(\"x\", \"y\"))\ndf3 = data.frame(a = 5:6, c = c(\"a\", \"b\"))\n\nbind_rows(df1, df3)  # ✅ Works, fills missing columns with NA\n\nSummary Comparison\n\n\n\nFeature\nrbind()\nbind_rows()\n\n\n\n\nFrom\nBase R\ndplyr (tidyverse)\n\n\nRequires same cols?\n✅ Yes\n❌ No\n\n\nFills missing cols\n❌ Error\n✅ With NA\n\n\nIdeal use case\nControlled data\nFlexible data wrangling\n\n\n\n\nTip\n\nUse rbind() if you’re sure the data frames have identical structure.\nUse bind_rows() for robust and flexible row-binding, especially in pipelines."
  },
  {
    "objectID": "s6/index.html#converting-crash-data-to-sf-object",
    "href": "s6/index.html#converting-crash-data-to-sf-object",
    "title": "Session 6: Joins and aggregations",
    "section": "2.3 Converting crash data to sf object",
    "text": "2.3 Converting crash data to sf object\nTo enable spatial operations, we need to convert our crash data into an sf (simple features) object:\n\n# Creating geographic crash data\n# This converts the data frame into a spatial object with point geometries\ncrashes_sf = format_sf(crashes)\n\nhead(crashes_sf)\n\nThe format_sf() function from the stats19 package converts the crash coordinates into a spatial object. The note you see indicates that rows with missing coordinate values are automatically removed, as spatial objects cannot have NA values for coordinates/geometry."
  },
  {
    "objectID": "s6/index.html#filtering-for-west-yorkshire",
    "href": "s6/index.html#filtering-for-west-yorkshire",
    "title": "Session 6: Joins and aggregations",
    "section": "2.4 Filtering for West Yorkshire",
    "text": "2.4 Filtering for West Yorkshire\nWe’ll now filter the crash data to include only incidents that occurred within the West Yorkshire police force area:\n\n# Filter crashes to only those that occurred in West Yorkshire\ncrashes_wy = crashes_sf |&gt; filter(police_force == \"West Yorkshire\")\n\n# Compare the number of rows before and after filtering\n# This shows how many crashes occurred in West Yorkshire vs. the whole dataset\nnrow(crashes_sf)\nnrow(crashes_wy)"
  },
  {
    "objectID": "s6/index.html#what-is-a-spatial-join",
    "href": "s6/index.html#what-is-a-spatial-join",
    "title": "Session 6: Joins and aggregations",
    "section": "3.1 What is a spatial join?",
    "text": "3.1 What is a spatial join?\nA spatial join combines two spatial datasets based on their geographic relationship — e.g., whether one geometry intersects, contains, or is within another.\nIn R, we use the sf package to handle spatial joins with functions like:\nst_join(x, y, join = st_intersects)\n\nx: the primary spatial object (e.g. point or line data)\ny: the reference spatial object (e.g. polygons)\nst_intersects, st_within, st_contains, etc. define the spatial relationship\n\n\n\n\n\n\n\nNoteSpatial relations (click to expand)\n\n\n\n\n\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue. The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string.\n\n\n\n\n\nWhy is it useful in transport data science?\nSpatial joins are essential tools in transport data science for:\n\nMapping transport observations (e.g. crashes, stops, GPS traces) to zones or regions\nEnriching data with attributes from other layers (e.g. population, accessibility, land use)\nAggregating or summarising transport data by spatial units\n\nThey allow you to combine spatially referenced datasets in meaningful ways to gain insights and build models. We’ll use this technique to identify which LSOA each crash occurred in."
  },
  {
    "objectID": "s6/index.html#loading-lsoa-boundary-data",
    "href": "s6/index.html#loading-lsoa-boundary-data",
    "title": "Session 6: Joins and aggregations",
    "section": "3.2 Loading LSOA boundary data",
    "text": "3.2 Loading LSOA boundary data\nFirst, we load the LSOA (Lower Super Output Area) boundaries for West Yorkshire:\n\n# Load the 2021 LSOA boundary data for West Yorkshire\nlsoa_wy = read_sf(\"https://github.com/itsleeds/tds/releases/download/2025/p6-lsoa_boundary_wy.geojson\")\n\n# Retain only useful variables: LSOA code (lsoa21cd) and LSOA name (lsoa21nm)\nlsoa_wy = lsoa_wy |&gt; select(lsoa21cd, lsoa21nm) \n\n# Check the structure of the data to understand what we're working with\nglimpse(lsoa_wy)\n\n# How many LSOAs are in our dataset?\n# The cat() function in R is short for “concatenate and print”\ncat(\"Number of LSOAs in West Yorkshire:\", nrow(lsoa_wy))\n\nLSOAs are small geographic areas in the UK designed for reporting census and other neighborhood statistics. Each LSOA typically contains 400-1,200 households, and usually have a resident population of 1,000-3,000 people. We’re using the 2021 LSOA boundaries, which align with the most recent UK Census."
  },
  {
    "objectID": "s6/index.html#performing-a-spatial-join",
    "href": "s6/index.html#performing-a-spatial-join",
    "title": "Session 6: Joins and aggregations",
    "section": "3.3 Performing a Spatial Join",
    "text": "3.3 Performing a Spatial Join\nNow we’ll perform a spatial join to determine which LSOA each crash occurred in:\n\n# Perform spatial join to determine which LSOA each crash occurred in\n# This adds LSOA information to each crash point\ncrashes_in_lsoa = st_join(lsoa_wy, crashes_wy)\n\n# Check which columns were added from the LSOA dataset\n# setdiff finds column names that are new in crashes_in_lsoa\nadded_columns = setdiff(colnames(crashes_in_lsoa), colnames(crashes_wy))\n\n# The collapse argument is used in the paste() function,\n# and it controls how to combine multiple elements into a single string. \n# Here we add \", \" between elements\ncat(\"Columns added from LSOA data:\", paste(added_columns, collapse=\", \"),\"\\n\")\n\n# Check if any crashes couldn't be assigned to an LSOA\nna_lsoa = sum(is.na(crashes_in_lsoa$lsoa21cd))\ncat(\"Number of crashes not matching any LSOA:\", na_lsoa)\n\nThe st_join() function links each crash point to the LSOA polygon that contains it. By default, it performs a “within” operation, checking if each point in the first dataset falls within any polygon in the second dataset. After this operation, each crash record will have additional columns from the LSOA dataset, including the LSOA code and name.\nWe use the setdiff() function to find column names that are new in crashes_in_lsoa — i.e., the ones that come from lsoa_wy."
  },
  {
    "objectID": "s6/index.html#aggregating-crashes-by-lsoa",
    "href": "s6/index.html#aggregating-crashes-by-lsoa",
    "title": "Session 6: Joins and aggregations",
    "section": "3.4 Aggregating Crashes by LSOA",
    "text": "3.4 Aggregating Crashes by LSOA\nNext, we’ll aggregate the crash data to count how many crashes of each severity occurred in each LSOA:\n\n# Aggregate crash data by LSOA, counting crashes of each severity\nlsoa_crashes_count = crashes_in_lsoa |&gt;\n  # Remove geometry column as we only need tabular data for aggregation\n  st_drop_geometry() |&gt;\n  # Group by LSOA identifiers\n  group_by(lsoa21cd) |&gt;\n  # Count crashes by severity level\n  summarise(\n    fatal_crashes_n = sum(accident_severity == \"Fatal\"),      # Number of fatal crashes\n    serious_crashes_n = sum(accident_severity == \"Serious\"),  # Number of serious crashes\n    slight_crashes_n = sum(accident_severity == \"Slight\"),     # Number of slight crashes\n    all_crashes_n = fatal_crashes_n + serious_crashes_n + slight_crashes_n # Total number of crashes\n  )\n\n# Display the first few rows of the aggregated data\nhead(lsoa_crashes_count)"
  },
  {
    "objectID": "s6/index.html#joining-aggregated-crash-count-data-to-lsoa-boundaries",
    "href": "s6/index.html#joining-aggregated-crash-count-data-to-lsoa-boundaries",
    "title": "Session 6: Joins and aggregations",
    "section": "4.1 Joining aggregated crash count data to LSOA Boundaries",
    "text": "4.1 Joining aggregated crash count data to LSOA Boundaries\nNow we’ll join the aggregated crash counts back to the LSOA boundary data:\n\n# Join crash count data back to LSOA boundaries\n# This preserves the spatial information while adding the crash statistics\nlsoa_crashes_wy = lsoa_wy |&gt;\n  left_join(lsoa_crashes_count, by = \"lsoa21cd\") \n\n# Check the columns in our joined dataset\ncolnames(lsoa_crashes_wy)\n\n# Replace NA values with 0 for LSOAs that had no crashes\nlsoa_crashes_wy = lsoa_crashes_wy |&gt;\n  mutate(across(c(all_crashes_n, fatal_crashes_n, serious_crashes_n, slight_crashes_n), \n                ~replace_na(., 0)))\n\n# Count how many LSOAs had zero crashes\nzero_crash_lsoas = sum(lsoa_crashes_wy$all_crashes_n == 0)\ncat(\"Number of LSOAs with zero recorded crashes:\", zero_crash_lsoas, \"\\n\")\ncat(\"Percentage of LSOAs with zero crashes:\", round((zero_crash_lsoas/nrow(lsoa_crashes_wy))*100, 2), \"%\\n\")\n\nWe use left_join() to preserve all LSOAs, even those with no crashes. The by parameter specifies the columns to use for matching rows between the datasets. After this join, we have the spatial boundaries with the crash counts attached.\nWe also replace NA values with 0 for LSOAs that had no crashes, and calculate how many LSOAs had zero crashes recorded.\nQuestion: What happens if you use right_join() instead?"
  },
  {
    "objectID": "s6/index.html#loading-census-population-data",
    "href": "s6/index.html#loading-census-population-data",
    "title": "Session 6: Joins and aggregations",
    "section": "4.2 Loading census population data",
    "text": "4.2 Loading census population data\nWe’ll now load census population data for the LSOAs, the census data is obtained from nomis, specifically the Number of usual residents in households and communal establishments in each LSOA. The data has been further cleaned for better processing.\n\n# Load 2021 Census population data for LSOAs\npop_lsoa = read_csv(\"https://github.com/itsleeds/tds/releases/download/2025/p6-census2021_lsoa_pop.csv\")\n\n# Display the first few rows\nhead(pop_lsoa)\n\n# Basic summary statistics of the population data\nsummary(pop_lsoa$pop)\n\nThis dataset contains population figures from the 2021 UK Census for each LSOA in England and Wales."
  },
  {
    "objectID": "s6/index.html#joining-population-data",
    "href": "s6/index.html#joining-population-data",
    "title": "Session 6: Joins and aggregations",
    "section": "4.3 Joining population data",
    "text": "4.3 Joining population data\nNext, we’ll join the population data to our LSOA crash data using the LSOA codes and names as keys:\n\n# Join population data to our LSOA crash data\nlsoa_crashes_wy = lsoa_crashes_wy |&gt;\n  left_join(pop_lsoa, by = c(\"lsoa21cd\", \"lsoa21nm\"))\n\n# Check the first few rows of our joined dataset\nhead(lsoa_crashes_wy)\n\n# Check if we have any missing population values after the join\nmissing_pop = sum(is.na(lsoa_crashes_wy$pop))\ncat(\"Number of LSOAs with missing population data:\", missing_pop, \"\\n\")\n\nThis operation adds the population data to our existing dataset, allowing us to calculate per-capita crash rates."
  },
  {
    "objectID": "s6/index.html#calculating-crashes-per-person",
    "href": "s6/index.html#calculating-crashes-per-person",
    "title": "Session 6: Joins and aggregations",
    "section": "5.1 Calculating Crashes Per Person",
    "text": "5.1 Calculating Crashes Per Person\nWe’ll calculate the number of crashes per person for each LSOA:\n\n# Calculate crashes per person for each LSOA\nlsoa_crashes_wy = lsoa_crashes_wy |&gt;\n  mutate(\n    # Crashes per person (raw rate)\n    crash_pp = all_crashes_n/pop,\n    \n    # Crashes per 1000 people (more intuitive scale)\n    crash_per_1000 = crash_pp * 1000,\n    \n    # Proportion of crashes that were fatal or serious\n    severity_ratio = (fatal_crashes_n + serious_crashes_n) / all_crashes_n\n  )\n\n# Replace NaN values in severity_ratio (from dividing by zero)\nlsoa_crashes_wy$severity_ratio[is.nan(lsoa_crashes_wy$severity_ratio)] = 0\n\n# Summary statistics of our derived metrics\nsummary(lsoa_crashes_wy$crash_pp)\nsummary(lsoa_crashes_wy$crash_per_1000)\nsummary(lsoa_crashes_wy$severity_ratio)\n\nThese derived metrics normalize the crash counts by population, allowing for more meaningful comparisons between areas with different population sizes. Areas with higher values have more crashes relative to their population."
  },
  {
    "objectID": "d1/index.html",
    "href": "d1/index.html",
    "title": "Welcome and set-up",
    "section": "",
    "text": "Dear Transport Data Science students,\nAs per your timetable, the first session is Thursday 30th January, from 10:00 to 13:00.\nLocation: Richard Hughes Cluster, in the “Cloth Workers Link Building”. If you’re wondering where that is, you’re not alone, I’m not 100% sure. So the first challenge of the module is to ensure that you get there on time, by 09:50, so you have time to get a seat in time for the 10:00 start."
  },
  {
    "objectID": "d1/index.html#homework-for-next-week-deadline-friday-31st-january-1400",
    "href": "d1/index.html#homework-for-next-week-deadline-friday-31st-january-1400",
    "title": "Welcome and set-up",
    "section": "1 Homework for next week (deadline: Friday 31st January, 14:00)",
    "text": "1 Homework for next week (deadline: Friday 31st January, 14:00)\n\nEnsure that you have the timetable stored safely in your calendar, so you do not miss important sessions or seminars.\nEnsure that you have the necessary software installed on your computer and that you have tested that you can use it for the datasets we will be using in the course, see https://itsleeds.github.io/tds/#software-requirements-and-installation for guidance on installing the software you need.\n\nAny issues you have with the software installation, please get in touch with me as soon as possible.\n\n\nTest that you have the necessary software installed by running the following code in R:\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_cran(\"tidyverse\")\nremotes::install_cran(\"osmextract\")\nlibrary(tidyverse)\nlibrary(osmextract)\nlibrary(sf)\n\n\nits = oe_get(\"ITS Leeds\", download_directory = tempdir())\n\n\nfigure = its |&gt;\n  ggplot() +\n  geom_sf(aes(colour = highway)) +\n  theme_void()\n# Save figure\nggsave(\"its.png\", figure, width = 6, height = 4)\n\n\nbrowseURL(\"its.png\")\n\n\nShow the map by executing the following code, which simply prints the map to the screen:\n\nfigure\n\nIf you see a map of the area around the Institute for Transport Studies, then you are ready for the first session. If you have any issues, please get in touch with me as soon as possible.\n\nTake a look at the reading list at https://itsleeds.github.io/tds/reading.html and have a read of the Transportation chapter of Geocomputation with R book (you will find the link to the book in the reading list).\nSign-up for a GitHub account if you do not already have one, and ensure that you have access to the TDS GitHub repository where you will find the course materials.\n\nPlease send me an email with you GitHub username so I can add you to the private repository that supports the course."
  },
  {
    "objectID": "datahack.html",
    "href": "datahack.html",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "This event is designed to build data, coding and reproducible research skills for Institute for Transport Studies (ITS) staff and students. It is also specifically designed to support ITS MSc students with their dissertation projects by providing a space to ask questions about importing, processing and visualising data.\nIt will take place on Thursday 7th May 2026. It is open to staff and students at ITS. Contact the organisers (Robin Lovelace) if you are not based at the University of Leeds and would like to join in.\nSee here to sign-up.\n\n\n\nTo create a supportive space for participants to ask questions about working with datasets in general and using data science techniques for working with transport datasets in particular\nGet support importing datasets for MSc dissertations and other projects\nData wrangling with the tidyverse R package and other tools\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nShowcase the potential of open data (transparency, participation, research) and reproducible/open work-flows\n\n\n\n\n\nNone: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details\n\n\n\n\n\n13:00 - 13:30: Introduction to importing, processing and visualising data with RStudio\n\nAn indroduction to RStudio\nAn introduction to Quarto for reproducible reports\nAn example with origin-destination data in Leeds\n\n13:30 - 14:00: Importing your datasets\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: Solo working on your datasets, asking questions, and getting support\n15:15 - 15:45: Presentation of the results (optional for participants)\n\nAn opportunity for participants to share what they learned\n\n15:45 - 16:00: Networking and sharing ideas\n\n\n\n\nThe prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers."
  },
  {
    "objectID": "datahack.html#objectives",
    "href": "datahack.html#objectives",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "To create a supportive space for participants to ask questions about working with datasets in general and using data science techniques for working with transport datasets in particular\nGet support importing datasets for MSc dissertations and other projects\nData wrangling with the tidyverse R package and other tools\nLearning the general skill of data visualisation and gain specific experience working with tap/on/tap/out data\nShowcase the potential of open data (transparency, participation, research) and reproducible/open work-flows"
  },
  {
    "objectID": "datahack.html#prerequisites",
    "href": "datahack.html#prerequisites",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "None: just an interest in transport data and a willingness to learn\nUseful: if you have experience with GitHub R, Python or other tools for reproducible data analysis you can join in with the coding, see the Transport Data Science module for more details"
  },
  {
    "objectID": "datahack.html#schedule",
    "href": "datahack.html#schedule",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "13:00 - 13:30: Introduction to importing, processing and visualising data with RStudio\n\nAn indroduction to RStudio\nAn introduction to Quarto for reproducible reports\nAn example with origin-destination data in Leeds\n\n13:30 - 14:00: Importing your datasets\n\nInstalling any necessary packages\nRequesting support for any issues\n\n14:00 - 14:05: Break\n14:05 - 15:00: Solo working on your datasets, asking questions, and getting support\n15:15 - 15:45: Presentation of the results (optional for participants)\n\nAn opportunity for participants to share what they learned\n\n15:45 - 16:00: Networking and sharing ideas"
  },
  {
    "objectID": "datahack.html#prizes",
    "href": "datahack.html#prizes",
    "title": "Transport Data Minihack 2026",
    "section": "",
    "text": "The prize will be Geocomputation with Python or Geocomputation with R (second edition). Prizes will be awarded based on importing, analysing and helping to document the challenge datasets (see Challenges section below):\n\nBest technical implementation and code\nMost creative or impactful use of data\n\nThe presentations will be assessed by the organisers."
  },
  {
    "objectID": "s6/slides.html#objectives",
    "href": "s6/slides.html#objectives",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand different types of joins (spatial and key-based)\nLearn how to perform spatial joins with sf and dplyr\nApply aggregations to summarize data\nVisualize joined datasets\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(stats19)"
  },
  {
    "objectID": "s6/slides.html#what-is-a-join",
    "href": "s6/slides.html#what-is-a-join",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "What is a Join?",
    "text": "What is a Join?\n\nCombining datasets based on common attributes or spatial relationships\nSpatial joins: Link points (e.g., crashes) to polygons (e.g., LSOAs) using geometry\nKey-based joins: Match IDs across tables (e.g., LSOA codes)"
  },
  {
    "objectID": "s6/slides.html#spatial-join-example",
    "href": "s6/slides.html#spatial-join-example",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Spatial Join Example",
    "text": "Spatial Join Example\n\nLoad spatial datasets:\n\n\npath &lt;- \"https://github.com/itsleeds/tds/releases/download/2025/p6-lsoa_boundary_wy.geojson\"\nlsoa &lt;- read_sf(path)\ncrashes &lt;- stats19::get_stats19(\"2023\")\n\n\nPerform spatial join:\n\n\njoined &lt;- st_join(lsoa, crashes, join = st_intersects)\n\n\nAggregate results:\n\n\nagg_data &lt;- joined |&gt;\n  group_by(lsoa_id) |&gt;\n  summarize(total_crashes = n())"
  },
  {
    "objectID": "s6/slides.html#key-based-joins",
    "href": "s6/slides.html#key-based-joins",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Key-Based Joins",
    "text": "Key-Based Joins\nUse dplyr functions like left_join to combine data by identifiers:\n\n# Example: Join crash counts with population data\nfinal &lt;- left_join(agg_data, pop_data, by = \"lsoa_id\")"
  },
  {
    "objectID": "s6/slides.html#aggregations",
    "href": "s6/slides.html#aggregations",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Aggregations",
    "text": "Aggregations\n\nGroup data by spatial units (e.g., LSOA)\nCalculate metrics like crashes per capita:\n\n\nfinal &lt;- final |&gt;\n  mutate(crashes_per_capita = total_crashes / population)"
  },
  {
    "objectID": "s6/slides.html#visualization",
    "href": "s6/slides.html#visualization",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "Visualization",
    "text": "Visualization\nCreate maps to visualize crash patterns using tmap:\n\ntmap_mode(\"plot\")\ntm_shape(final) +\n  tm_polygons(\"crashes_per_capita\")"
  },
  {
    "objectID": "s6/slides.html#references",
    "href": "s6/slides.html#references",
    "title": "Joins and Aggregations in Transport Data Science",
    "section": "References",
    "text": "References\n{references}"
  },
  {
    "objectID": "examples/test.html",
    "href": "examples/test.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "x = 1\nprint(x)\n\n1"
  },
  {
    "objectID": "s1/slides.html#who-transport-data-science-team",
    "href": "s1/slides.html#who-transport-data-science-team",
    "title": "Introduction to transport data science",
    "section": "Who: Transport Data Science team",
    "text": "Who: Transport Data Science team\nRobin Lovelace\n\nAssociate Professor of Transport Data Science\nResearching transport futures and active travel planning\nR developer and teacher, author of Geocomputation with R\n\nYuanxuan Yang\n\nLecturer in Data Science of Transport\nNew and Emerging Forms of Data: Investigating novel data sources and their applications in urban mobility and transport planning."
  },
  {
    "objectID": "s1/slides.html#tds-team-ii",
    "href": "s1/slides.html#tds-team-ii",
    "title": "Introduction to transport data science",
    "section": "TDS Team II",
    "text": "TDS Team II\nMalcolm Morgan\n\nSenior researcher at ITS with expertise in routing + web\nDeveloper of the Propensity to Cycle Tool and PBCC\n\nZhao Wang\n\nCivil Engineer and Data Scientist with expertise in machine learning\n\nDemonstrators\nYou!"
  },
  {
    "objectID": "s1/slides.html#what-is-transport-data-science",
    "href": "s1/slides.html#what-is-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "What is transport data science?",
    "text": "What is transport data science?\n\nThe application of data science to transport datasets and problems\nRaising the question…\nWhat is data science?\nA discipline “that allows you to turn raw data into understanding, insight, and knowledge” (Grolemund, 2016)\n\nIn other words…\n\nStatistics that is actually useful!"
  },
  {
    "objectID": "s1/slides.html#why-take-transport-data-science",
    "href": "s1/slides.html#why-take-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "Why take Transport Data Science",
    "text": "Why take Transport Data Science\n\n\n\nNew skills (cutting edge R and/or Python packages)\nPotential for impacts\nAllows you to do new things with data\nIt might get you a job!"
  },
  {
    "objectID": "s1/slides.html#example",
    "href": "s1/slides.html#example",
    "title": "Introduction to transport data science",
    "section": "Example",
    "text": "Example\nData science spin-out company: ImpactML"
  },
  {
    "objectID": "s1/slides.html#data-science-employability",
    "href": "s1/slides.html#data-science-employability",
    "title": "Introduction to transport data science",
    "section": "Data science employability",
    "text": "Data science employability\n\n\n\nThe Bureau of Labor Statistics in the US projects a 35% increase in data science roles in decade 2022-2032.” Source: visualisecurious.com"
  },
  {
    "objectID": "s1/slides.html#live-demo-npt.scot-web-app",
    "href": "s1/slides.html#live-demo-npt.scot-web-app",
    "title": "Introduction to transport data science",
    "section": "Live demo: npt.scot web app",
    "text": "Live demo: npt.scot web app"
  },
  {
    "objectID": "s1/slides.html#the-history-of-tds",
    "href": "s1/slides.html#the-history-of-tds",
    "title": "Introduction to transport data science",
    "section": "The history of TDS",
    "text": "The history of TDS\n\n2017: Transport Data Science created, led by Dr Charles Fox, Computer Scientist, author of Transport Data Science book (Fox, 2018)\nThe focus was on databases and Bayesian methods\n2019: I inherited the module, which was attended by ITS students\nSummer 2019: Python code published in the module ‘repo’:\n\ngithub.com/ITSLeeds"
  },
  {
    "objectID": "s1/slides.html#history-of-tds-ii",
    "href": "s1/slides.html#history-of-tds-ii",
    "title": "Introduction to transport data science",
    "section": "History of TDS II",
    "text": "History of TDS II\n\nJanuary 2020: Available, Data Science MSc course\nMarch 2020: Switch to online teaching\n2021-2023: Updated module, focus on methods\n2024: Switch to combined lecture and practical sessions\n2025+: Expand, online course? book? stay in touch!\n\n\n\nMilestone passed in my academic career, first online-only delivery of lecture ITSLeeds (2025), seems to have worked, live code demo with #rstats/rstudio, recording, chat + all🎉Thanks students for ‘attending’ + remote participation, we’ll get through this together.#coronavirus pic.twitter.com/wlAUxmZj5r\n\n— Robin Lovelace March 17, 2020"
  },
  {
    "objectID": "s1/slides.html#reading-list",
    "href": "s1/slides.html#reading-list",
    "title": "Introduction to transport data science",
    "section": "Reading list",
    "text": "Reading list\nSee the reading list for details"
  },
  {
    "objectID": "s1/slides.html#objectives",
    "href": "s1/slides.html#objectives",
    "title": "Introduction to transport data science",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the structure of transport datasets\nUnderstand how to obtain, clean and store transport related data\nGain proficiency in command-line tools for handling large transport datasets\nProduce data visualizations, static and interactive\n Learn how to join together the components of transport data science into a cohesive project portfolio"
  },
  {
    "objectID": "s1/slides.html#assessment-for-those-doing-this-as-credit-bearing",
    "href": "s1/slides.html#assessment-for-those-doing-this-as-credit-bearing",
    "title": "Introduction to transport data science",
    "section": "Assessment (for those doing this as credit-bearing)",
    "text": "Assessment (for those doing this as credit-bearing)\n\nYou will build-up a portfolio of work\n100% coursework assessed, you will submit by\nWritten in code - will be graded for reproducibility\nCode chunks and figures are encouraged\nYou will submit a non-assessed 2 page pdf + qmd"
  },
  {
    "objectID": "s1/slides.html#feedback",
    "href": "s1/slides.html#feedback",
    "title": "Introduction to transport data science",
    "section": "Feedback",
    "text": "Feedback\n\nThe module is taught by two really well organised and enthusiastic professors, great module, the seminars, structured and unstructured learning was great and well thought out, all came together well\n\n\nI wish this module was 60 credits instead of 15 because i just want more of it."
  },
  {
    "objectID": "s1/slides.html#timetable",
    "href": "s1/slides.html#timetable",
    "title": "Introduction to transport data science",
    "section": "Timetable",
    "text": "Timetable\nSee the schedule for details"
  },
  {
    "objectID": "s1/slides.html#what-is-science",
    "href": "s1/slides.html#what-is-science",
    "title": "Introduction to transport data science",
    "section": "What is science?",
    "text": "What is science?\n\n\n\nScientific knowledge is hypotheses that can be falsified\nScience is the process of generating falsifiable hypotheses and testing them\nIn a reproducible way\nSystematically\n\n\n\n\nFalsifiability is central to the scientific process (Popper 1959)\nAll of which requires software conducive to reproducibility"
  },
  {
    "objectID": "s1/slides.html#transport-planning-software",
    "href": "s1/slides.html#transport-planning-software",
    "title": "Introduction to transport data science",
    "section": "Transport planning software",
    "text": "Transport planning software\nTransport modelling software products are a vital component of modern transport planning and research.\n\nThey generate the evidence base on which strategic investments are made and, furthermore,\nprovide a powerful mechanism for researching alternative futures.\n\nIt would not be an overstatement to say that software determines the range of futures that are visible to policymakers. This makes status of transport modelling software and how it may evolve in the future important questions.\nWhat will transport software look like? What will their capabilities be? And who will control? Answers to each of these questions will affect the future of transport systems.\n\nPremise: transport planning/modelling software used in practice will become is becoming increasingly data-driven, modular and open."
  },
  {
    "objectID": "s1/slides.html#current-transport-software",
    "href": "s1/slides.html#current-transport-software",
    "title": "Introduction to transport data science",
    "section": "Current transport software",
    "text": "Current transport software\n\n\n\n\n\n\n\n\n\n4-stage model still dominates transport planning models (Boyce and Williams 2015)"
  },
  {
    "objectID": "s1/slides.html#the-four-stage-model",
    "href": "s1/slides.html#the-four-stage-model",
    "title": "Introduction to transport data science",
    "section": "The four stage model",
    "text": "The four stage model\n\nImpacts the current software landscape\nDominated by a few proprietary products\nLimited support community online\nHigh degree of lock-in\nLimited cross-department collaboration"
  },
  {
    "objectID": "s1/slides.html#existing-products",
    "href": "s1/slides.html#existing-products",
    "title": "Introduction to transport data science",
    "section": "Existing products",
    "text": "Existing products\nSample of transport modelling software in use by practitioners. \n\n\n\n\n\nSoftware\nCompany/Developer\nCompany HQ\nLicence\nCitations\n\n\n\n\nVisum\nPTV\nGermany\nProprietary\n1810\n\n\nMATSim\nTU Berlin\nGermany\nOpen source (GPL)\n1470\n\n\nTransCAD\nCaliper\nUSA\nProprietary\n1360\n\n\nSUMO\nDLR\nGermany\nOpen source (EPL)\n1310\n\n\nEmme\nINRO\nCanada\nProprietary\n780\n\n\nCube\nCitilabs\nUSA\nProprietary\n400\n\n\nsDNA\nCardiff University\nUK\nOpen source (GPL)\n170"
  },
  {
    "objectID": "s1/slides.html#user-support",
    "href": "s1/slides.html#user-support",
    "title": "Introduction to transport data science",
    "section": "User support",
    "text": "User support\nGetting help is vital for leaning/improving software\n\n“10-Hour Service Pack $2,000” (source: caliper.com/tcprice.htm)"
  },
  {
    "objectID": "s1/slides.html#online-communities",
    "href": "s1/slides.html#online-communities",
    "title": "Introduction to transport data science",
    "section": "Online communities",
    "text": "Online communities\n\ngis.stackexchange.com has 21,314 questions\nr-sig-geo has 1000s of posts\nRStudio’s Discourse community has 65,000+ posts already!\nNo clear transport equivalent (e.g. earthscience.stackexchange.com is in beta)\nSolution: build our own community!\n\nSee https://github.com/ITSLeeds/TDS/issues for example\nPlace for discussions: https://github.com/itsleeds/tds/discussions"
  },
  {
    "objectID": "s1/slides.html#best-way-to-get-support-is-peer-to-peer",
    "href": "s1/slides.html#best-way-to-get-support-is-peer-to-peer",
    "title": "Introduction to transport data science",
    "section": "Best way to get support is peer-to-peer:",
    "text": "Best way to get support is peer-to-peer:\n\nSource: https://community.rstudio.com/about"
  },
  {
    "objectID": "s1/slides.html#how-is-data-science-used-in-the-pct",
    "href": "s1/slides.html#how-is-data-science-used-in-the-pct",
    "title": "Introduction to transport data science",
    "section": "How is data science used in the PCT?",
    "text": "How is data science used in the PCT?\n\nIt’s all reproducible, e.g.:\nFind commuting desire lines in West Yorkshire between 1 and 3 km long in which more people drive than cycle:"
  },
  {
    "objectID": "s1/slides.html#visualising-data",
    "href": "s1/slides.html#visualising-data",
    "title": "Introduction to transport data science",
    "section": "Visualising data",
    "text": "Visualising data\nA fundamental part of data science is being able to understand your data.\nThat requires visualisation, R is great for that:"
  },
  {
    "objectID": "s1/slides.html#interactively",
    "href": "s1/slides.html#interactively",
    "title": "Introduction to transport data science",
    "section": "Interactively",
    "text": "Interactively"
  },
  {
    "objectID": "s1/slides.html#processing-data-with-code",
    "href": "s1/slides.html#processing-data-with-code",
    "title": "Introduction to transport data science",
    "section": "Processing data with code",
    "text": "Processing data with code\n\nNow we have data in our computer, and verified it works, we can use it\nWhich places are most car dependent?"
  },
  {
    "objectID": "s1/slides.html#checking-the-results",
    "href": "s1/slides.html#checking-the-results",
    "title": "Introduction to transport data science",
    "section": "Checking the results:",
    "text": "Checking the results:"
  },
  {
    "objectID": "s1/slides.html#r-vs-python",
    "href": "s1/slides.html#r-vs-python",
    "title": "Introduction to transport data science",
    "section": "R vs Python",
    "text": "R vs Python\n\nLots of debate on this topic - see https://blog.usejournal.com/python-vs-and-r-for-data-science-833b48ccc91d\n\nHow to decide?\n\nIf priority: getting things done quick (with support from me ;) go with R\nIf you already know Python and are 100% confident you can generate reproducible results, go with that\nIf you want to be avant-garde and try something else like Julia, do it (as long as it’s reproducible)\n\n\nGamification\n\n\n\n\n\n\n\n\n\n\nCompletely open source, written in rust\nSource: video at https://github.com/dabreegster/abstreet/#ab-street"
  },
  {
    "objectID": "s1/slides.html#summary",
    "href": "s1/slides.html#summary",
    "title": "Introduction to transport data science",
    "section": "Summary",
    "text": "Summary\n\nWalk and understand the data before doing complex things\nVisualise the data, ask questions of it, descriptive stats\nOnly then add complexity to your analysis\nStarting point for this: Transport chapter of Geocomputation with R (Lovelace, Nowosad, and Münchow 2025)"
  },
  {
    "objectID": "s1/s1project/foundations.html",
    "href": "s1/s1project/foundations.html",
    "title": "1 Python example",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nHello this is some text.\n\ncasualty_type = c(\"cat\", \"dog\", \"person\")\ncasualty_age = seq(from = 20, to = 60, by = 20)\ncrashes = data.frame(casualty_type, casualty_age)\nplot(crashes$casualty_age)\n\n\n\n\n\n\n\n\nSubsetting.\n\ncrashes$casualty_type\n\n[1] \"cat\"    \"dog\"    \"person\"\n\ncrashes[[1]]\n\n[1] \"cat\"    \"dog\"    \"person\"\n\ncrashes[2,1]\n\n[1] \"dog\"\n\n\n\ncrashes |&gt;\n  select(casualty_type)\n\n  casualty_type\n1           cat\n2           dog\n3        person\n\ncrashes |&gt; \n  filter(casualty_age &gt; 35)\n\n  casualty_type casualty_age\n1           dog           40\n2        person           60\n\ncrashes |&gt; \n  filter(casualty_age-20 &gt; 35)\n\n  casualty_type casualty_age\n1        person           60\n\ncrashes |&gt;\n  ggplot() +\n  geom_bar(aes(x = casualty_age, fill = casualty_type))\n\n\n\n\n\n\n\n\n\nac = stats19::get_stats19(year = 2020, type = \"collision\")\n\nFiles identified: dft-road-casualty-statistics-collision-2020.csv\n\n\n   https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-collision-2020.csv\n\n\nData saved at /tmp/Rtmp1gpWpL/dft-road-casualty-statistics-collision-2020.csv\n\n\nReading in: \n\n\n/tmp/Rtmp1gpWpL/dft-road-casualty-statistics-collision-2020.csv\n\n\ndate and time columns present, creating formatted datetime column\n\nclass(ac)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\ndim(ac)\n\n[1] 91199    45\n\nac_2021 = stats19::get_stats19(year = 2021, type = \"collision\")\n\nFiles identified: dft-road-casualty-statistics-collision-2021.csv\n\n\n   https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-collision-2021.csv\n\n\nData saved at /tmp/Rtmp1gpWpL/dft-road-casualty-statistics-collision-2021.csv\n\n\nReading in: \n\n\n/tmp/Rtmp1gpWpL/dft-road-casualty-statistics-collision-2021.csv\n\n\ndate and time columns present, creating formatted datetime column\n\nnrow(ac)\n\n[1] 91199\n\nnrow(ac_2021)\n\n[1] 101087\n\n# # After googling \"combine 2 data frames\" let's try rbind\n# ??combine\n# ?rbind\nac = rbind(ac, ac_2021)\ndim(ac)\n\n[1] 192286     45\n\nac_datetime = c(ac$datetime, ac_2021$datetime)\nlength(ac_datetime)\n\n[1] 293373\n\nrange(ac_datetime)\n\n[1] \"2020-01-01 00:01:00 GMT\" \"2021-12-31 23:55:00 GMT\"\n\nclass(ac)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nstr(ac)\n\nspc_tbl_ [192,286 × 45] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ collision_index                                 : chr [1:192286] \"2020170H10890\" \"2020170L20690\" \"2020070094394\" \"2020070453828\" ...\n $ collision_year                                  : int [1:192286] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 ...\n $ collision_ref_no                                : chr [1:192286] \"170H10890\" \"170L20690\" \"070094394\" \"070453828\" ...\n $ location_easting_osgr                           : int [1:192286] 446191 455573 355466 350786 360959 368986 369533 369274 330668 508733 ...\n $ location_northing_osgr                          : int [1:192286] 534540 519900 383570 385820 388689 422593 428752 429065 437057 431050 ...\n $ longitude                                       : int [1:192286] NA NA NA NA NA NA NA NA NA NA ...\n $ latitude                                        : int [1:192286] NA NA NA NA NA NA NA NA NA NA ...\n $ police_force                                    : chr [1:192286] \"Cleveland\" \"Cleveland\" \"Cheshire\" \"Cheshire\" ...\n $ collision_severity                              : chr [1:192286] \"Slight\" \"Slight\" \"Slight\" \"Slight\" ...\n $ number_of_vehicles                              : chr [1:192286] \"2\" \"1\" \"2\" \"2\" ...\n $ number_of_casualties                            : chr [1:192286] \"1\" \"1\" \"2\" \"1\" ...\n $ date                                            : Date[1:192286], format: \"2020-10-28\" \"2020-08-31\" ...\n $ day_of_week                                     : chr [1:192286] \"Wednesday\" \"Monday\" \"Wednesday\" \"Friday\" ...\n $ time                                            : chr [1:192286] \"09:08\" \"13:25\" \"21:30\" \"20:05\" ...\n $ local_authority_district                        : chr [1:192286] \"Hartlepool\" \"Redcar and Cleveland\" \"Halton\" \"Halton\" ...\n $ local_authority_ons_district                    : chr [1:192286] \"Hartlepool\" \"Redcar and Cleveland\" \"Halton\" \"Halton\" ...\n $ local_authority_highway                         : chr [1:192286] \"Hartlepool\" \"Redcar and Cleveland\" \"Halton\" \"Halton\" ...\n $ local_authority_highway_current                 : chr [1:192286] \"E06000001\" \"E06000003\" \"E06000006\" \"E06000006\" ...\n $ first_road_class                                : chr [1:192286] \"A\" \"Unclassified\" \"A\" \"Unclassified\" ...\n $ first_road_number                               : chr [1:192286] \"179\" \"first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero \" \"558\" \"first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero \" ...\n $ road_type                                       : chr [1:192286] \"Single carriageway\" \"Single carriageway\" \"Slip road\" \"Single carriageway\" ...\n $ speed_limit                                     : chr [1:192286] \"60\" \"30\" \"60\" \"20\" ...\n $ junction_detail_historic                        : chr [1:192286] \"T or staggered junction\" \"Crossroads\" \"Roundabout\" \"T or staggered junction\" ...\n $ junction_detail                                 : chr [1:192286] \"T or staggered junction\" \"Crossroads\" \"Not at junction or within 20 metres\" \"T or staggered junction\" ...\n $ junction_control                                : chr [1:192286] \"Give way or uncontrolled\" \"Give way or uncontrolled\" \"Give way or uncontrolled\" \"Give way or uncontrolled\" ...\n $ second_road_class                               : chr [1:192286] \"Unclassified\" \"Unclassified\" \"Unclassified\" \"Unclassified\" ...\n $ second_road_number                              : chr [1:192286] \"first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero \" \"first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero \" \"first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero \" \"first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero \" ...\n $ pedestrian_crossing_human_control_historic      : chr [1:192286] \"None within 50 metres \" \"None within 50 metres \" \"None within 50 metres \" \"None within 50 metres \" ...\n $ pedestrian_crossing_physical_facilities_historic: chr [1:192286] \"No physical crossing facilities within 50 metres\" \"No physical crossing facilities within 50 metres\" \"No physical crossing facilities within 50 metres\" \"No physical crossing facilities within 50 metres\" ...\n $ pedestrian_crossing                             : chr [1:192286] \"No physical crossing facility within 50m\" \"No physical crossing facility within 50m\" \"No physical crossing facility within 50m\" \"No physical crossing facility within 50m\" ...\n $ light_conditions                                : chr [1:192286] \"Daylight\" \"Daylight\" \"Darkness - lighting unknown\" \"Daylight\" ...\n $ weather_conditions                              : chr [1:192286] \"Fine no high winds\" \"Fine no high winds\" \"Fine no high winds\" \"Fine no high winds\" ...\n $ road_surface_conditions                         : chr [1:192286] \"Dry\" \"Dry\" \"Wet or damp\" \"Dry\" ...\n $ special_conditions_at_site                      : chr [1:192286] \"None\" \"None\" \"None\" \"None\" ...\n $ carriageway_hazards_historic                    : chr [1:192286] \"None\" \"None\" \"None\" \"None\" ...\n $ carriageway_hazards                             : chr [1:192286] \"None\" \"None\" \"None\" \"None\" ...\n $ urban_or_rural_area                             : chr [1:192286] \"Rural\" \"Urban\" \"Rural\" \"Urban\" ...\n $ did_police_officer_attend_scene_of_accident     : chr [1:192286] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ trunk_road_flag                                 : chr [1:192286] \"Non-trunk\" \"Non-trunk\" \"Non-trunk\" \"Non-trunk\" ...\n $ lsoa_of_accident_location                       : chr [1:192286] \"E01011959\" \"E01033471\" \"E01012386\" \"E01012427\" ...\n $ enhanced_severity_collision                     : num [1:192286] -1 -1 -1 -1 -1 7 3 3 3 3 ...\n $ collision_injury_based                          : chr [1:192286] \"Based on severity reporting\" \"Based on severity reporting\" \"Based on severity reporting\" \"Based on severity reporting\" ...\n $ collision_adjusted_severity_serious             : num [1:192286] 0.2309 0.117 0.0431 0.0239 0.1138 ...\n $ collision_adjusted_severity_slight              : num [1:192286] 0.769 0.883 0.957 0.976 0.886 ...\n $ datetime                                        : POSIXct[1:192286], format: \"2020-10-28 09:08:00\" \"2020-08-31 13:25:00\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   collision_index = col_character(),\n  ..   collision_year = col_integer(),\n  ..   collision_ref_no = col_character(),\n  ..   location_easting_osgr = col_integer(),\n  ..   location_northing_osgr = col_integer(),\n  ..   longitude = col_integer(),\n  ..   latitude = col_integer(),\n  ..   police_force = col_character(),\n  ..   collision_severity = col_character(),\n  ..   number_of_vehicles = col_character(),\n  ..   number_of_casualties = col_character(),\n  ..   date = col_character(),\n  ..   day_of_week = col_character(),\n  ..   time = col_character(),\n  ..   local_authority_district = col_character(),\n  ..   local_authority_ons_district = col_character(),\n  ..   local_authority_highway = col_character(),\n  ..   local_authority_highway_current = col_character(),\n  ..   first_road_class = col_character(),\n  ..   first_road_number = col_character(),\n  ..   road_type = col_character(),\n  ..   speed_limit = col_character(),\n  ..   junction_detail_historic = col_character(),\n  ..   junction_detail = col_character(),\n  ..   junction_control = col_character(),\n  ..   second_road_class = col_character(),\n  ..   second_road_number = col_character(),\n  ..   pedestrian_crossing_human_control_historic = col_character(),\n  ..   pedestrian_crossing_physical_facilities_historic = col_character(),\n  ..   pedestrian_crossing = col_character(),\n  ..   light_conditions = col_character(),\n  ..   weather_conditions = col_character(),\n  ..   road_surface_conditions = col_character(),\n  ..   special_conditions_at_site = col_character(),\n  ..   carriageway_hazards_historic = col_character(),\n  ..   carriageway_hazards = col_character(),\n  ..   urban_or_rural_area = col_character(),\n  ..   did_police_officer_attend_scene_of_accident = col_character(),\n  ..   trunk_road_flag = col_character(),\n  ..   lsoa_of_accident_location = col_character(),\n  ..   enhanced_severity_collision = col_double(),\n  ..   collision_injury_based = col_character(),\n  ..   collision_adjusted_severity_serious = col_double(),\n  ..   collision_adjusted_severity_slight = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nnames(ac)\n\n [1] \"collision_index\"                                 \n [2] \"collision_year\"                                  \n [3] \"collision_ref_no\"                                \n [4] \"location_easting_osgr\"                           \n [5] \"location_northing_osgr\"                          \n [6] \"longitude\"                                       \n [7] \"latitude\"                                        \n [8] \"police_force\"                                    \n [9] \"collision_severity\"                              \n[10] \"number_of_vehicles\"                              \n[11] \"number_of_casualties\"                            \n[12] \"date\"                                            \n[13] \"day_of_week\"                                     \n[14] \"time\"                                            \n[15] \"local_authority_district\"                        \n[16] \"local_authority_ons_district\"                    \n[17] \"local_authority_highway\"                         \n[18] \"local_authority_highway_current\"                 \n[19] \"first_road_class\"                                \n[20] \"first_road_number\"                               \n[21] \"road_type\"                                       \n[22] \"speed_limit\"                                     \n[23] \"junction_detail_historic\"                        \n[24] \"junction_detail\"                                 \n[25] \"junction_control\"                                \n[26] \"second_road_class\"                               \n[27] \"second_road_number\"                              \n[28] \"pedestrian_crossing_human_control_historic\"      \n[29] \"pedestrian_crossing_physical_facilities_historic\"\n[30] \"pedestrian_crossing\"                             \n[31] \"light_conditions\"                                \n[32] \"weather_conditions\"                              \n[33] \"road_surface_conditions\"                         \n[34] \"special_conditions_at_site\"                      \n[35] \"carriageway_hazards_historic\"                    \n[36] \"carriageway_hazards\"                             \n[37] \"urban_or_rural_area\"                             \n[38] \"did_police_officer_attend_scene_of_accident\"     \n[39] \"trunk_road_flag\"                                 \n[40] \"lsoa_of_accident_location\"                       \n[41] \"enhanced_severity_collision\"                     \n[42] \"collision_injury_based\"                          \n[43] \"collision_adjusted_severity_serious\"             \n[44] \"collision_adjusted_severity_slight\"              \n[45] \"datetime\"                                        \n\n# aggregate this by day to show \n# how crash numbers varied over the year\nac_by_year = ac |&gt;\n  group_by(date) |&gt;\n  summarise(\n    n_crashes = n()\n  )\nac_by_year |&gt;\n  mutate(\n    `N. crashes per year` = n_crashes,\n    `Week average` = zoo::rollmean(n_crashes, 7, na.pad = TRUE),\n    Date = date,\n  ) |&gt; \n  ggplot(aes(x = Date, y = `N. crashes per year`)) +\n  geom_point(alpha = 0.1) +\n  ylim(c(0, NA)) +\n  # geom_smooth() +\n  # weekly rolling average\n  geom_line(aes(Date, `Week average`), colour = \"red\") +\n  theme_minimal()\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n# Updated plot with title and legend...\nac_by_year |&gt;\n  mutate(\n    `N. crashes per year` = n_crashes,\n    `Week average` = zoo::rollmean(n_crashes, 7, na.pad = TRUE),\n    Date = date,\n  ) |&gt; \n  ggplot(aes(x = Date, y = `N. crashes per year`)) +\n  geom_point(alpha = 0.1) +\n  ylim(c(0, NA)) +\n  # geom_smooth() +\n  # weekly rolling average\n  geom_line(aes(Date, `Week average`, colour = \"Week average\")) +\n  theme_minimal() +\n  labs(\n    colour = \"Legend\"\n  ) +\n  scale_colour_manual(values = c(\"Week average\" = \"red\")) +\n  ggtitle(\"Collions/day, 2020 to 2021\") +\n  theme(\n    legend.position = \"bottom\"\n  )\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n1 Python example\n\ncasualty_type_py = [\"a\", \"B\", \"c\"]\ncasualty_type_py\n\n['a', 'B', 'c']"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "This module focuses on applying data science techniques to solve real-world transport problems. Based at the University of Leeds’ Institute for Transport Studies (module code TRAN5340M), the course is led by Robin Lovelace, Professor of Transport Data Science and developer of several data-driven solutions for effective transport planning.\nThe course has evolved over a decade of teaching and research in the field. It aims to equip you with up-to-date and future-proof skills through practical examples and reproducible workflows using industry-standard data science tools.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#hardware",
    "href": "index.html#hardware",
    "title": "Transport Data Science",
    "section": "Hardware",
    "text": "Hardware\nWe highly recommend having access to a computer with at least 8 GB of RAM that you have permission to install software on.\nAlternatively, you could use cloud-based services such as RStudio Cloud, Google Colab, or GitHub Codespaces. However, you would need to be comfortable using these services and may miss out on some benefits of using your own computer.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#computing-experience",
    "href": "index.html#computing-experience",
    "title": "Transport Data Science",
    "section": "Computing experience",
    "text": "Computing experience\nYou should be comfortable with general computing tasks, such as:\n\nCreating folders and managing files\nInstalling software\nUsing command line interfaces (PowerShell in Windows, Terminal in macOS, or Linux shell)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#data-science-experience-prerequisites",
    "href": "index.html#data-science-experience-prerequisites",
    "title": "Transport Data Science",
    "section": "Data science experience prerequisites",
    "text": "Data science experience prerequisites\nPrior experience using R or Python is essential. This could include:\n\nUsing these languages in professional work\nExperience from previous degrees\nCompletion of relevant online courses\n\nStudents can demonstrate this prerequisite knowledge by showing evidence they have:\n\nWorked with R previously\nCompleted online courses such as the first 4 sessions in the RStudio Primers series\nCompleted DataCamp’s Free Introduction to R course\n\nSubstantial programming and data science experience in previous professional or academic work using languages like R or Python also satisfies the prerequisite requirements.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#quickstart-with-github-codespaces",
    "href": "index.html#quickstart-with-github-codespaces",
    "title": "Transport Data Science",
    "section": "Quickstart with GitHub Codespaces",
    "text": "Quickstart with GitHub Codespaces\nFor a quick cloud-based setup, you can use GitHub Codespaces to access the course materials:\n\nSign up to GitHub\nFork the repository\nClick the “Open in GitHub Codespaces” button above\n\nAlternatively, use the following link:\n\n\n\nOpen in GitHub Codespaces",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#r",
    "href": "index.html#r",
    "title": "Transport Data Science",
    "section": "R",
    "text": "R\nInstall a recent version of R (4.3.0 or above) and an IDE:\n\nR from cran.r-project.org\nRStudio from rstudio.com (recommended)\nAlternatively, VS Code with the R extension installed (if you have prior experience with it)\n\nYou’ll also need to install R packages:\n\nIndividual packages can be installed by opening RStudio and typing commands like install.packages(\"stats19\") in the R console\nTo install all dependencies for the module at once, run the following command in the R console:\n\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"itsleeds/tds\")\n\nSee Section 1.5 of the online guide Reproducible Road Safety Research with R for instructions on how to install key packages we will use in the module.1",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Transport Data Science",
    "section": "Python",
    "text": "Python\nIf you choose to use Python, you should be comfortable with:\n\nInstalling Python\nManaging your own Python environment\nInstalling packages and resolving package conflicts\n\nFor Python users, we recommend using an environment manager such as:\n\npixi (which can manage both R and Python environments)\nDocker (best practice for reproducibility and isolation)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#docker-advanced",
    "href": "index.html#docker-advanced",
    "title": "Transport Data Science",
    "section": "Docker (advanced)",
    "text": "Docker (advanced)\nWe maintain a Docker image containing all necessary software to complete the course with VS Code, Quarto, and a Devcontainer setup.\nAdvantages:\n\nEnsures reproducibility\nSaves time installing software\n\nDisadvantages:\n\nDocker can be challenging to install\nDifficult to use if you’re unfamiliar with Docker\n\nWe recommend this approach only for people who are confident with Docker or willing to invest time learning it.\nFor guidance, see:\n\nDocker installation instructions\nDevcontainers documentation on github.com\nThe tds Dockerfile and devcontainer.json",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Transport Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n For further guidance on setting-up your computer to run R and RStudio for spatial data, see these links, we recommend Chapter 2 of Geocomputation with R (the Prerequisites section contains links for installing spatial software on Mac, Linux and Windows): https://r.geocompx.org/spatial-class.html and Chapter 2 of the online book Efficient R Programming, particularly sections 2.3 and 2.5, for details on R installation and set-up and the project management section.↩︎",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "assessment-overview.html",
    "href": "assessment-overview.html",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "",
    "text": "This module’s assessments are designed to help you:\n\nDevelop practical data science skills for solving real-world transport problems\nApply programming and analysis techniques to transport datasets\nGenerate insights from transport data that can inform policy and planning decisions\nDemonstrate reproducible research practices in transport studies"
  },
  {
    "objectID": "assessment-overview.html#objectives",
    "href": "assessment-overview.html#objectives",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "",
    "text": "This module’s assessments are designed to help you:\n\nDevelop practical data science skills for solving real-world transport problems\nApply programming and analysis techniques to transport datasets\nGenerate insights from transport data that can inform policy and planning decisions\nDemonstrate reproducible research practices in transport studies"
  },
  {
    "objectID": "assessment-overview.html#module-assessment-structure",
    "href": "assessment-overview.html#module-assessment-structure",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "2 Module Assessment Structure",
    "text": "2 Module Assessment Structure\nThe module is assessed through two coursework assignments:\n\nFormative Assessment (CW1)\n\nRequired but non-assessed (0% of final mark)\nDue: 28th February 2025, 13:59\nProject plan and reproducible code demonstration\nLength: 2 pages recommended (5 pages maximum)\n\nSummative Assessment (CW2)\n\nWorth 100% of module mark\nDue: 16th May 2025, 14:00\nComplete data science project report\nLength: 10 pages maximum + appendices"
  },
  {
    "objectID": "assessment-overview.html#file-naming-convention",
    "href": "assessment-overview.html#file-naming-convention",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "3 File Naming Convention",
    "text": "3 File Naming Convention\nYou must name your files using the following format:\nTRAN5340M_StudentIDNumber.file_type\nFor example: - TRAN5340M_201234567.zip"
  },
  {
    "objectID": "assessment-overview.html#submission-format-requirements",
    "href": "assessment-overview.html#submission-format-requirements",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "4 Submission Format Requirements",
    "text": "4 Submission Format Requirements\n\n4.1 Document Formatting\n\nInclude student ID in the title page\nDo not include your name (for anonymous marking)\nUse the default Quarto referencing style\n\n\n\n4.2 File Requirements\nFormative Assessment Package: - PDF report (max 5 pages) - Reproducible code (.qmd file) - Maximum .zip file size: 30 MB\nSummative Assessment Package: - PDF report (max 10 pages) - Maximum 3,000 words (excluding tables/code/references/captions) - Reproducible code (.Rmd or .qmd file) - Maximum .zip file size: 40 MB"
  },
  {
    "objectID": "assessment-overview.html#submission-process",
    "href": "assessment-overview.html#submission-process",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "5 Submission Process",
    "text": "5 Submission Process\n\nPrepare Your Submission\n\nEnsure correct file naming\nCheck formatting requirements\nTest code reproducibility\nVerify file sizes\n\nSubmission: Via Minerva (Blackboard Assignment)\n\nDeadline is 14:00 on submission day\nEach assignment has its own submission point\nKeep submission confirmation"
  },
  {
    "objectID": "assessment-overview.html#marking-criteria",
    "href": "assessment-overview.html#marking-criteria",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "6 Marking Criteria",
    "text": "6 Marking Criteria\n\n6.1 Summative Assessment (100% of module mark)\n\nData Processing (20%)\n\nDataset selection and usage\nData cleaning and preparation\nFeature engineering\nData transformation\n\nVisualization and Report Quality (20%)\n\nFigure design and clarity\nProfessional formatting\nEffective communication\nDocumentation quality\n\nCode Quality and Reproducibility (20%)\n\nCode efficiency and style\nDocumentation\nReproducibility\nTechnical implementation\n\nUnderstanding and Impact (40%)\n\nResearch question clarity\nMethodological approach\nCritical analysis\nPolicy implications\nLiterature engagement"
  },
  {
    "objectID": "assessment-overview.html#important-notes",
    "href": "assessment-overview.html#important-notes",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "7 Important Notes",
    "text": "7 Important Notes\n\n7.1 Topic Selection\n\nTopics should address real transport planning/policy challenges\nThe module team can provide guidance on topic selection\nGuidance on topics and datasets is provided in the module documents, including the formative assessment brief\n\n\n\n7.2 Use of AI Tools\nBoth assessments are categorized as AMBER for AI usage: - AI tools permitted in assistive role only - Usage must be acknowledged - Core analysis must be original work\n\n\n7.3 Academic Integrity\n\nAll work must meet university standards\nProper referencing required\nPlagiarism checks applied through Turnitin"
  },
  {
    "objectID": "assessment-overview.html#support-available",
    "href": "assessment-overview.html#support-available",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "8 Support Available",
    "text": "8 Support Available\n\nAcademic Support\n\nModule team can be contacted via email\nWeekly sessions\n\nTechnical Support\n\nCode templates and examples provided in the course website\nR/RStudio guidance\nData access support\n\nWriting Support\n\nSkills@Library\nAcademic writing guidance\nReferencing support"
  },
  {
    "objectID": "assessment-overview.html#key-dates-2024-25",
    "href": "assessment-overview.html#key-dates-2024-25",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "9 Key Dates (2024-25)",
    "text": "9 Key Dates (2024-25)\n\n28 February 2025, 13:59: Formative assessment (CW1)\n16 May 2025, 14:00: Summative assessment (CW2)\nFeedback provided within 15 working days"
  },
  {
    "objectID": "assessment-overview.html#assessment-checklist",
    "href": "assessment-overview.html#assessment-checklist",
    "title": "Assessment Overview: Transport Data Science (TRAN5340M)",
    "section": "10 Assessment Checklist",
    "text": "10 Assessment Checklist\n\n10.1 For Both Submissions\n\nCorrect file naming format used\nWork meets length requirements\nCode is reproducible\nZipped file size is within limits (you may need to remove large datasets from a copy of the folder to ensure this)\nAI usage is acknowledged\nReferences section included\n\n\n\n10.2 Additional for Summative\n\nMeets marking criteria\nIncludes policy implications\nReport is cohesive\nProfessional presentation"
  },
  {
    "objectID": "slides/intro.html#who-transport-data-science-team",
    "href": "slides/intro.html#who-transport-data-science-team",
    "title": "Introduction to transport data science",
    "section": "Who: Transport Data Science team",
    "text": "Who: Transport Data Science team\nRobin Lovelace\n\nAssociate Professor of Transport Data Science\nResearching transport futures and active travel planning\nR developer and teacher, author of Geocomputation with R\n\nYuanxuan Yang\n\nLecturer in Data Science of Transport\nNew and Emerging Forms of Data: Investigating novel data sources and their applications in urban mobility and transport planning."
  },
  {
    "objectID": "slides/intro.html#tds-team-ii",
    "href": "slides/intro.html#tds-team-ii",
    "title": "Introduction to transport data science",
    "section": "TDS Team II",
    "text": "TDS Team II\nMalcolm Morgan\n\nSenior researcher at ITS with expertise in routing + web\nDeveloper of the Propensity to Cycle Tool and PBCC\n\nZhao Wang\n\nCivil Engineer and Data Scientist with expertise in machine learning\n\nDemonstrators\n\nJuan Pablo Fonseca Zamora\n\nYou!"
  },
  {
    "objectID": "slides/intro.html#what-is-transport-data-science",
    "href": "slides/intro.html#what-is-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "What is transport data science?",
    "text": "What is transport data science?\n\nThe application of data science to transport datasets and problems\nRaising the question…\nWhat is data science?\nA discipline “that allows you to turn raw data into understanding, insight, and knowledge” (Grolemund, 2016)\n\nIn other words…\n\nStatistics that is actually useful!"
  },
  {
    "objectID": "slides/intro.html#why-take-transport-data-science",
    "href": "slides/intro.html#why-take-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "Why take Transport Data Science",
    "text": "Why take Transport Data Science\n\n\n\nNew skills (cutting edge R and/or Python packages)\nPotential for impacts\nAllows you to do new things with data\nIt might get you a job!"
  },
  {
    "objectID": "slides/intro.html#live-demo-npt.scot-web-app",
    "href": "slides/intro.html#live-demo-npt.scot-web-app",
    "title": "Introduction to transport data science",
    "section": "Live demo: npt.scot web app",
    "text": "Live demo: npt.scot web app"
  },
  {
    "objectID": "slides/intro.html#the-history-of-tds",
    "href": "slides/intro.html#the-history-of-tds",
    "title": "Introduction to transport data science",
    "section": "The history of TDS",
    "text": "The history of TDS\n\n2017: Transport Data Science created, led by Dr Charles Fox, Computer Scientist, author of Transport Data Science book (Fox, 2018)\nThe focus was on databases and Bayesian methods\n2019: I inherited the module, which was attended by ITS students\nSummer 2019: Python code published in the module ‘repo’:\n\ngithub.com/ITSLeeds"
  },
  {
    "objectID": "slides/intro.html#history-of-tds-ii",
    "href": "slides/intro.html#history-of-tds-ii",
    "title": "Introduction to transport data science",
    "section": "History of TDS II",
    "text": "History of TDS II\n\nJanuary 2020: Available, Data Science MSc course\nMarch 2020: Switch to online teaching\n2021-2023: Updated module, focus on methods\n2024: Switch to combined lecture and practical sessions\n2025+: Expand, online course? book? stay in touch!\n\n\n\nMilestone passed in my academic career, first online-only delivery of lecture @ITSLeeds, seems to have worked, live code demo with #rstats/rstudio, recording, chat + all🎉Thanks students for ‘attending’ + remote participation, we’ll get through this together.#coronavirus pic.twitter.com/wlAUxmZj5r\n\n— Robin Lovelace March 17, 2020"
  },
  {
    "objectID": "slides/intro.html#essential-reading",
    "href": "slides/intro.html#essential-reading",
    "title": "Introduction to transport data science",
    "section": "Essential reading",
    "text": "Essential reading\n\nChapter 13, Transportation of Geocomputation with R, a open book on geographic data in R (available free online) (Lovelace et al. 2019)\nReproducible Road Safety Research with R (RRSRR): https://itsleeds.github.io/rrsrr/"
  },
  {
    "objectID": "slides/intro.html#core-reading-materials",
    "href": "slides/intro.html#core-reading-materials",
    "title": "Introduction to transport data science",
    "section": "Core reading materials",
    "text": "Core reading materials\n\nR for Data Science, an introduction to data science with R (available free online)\nPython equivalent"
  },
  {
    "objectID": "slides/intro.html#optional",
    "href": "slides/intro.html#optional",
    "title": "Introduction to transport data science",
    "section": "Optional",
    "text": "Optional\nThere are many good resources on data science for transport applications. Do your own research and reading! The following are good:\n\nIf you’re interested in network analysis/Python, see this paper on analysing OSM data in Python (Boeing and Waddell, 2017) (available online)\nIf you’re interested in the range of transport modelling tools, see Lovelace (2021). \n\nFor more references, see the bibliography at github.com/ITSLeeds/TDS"
  },
  {
    "objectID": "slides/intro.html#objectives",
    "href": "slides/intro.html#objectives",
    "title": "Introduction to transport data science",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the structure of transport datasets\nUnderstand how to obtain, clean and store transport related data\nGain proficiency in command-line tools for handling large transport datasets\nProduce data visualizations, static and interactive\n Learn how to join together the components of transport data science into a cohesive project portfolio"
  },
  {
    "objectID": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "href": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "title": "Introduction to transport data science",
    "section": "Assessment (for those doing this as credit-bearing)",
    "text": "Assessment (for those doing this as credit-bearing)\n\nYou will build-up a portfolio of work\n100% coursework assessed, you will submit by\nWritten in code - will be graded for reproducibility\nCode chunks and figures are encouraged\nYou will submit a non-assessed 2 page pdf + qmd"
  },
  {
    "objectID": "slides/intro.html#schedule",
    "href": "slides/intro.html#schedule",
    "title": "Introduction to transport data science",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "slides/intro.html#feedback",
    "href": "slides/intro.html#feedback",
    "title": "Introduction to transport data science",
    "section": "Feedback",
    "text": "Feedback\n\nThe module is taught by two really well organised and enthusiastic professors, great module, the seminars, structured and unstructured learning was great and well thought out, all came together well\n\n\nI wish this module was 60 credits instead of 15 because i just want more of it."
  },
  {
    "objectID": "slides/professions.html#reminder-on-coursework",
    "href": "slides/professions.html#reminder-on-coursework",
    "title": "Professional transport data science workflows and project work",
    "section": "Reminder on Coursework",
    "text": "Reminder on Coursework\n\nWhen’s the deadline?\n\n\nas.Date(\"2025-05-16\")\n\n\nRemember the marking criteria:"
  },
  {
    "objectID": "slides/professions.html#data-science-workflows",
    "href": "slides/professions.html#data-science-workflows",
    "title": "Professional transport data science workflows and project work",
    "section": "Data science workflows",
    "text": "Data science workflows\nSource: R for Data Science open source book"
  },
  {
    "objectID": "slides/professions.html#questions-to-consider-and-discuss-with-the-module-team",
    "href": "slides/professions.html#questions-to-consider-and-discuss-with-the-module-team",
    "title": "Professional transport data science workflows and project work",
    "section": "Questions to consider and discuss with the module team",
    "text": "Questions to consider and discuss with the module team\nAround 1 hour for this, in parallel with solo working on projects, providing time for the module to talk to each student\n\nWhat are the strongest aspects of your coursework idea so far? What are the weakest?\nWhat are the priorities over the next three weeks (break it down into a small number of parts)?\nWhat do you need to find more literature on?\nWhat do you need more data on (you should have all the data already)?\nWhat additional skills do you need (now is a good time to ask)?"
  },
  {
    "objectID": "slides/professions.html#working-on-your-projects",
    "href": "slides/professions.html#working-on-your-projects",
    "title": "Professional transport data science workflows and project work",
    "section": "Working on your projects",
    "text": "Working on your projects\n\nPlan ahead: what else do you need to do on your project?\nSchedule work: when will you find time to do it?\nUse Microsoft Calendar or similar: put it in the calendar.\nReproducibility: ensure your .Rmd files are reproducible\nAsk for help: what do you need help with?\nWork: get you head down and make use of this time!"
  },
  {
    "objectID": "s5/index.html",
    "href": "s5/index.html",
    "title": "Session 5: Visualising transport data",
    "section": "",
    "text": "1 Introduction\nIn this session, we will build on the routing techniques from Session 4 by exploring data visualization methods for transport analysis. By the end of this session, you should be able to:\n\nLoad and preprocess OD flow data\nVisualize OD lines and proportional symbol maps\nCompare walking, driving, and cycling flows\nAggregate flows along the road network\nIdentify critical road segments via network centrality\n\n\n\n2 Setup\nBelow are the libraries we will use throughout this session:\n\nknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  fig.width = 7,\n  fig.height = 5,\n  out.width = \"700px\"\n)\n\n# Load necessary libraries\nlibrary(opentripplanner)  # Routing engine (OpenTripPlanner client)\nlibrary(sf)               # Spatial data handling\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\nlibrary(tmap)             # Thematic mapping\nlibrary(stplanr)          # Transport data functions\nlibrary(dplyr)            # Data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(osmextract)       # OSM data handling\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\nCheck the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nlibrary(dodgr)            # Network analysis\n\n\n# Set interactive mapping mode\ntmap_mode(\"view\")\n\n\n\n3 Flow Map Visualization\nFlow maps are useful for understanding the volume of travel between origins and destinations. In this section, we will:\n\nLoad desire lines (flows) data from a GeoJSON file.\nVisualize these lines with widths or colors proportional to demand.\nOptionally aggregate route geometries for more realistic depiction of flows along an actual road network.\n\n\n# Load Demand Data\ndesire_lines = read_sf(\"https://github.com/ITSLeeds/TDS/releases/download/22/NTEM_flow.geojson\") |&gt;\n  select(from, to, all, walk, drive, cycle)\n\ndim(desire_lines)\n\n[1] 502   7\n\n# Let's take the top 50 car trips for demonstration\ndesire_lines_top = desire_lines |&gt;\n  arrange(desc(drive)) |&gt;\n  head(50)\n\n# Quick map to see the distribution of car trips\ntm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"drive\",\n    lwd.scale = tm_scale_continuous(values.scale = 9)\n  ) +\n  tm_layout(legend.bg.color = \"white\")\n\n\n\n\n\n\n\n\n4 Proportional Symbol Flow Maps\nNow, let’s illustrate an alternative method: proportional symbols at origin or destination points. This is useful when you want to quickly see where demand is concentrated.\n\n# Summarize total flows by origin\n\norigin_flows = desire_lines |&gt;\n  group_by(from) |&gt;\n  summarise(\n    total_drive = sum(drive, na.rm = TRUE),\n    total_walk  = sum(walk, na.rm = TRUE),\n    total_cycle = sum(cycle, na.rm = TRUE),\n    `% drive` = total_drive / sum(all, na.rm = TRUE),\n    geometry = st_centroid(st_union(geometry))  \n  )\n\n# Simple map with proportional circles for drive volumes\n\ntm_shape(origin_flows) +\n  tm_bubbles(\n    size    = \"total_drive\",       # bubble size ~ drive volume\n    size.scale = tm_scale_intervals(values.scale = 2, values.range = c(0.5, 2)),\n    fill = \"% drive\",\n    fill.scale = tm_scale_continuous(values = \"brewer.reds\")\n  ) +\n  tm_title(\"Proportional Symbol Map of Drive Demand by Origin\")\n\n\n\n\n\n\n\nEach origin is represented by a circle whose radius and color intensity reflect the total number of driving trips. You can modify palettes, breaks, and scaling to highlight variations.\n\n\n5 Mode-Specific Analysis\nWe have have columns walk, drive, cycle in desire_lines. We can map them separately or side-by-side. We can also color lines by the dominant mode.\n\n# Let's create 3 separate maps: drive, walk, cycle\ntmap_mode(\"plot\")\nm_drive = tm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"drive\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"red\"\n  ) +\n  tm_title(\"Driving Flows\")\n\nm_walk = tm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"walk\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"green\"\n  ) +\n  tm_title(\"Walking Flows\")\n\nm_cycle = tm_shape(desire_lines_top) +\n  tm_lines(\n    lwd = \"cycle\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"blue\"\n  ) +\n  tm_title(\"Cycling Flows\")\n\ntmap_arrange(m_drive, m_walk, m_cycle, ncol=3)\n\n\n\n\n\n\n\n\nThis tmap_arrange() will output a single figure with three columns, each illustrating flows by one mode. Students can visually compare the differences: maybe driving flows are much thicker on longer corridors, while walking flows are concentrated in the city center.\n\n\n6 Aggregating Flows with Actual Routes\nRather than drawing direct origin-destination lines, we can route each flow along the road network and then aggregate them to see which streets carry the most traffic. This uses stplanr::overline() to merge lines that overlap.\n\n# Download pre-routed lines for demonstration\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/routes_drive_25.geojson\"\nroutes_drive = read_sf(u)\n\n# Inspect the summary of the drive.x variable (car trips)\nsummary(routes_drive$drive.x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   41.0   171.2   327.0   415.1   546.5  2346.0 \n\ntm_shape(routes_drive) +\n  tm_lines(\n    lwd = \"drive.x\",\n    lwd.scale = tm_scale_continuous(values.scale = 9),\n    col = \"red\"\n  ) +\n  tm_title(\"Road Congestion (drive_total)\")\n\n\n\n\n\n\n\n\n\n\n7 Network Centrality Analysis\nBetweenness centrality indicates how often a road (or node) lies on the shortest path between other points in a network. Roads with high centrality are typically crucial for overall connectivity.\nHere, we demonstrate how to:\n\nDownload roads from OpenStreetMap (OSM) for the Isle of Wight.\nWeight the network for motor vehicle usage.\nCompute betweenness centrality with dodgr.\nConvert the results back to sf for mapping.\n\n\n# Get Isle of Wight road network\n# We choose 'primary', 'secondary', 'tertiary' roads for demonstration\nroads = oe_get(\"Isle of Wight\", \n                extra_tags = c(\"maxspeed\", \"oneway\")) |&gt;\n  filter(highway %in% c(\"primary\", \"secondary\", \"tertiary\"))\n\n# Weight the street network for motorcar usage\ngraph = weight_streetnet(\n  roads,\n  wt_profile = \"motorcar\",\n  type_col = \"highway\",\n  id_col = \"osm_id\",\n  keep_cols = c(\"maxspeed\", \"oneway\")\n)\n\n# Calculate betweenness centrality\ncentrality = dodgr_centrality(graph)\n\n# Convert to sf for visualization\ncentrality_sf = dodgr_to_sf(centrality)\n\n# Visualize critical links\ntm_shape(centrality_sf) +\n  tm_lines(\n    col = \"centrality\",\n    col.scale = tm_scale_intervals(style = \"fisher\", values = \"-viridis\"),\n    col.legend = tm_legend(title = \"Betweenness Centrality\"),\n    lwd = 3\n  ) \n\nThe code above should generate a map that looks something like this:\n\nHigh values in the centrality column indicate roads that act as vital connectors in the regional transport network.\n\n\n8 Extra Exercises: 3D Visualization\nA 3D perspective can often reveal relationships between travel flows and the underlying topography more effectively. Below, we demonstrate how to retrieve elevation data and render a 3D hillshade using the rayshader package. You may also be interested in overlaying flow lines onto a 3D terrain model to enhance visualization.\nNote: the following code requires you to install the rayshader elevatr gifski rgl package. Results not shown in website.\n\nlibrary(rayshader)        # 3D data visualization\nlibrary(elevatr)          # Elevation data\nlibrary(gifski)           # Creating GIF animations\nlibrary(rgl)              # 3D visualization device\n\nassign(\"has_internet_via_proxy\", TRUE, environment(curl::has_internet))\ncurl::has_internet()\n# Example: Elevation data near a location in the UK\ncoords = data.frame(x = -2.087918, y = 53.71534)\ncoords_sf = st_as_sf(coords, coords = c(\"x\", \"y\"), crs = 4326)\n# Get an elevation raster at zoom level 11 (~ 10m resolution, depending on region)\nelevation = elevatr::get_elev_raster(\n  locations = coords_sf,\n  z = 11\n)\n# Convert the raster to a matrix for rayshader\nelev_matrix = rayshader::raster_to_matrix(elevation)\n# Create a hillshade layer\nhillshade_matrix = rayshader::ray_shade(elev_matrix, zscale = 15)\n# Clear existing rgl device\nrgl::rgl.clear()\n# Render a 3D plot of the terrain\nrayshader::plot_3d(\n  heightmap = elev_matrix,\n  hillshade = hillshade_matrix,\n  zscale = 15,\n  windowsize = c(1000, 800)\n)\n# Adjust camera view\nrgl::view3d(theta = 30, phi = 30, zoom = 0.75)\nrgl::rglwidget()\n\n\n\n9 Conclusions\nIn this session, you learned how to:\n\nCreate flow maps to visualize travel demand from an OD dataset.\nCompare flows by mode (driving, walking, cycling) to understand differences in spatial patterns.\nAggregate routes along the road network (using stplanr::overline) to highlight heavily used corridors.\nCompute betweenness centrality (using dodgr) to pinpoint critical road segments crucial for connectivity."
  },
  {
    "objectID": "s2/slides.html#objectives",
    "href": "s2/slides.html#objectives",
    "title": "Accessing data from the Internet",
    "section": "Objectives",
    "text": "Objectives\n\nLearn where to find large transport datasets and assess data quality"
  },
  {
    "objectID": "s2/slides.html#learning-outcomes",
    "href": "s2/slides.html#learning-outcomes",
    "title": "Accessing data from the Internet",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nIdentify available datasets and access and clean them"
  },
  {
    "objectID": "s2/slides.html#this-lecture-will",
    "href": "s2/slides.html#this-lecture-will",
    "title": "Accessing data from the Internet",
    "section": "This lecture will…",
    "text": "This lecture will…\n\nBe primarily practical\nProvide an overview of data access options\nShow how R packages and web services provide access to some datasets"
  },
  {
    "objectID": "s2/slides.html#data-access-in-context",
    "href": "s2/slides.html#data-access-in-context",
    "title": "Accessing data from the Internet",
    "section": "Data access in context",
    "text": "Data access in context\n\nData cleaning (or ‘tidying’ or ‘wrangling’) is part of a wider process (Wickham, Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\nIt’s important to have an idea where you’re heading with the analysis\nOften best to start with pen and paper"
  },
  {
    "objectID": "s2/slides.html#data-accesscleaning-vs-modelling-time",
    "href": "s2/slides.html#data-accesscleaning-vs-modelling-time",
    "title": "Accessing data from the Internet",
    "section": "Data access/cleaning vs modelling time",
    "text": "Data access/cleaning vs modelling time\n\n\nTapson’s Rules of Machine Learning:4. Time spent on data cleaning is an order of magnitude more productive than time spent on hyperparameter tuning.(Extreme example: achieved a Top 10 result in Kaggle using linear regression, as the only team that cleaned 50/60Hz noise first.)\n\n— Jonathan Tapson ((jontapson?)) March 5, 2019\n\n\nSource: https://twitter.com/jontapson/status/1103024752019402753\nbackground-image: url() background-size: cover class: center, middle"
  },
  {
    "objectID": "s2/slides.html#information-and-data-pyramids",
    "href": "s2/slides.html#information-and-data-pyramids",
    "title": "Accessing data from the Internet",
    "section": "Information and data pyramids",
    "text": "Information and data pyramids\nData science is climbing the DIKW pyramid"
  },
  {
    "objectID": "s2/slides.html#a-geographic-availability-pyramid",
    "href": "s2/slides.html#a-geographic-availability-pyramid",
    "title": "Accessing data from the Internet",
    "section": "A geographic availability pyramid",
    "text": "A geographic availability pyramid\n\nRecommendations\nBuild this here!\nCity-specific datasets\n\nBristol cycle count data\n\nHard-to-access national data\nOpen international/national datasets\n\nOpen origin-destination data from UK Census\n\nGlobally available, low-grade data (bottom)\n\nOpenStreetMap, Elevation data"
  },
  {
    "objectID": "s2/slides.html#an-ease-of-access-pyramid",
    "href": "s2/slides.html#an-ease-of-access-pyramid",
    "title": "Accessing data from the Internet",
    "section": "An ease-of access pyramid",
    "text": "An ease-of access pyramid\n\nData provision packages\n\nUse the pct package\nstats19 package\n\nPre-processed data\n\nE.g. downloading data from website www.pct.bike\n\nMessy official data\n\nRaw STATS19 data"
  },
  {
    "objectID": "s2/slides.html#a-geographic-level-of-detail-pyramid",
    "href": "s2/slides.html#a-geographic-level-of-detail-pyramid",
    "title": "Accessing data from the Internet",
    "section": "A geographic level of detail pyramid",
    "text": "A geographic level of detail pyramid\n\nAgents\nRoute networks\nNodes\nRoutes\nDesire lines\nTransport zones"
  },
  {
    "objectID": "s2/slides.html#observations",
    "href": "s2/slides.html#observations",
    "title": "Accessing data from the Internet",
    "section": "Observations",
    "text": "Observations\n\nOfficial sources are often smaller in sizes but higher in Quality\nUnofficial sources provide higher volumes but tend to be noisy\nAnother way to classify data is by quality: signal/noise ratios\nGlobally available datasets would be at the bottom of this pyramid; local surveys at the top.\nWhich would be best to inform policy?"
  },
  {
    "objectID": "s2/slides.html#portals",
    "href": "s2/slides.html#portals",
    "title": "Accessing data from the Internet",
    "section": "Portals",
    "text": "Portals\n\nUK geoportal, providing geographic data at many levels\nOther national geoportals exist\nA good source of cleaned origin destination data is the Region downloads tab in the Propensity to Cycle Tool - see the Region data tab for West Yorkshire here, for example\nOpenStreetMap is an excellent source of geographic data with global coverage. You can download data on specific queries (e.g. highway=cycleway) from the overpass-turbo service or with the osmdata or osmextract packages"
  },
  {
    "objectID": "s2/slides.html#online-lists",
    "href": "s2/slides.html#online-lists",
    "title": "Accessing data from the Internet",
    "section": "Online lists",
    "text": "Online lists\nFor other datasets, search online! Good starting points in your research may be:\n\nThe open data section in Geocomputation with R (r.geocompx.org/read-write) \nTransport datasets mentioned in data.world \nUK government transport data: Department for Transport"
  },
  {
    "objectID": "s2/slides.html#data-packages",
    "href": "s2/slides.html#data-packages",
    "title": "Accessing data from the Internet",
    "section": "Data packages",
    "text": "Data packages\n\nThe openrouteservice github package provides routing data\nThe stats19 package can get road crash data for anywhere in Great Britain (Lovelace et al. 2019) see docs.ropensci.org/stats19 \nThe pct package provides access to data in the PCT project, including origin-destination data for the UK (Lovelace et al. 2017) see github.com/ITSLeeds/pct \nThere are many other R packages to help access data, including the spanishoddata package for Spanish origin-destination data"
  },
  {
    "objectID": "s2/slides.html#demo",
    "href": "s2/slides.html#demo",
    "title": "Accessing data from the Internet",
    "section": "Demo",
    "text": "Demo\nSee session activities at itsleeds.github.io/tds/s2/\n\nThat involves:\nGetting data from OSM: overpass turbo\nData from stats19\nData from the Census\nBonus: getting data from Cadence platform"
  },
  {
    "objectID": "s2/slides.html#references",
    "href": "s2/slides.html#references",
    "title": "Accessing data from the Internet",
    "section": "References",
    "text": "References\n\n\n\n\nLovelace, Robin, Anna Goodman, Rachel Aldred, Nikolai Berkoff, Ali Abbas, and James Woodcock. 2017. “The Propensity to Cycle Tool: An Open Source Online System for Sustainable Transport Planning.” Journal of Transport and Land Use 10 (1). https://doi.org/10.5198/jtlu.2016.862.\n\n\nLovelace, Robin, Malcolm Morgan, Layik Hama, and Mark Padgham. 2019. “Stats19: A Package for Working with Open Road Crash Data.” Journal of Open Source Software. https://doi.org/10/gkb498.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "s2/homework.html",
    "href": "s2/homework.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Hi all,\nFor anyone who wanted to recap on Session 1 or Session 2 content, see the recorded sessions + manuscripts here:\n\nSession 1: TDS Session 1_ Introduction to transport data science (RL, MM, ZW, YY)-20250130_100448-Meeting Recording.mp4\nSession 2: TDS Session 2_ Getting transport data (RL, YY)-20250206_100118-Meeting Recording.mp4\n\nAlso, please note the updated homework here (note the new item 4 and bonus 5, if you get stuck or hit any error messages just let me know): https://itsleeds.github.io/tds/s2/#homework\nYou will present the code / .qmd files you wrote as part of this homework to colleagues in the next session, so please come prepared, any visualisations of outputs and questions you would like to asks demonstrators especially welcome.\nGreat work everyone and thanks for engaging so well with the content today and looking forward to the session on origin-destination data next week.\nRobin"
  },
  {
    "objectID": "s4/index.html",
    "href": "s4/index.html",
    "title": "Session 4: Routing",
    "section": "",
    "text": "In this session, we will learn how to use routing engines and network analysis. The contents of the session are as follows:\n\nWe will start with reviewing the homework from the previous session\nA lecture on graph theory and routing. Slides here\nSession activities: routing in R\nHomework and next session"
  },
  {
    "objectID": "s4/index.html#setup",
    "href": "s4/index.html#setup",
    "title": "Session 4: Routing",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nIf you have not installed the packages before hand. You can use ITS Go to do an easy set-up of your computer\n\nsource(\"https://git.io/JvGjF\")\n\nThe packages we will be using are:\n\nlibrary(sf)               # Spatial data functions\nlibrary(lwgeom)           # For advanced spatial functions\nlibrary(tidyverse)        # General data manipulation\nlibrary(stplanr)          # General transport data functions\nlibrary(dodgr)            # Local routing and network analysis\nlibrary(opentripplanner)  # Connect to and use OpenTripPlanner\nlibrary(tmap)             # Make maps\nlibrary(osmextract)       # Download and import OpenStreetMap data\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "s4/index.html#using-opentripplanner-to-get-routes",
    "href": "s4/index.html#using-opentripplanner-to-get-routes",
    "title": "Session 4: Routing",
    "section": "3.2 Using OpenTripPlanner to get routes",
    "text": "3.2 Using OpenTripPlanner to get routes\nWe have set up the Multi-modal routing service OpenTripPlanner for West Yorkshire. Try typing this URL https://otp.robinlovelace.net/ during the session into your browser. You should see something like this:\n Note if you see a grey map use the layers button in the top right to switch to a different basemap.\n\n3.2.1 OTP Web GUI\nExercise\n\nPlay with the web interface, finding different types of routes. What strengths/limitations can you find?\n\n\n\n3.2.2 Connecting to OpenTripPlanner in R\nTo allow R to connect to the OpenTripPlanner server, we will use the opentripplanner package and the function otp_connect.\n\n# ip = \"localhost\" # to run it on your computer (see final bonus exercise)\nip = \"otp.robinlovelace.net\" # an actual server\notpcon = otp_connect(\n  hostname = ip, \n  ssl = TRUE,\n  port = 443,\n  router = \"west-yorkshire\"\n)\n\n\nIf you have connected successfully, then you should get a message “Router exists.”\nCreate a test route. Notice than in the web UI the coordinates are Lat/Lng but R uses Lng/Lat\n\nroutes_test = otp_plan(\n  otpcon = otpcon,\n  fromPlace = c(-1.55555, 53.81005),\n  toPlace = c(-1.54710, 53.79519),\n  mode = \"WALK\"\n)\n\nYou can use multiple modes and combinations try:\n\nmode = \"WALK\"\nmode = c(\"WALK\",\"TRANSIT\")\nmode = c(\"BICYCLE\",\"TRANSIT\")\nmode = \"CAR\"\nmode = c(\"CAR_PARK\",\"TRANSIT\")\n\nUse some of the functions you have already learnt like head,plot, and summary to understand the data you have produced.\nTo get some more routes, we will start by importing some data. The NTEM_flow.geojson dataset the contains the top desire lines in West Yorkshire. It was produced from a transport model called the National Trip End Model and research from the University of Leeds.\n\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/NTEM_flow.geojson\"\ndesire_lines = read_sf(u)\nhead(desire_lines)\n\nWe will also download the points that represent the possible start and end point of trips in the model\n\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/NTEM_cents.geojson\"\ncentroids = read_sf(u)\nhead(centroids)\n\nExercise\n\nPlot the desire_lines and centroids objects using the tmap package to show the number of travellers on each desire_line and the locations of all centroids.\n\n\ntmap_mode(\"plot\") # Change to view for interactive map\ntm_shape(desire_lines) +\n  tm_lines(col = \"all\",\n           lwd = \"all\",  \n           lwd.scale = tm_scale_continuous(values.scale = 10), \n           col.scale = tm_scale_continuous(values  = \"-viridis\")) +\n  tm_shape(centroids) +\n  tm_dots(fill = \"red\")\n\n\nProduce some different maps for each mode of travel in the desire_lines dataset. How do the numbers of travellers change for walking, driving, and train travel? See example plot below.\n\nThis dataset has desire lines, but most routing packages need start and endpoints, so we will extract the start and endpoints using the package lwgeom\nExercise\n\nProduce a data frame called desire_top which contains the top three desire_lines for all travellers. Hint ?slice_max\n\n\nWe need to extract start and end point from those desire lines. We would also like to give each place an ID value\n\n\n# Extract start and end points\nfromPlace = sf::st_sf(\n  data.frame(id = desire_top$from),\n  geometry = lwgeom::st_startpoint(desire_top)\n)\ntoPlace = sf::st_sf(\n  data.frame(id = desire_top$to),\n  geometry = lwgeom::st_endpoint(desire_top)\n)\n\n\nCreate a new object called routes_drive_top, with driving routes between the OD pairs represented in the desire_top object.\n\nCalculate routes for the first three desire lines with the following command:\n\nroutes_drive_top = otp_plan(otpcon = otpcon,\n  otpcon = otpcon,\n  fromPlace = fromPlace,\n  toPlace = toPlace,\n  fromID = fromPlace$id,\n  toID = toPlace$id,\n  mode = \"CAR\"\n)\n\n\nPlot routes_drive_top using the tmap package mode. You should see something like the image below.\n\n\ntm_shape(routes_drive_top) + tm_lines()\n\n\n\n3.2.3 Isochrones\nWe can also get Isochrones from OTP. In this case lets work out how far we can travel in one hour by cycling and public transport.\n\nisochrone = otp_isochrone(otpcon, \n                          fromPlace = c(-1.558655, 53.807870), \n                          mode = c(\"BICYCLE\",\"TRANSIT\"),\n                          maxWalkDistance = 3000)\nisochrone$time = isochrone$time / 60 # Convert from seconds to minutes\ntm_shape(isochrone) +\n  tm_fill(\"time\", alpha = 0.6)\n\nExperiment with some different isochrones by changing the mode, and start location. Ty overlaying the OD data on top of you isochrones. Can you see a relationship between travel time and travel demand?\nTo save you time and to prevent overloading the server, we have pre-generated some extra routes. Download these routes and load them into R.\n\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/routes_drive.geojson\"\nroutes_drive = read_sf(u)\nu = \"https://github.com/ITSLeeds/TDS/releases/download/22/routes_transit.geojson\"\nroutes_transit = read_sf(u)\n\nWe will now join the number of drivers onto the driving routes.\nExercise\n\nCreate a dataset called n_driver from desire_lines which only have the columns from to and drive. Hint ?dplyr::select and ?sf::st_drop_geometry\n\n\nJoin the n_driver data onto the routes_drive data by linking fromPlace = from and toPlace = to. Hint ?dplyr::left_join.\n\n\nPlot the routes showing the number of drivers on each route."
  },
  {
    "objectID": "s4/index.html#route-networks-also-called-flow-maps",
    "href": "s4/index.html#route-networks-also-called-flow-maps",
    "title": "Session 4: Routing",
    "section": "3.3 Route Networks (also called flow maps)",
    "text": "3.3 Route Networks (also called flow maps)\nThe map above shows some useful information about where people drive. But it has a problem. When many routes overlap it hides some of the drivers. What would be more useful would be to add those drivers together so we can see the total number of drivers on each road. This is what a route network does.\nWe can produce a route network map using stplanr::overline.\n\nrnet_drive = overline(routes_drive, \"drive\")\n\nExercise\n\nMake a route network for driving and plot it using the tmap package. How is is different from just plotting the routes?\n\nBonus Exercise\n\nRead the paper about the overline function.\n\n\n3.3.1 Line Merging\nNotice that routes_transit has returned separate rows for each mode (WALK, RAIL, BUS). Notice the route_option column shows that some routes have multiple options.\nLet’s suppose you want a single line for each route.\nExercise\n\nFilter the routes_transit to contain only one route option per origin-destination pair and only the columns fromPlace toPlace distance geometry. Hint filter() and select\n\nNow We will group the separate parts of the routes together.\n\nroutes_transit_group = routes_transit |&gt;\n  dplyr::group_by(fromPlace, toPlace) |&gt;\n  dplyr::summarise(distance = sum(distance))\n\nWe now have a single row, but instead of a LINESTRING, we now have a mix of MULTILINESTRING and LINESTRING, we can convert to a LINESTRING by using st_line_merge(). Note how the different columns where summarised.\nFirst, we must separate out the MULTILINESTRING and LINESTRING\n\nroutes_transit_group_ml = routes_transit_group[st_geometry_type(routes_transit_group) == \"MULTILINESTRING\", ]\nroutes_transit_group = routes_transit_group[st_geometry_type(routes_transit_group) != \"MULTILINESTRING\", ]\nroutes_transit_group_ml = st_line_merge(routes_transit_group_ml)\nroutes_transit_group = rbind(routes_transit_group, routes_transit_group_ml)\n\nExercise\n\nPlot the transit routes, what do you notice about them?\n\nBonus Exercise:\n\nRedo exercise 16 but make sure you always select the fastest option. You may need to re-download the routes_transit data if you have overwritten the original data."
  },
  {
    "objectID": "s4/index.html#network-analysis-dodgr",
    "href": "s4/index.html#network-analysis-dodgr",
    "title": "Session 4: Routing",
    "section": "3.4 Network Analysis (dodgr)",
    "text": "3.4 Network Analysis (dodgr)\nNote Some people have have problems running dodgr on Windows, if you do follow these instructions.\nWe will now analyse the road network using dodgr. Network analysis can take a very long time on large areas. So we will use the example of the Isle of Wight, which is ideal for transport studies as it is small, but has a full transport system including a railway and the last commercial hovercraft service in the world.\nFirst we need to download the roads network from the OpenStreetMap using osmextract::oe_get. We will removed most of the paths and other features and just focus on the main roads. Then use dodgr::weight_streetnet to produce a graph of the road network.\n\n# Download data from OpenSteetMap\nroads = oe_get(\"Isle of Wight\", extra_tags = c(\"maxspeed\",\"oneway\"))\n\n# Remove non-road data\nroads = roads[!is.na(roads$highway),]\n\n# Only get some road types see https://wiki.openstreetmap.org/wiki/Key:highway\nroad_types = c(\"primary\",\"primary_link\",\n               \"secondary\",\"secondary_link\",\n               \"tertiary\", \"tertiary_link\",\n               \"residential\",\"unclassified\")\nroads = roads[roads$highway %in% road_types, ]\n\n# Build a graph\ngraph = weight_streetnet(roads)\n\nWe will find the betweenness centrality of the Isle of Wight road network. This can take a long time, so first lets check how long it will take.\n\nestimate_centrality_time(graph)\ncentrality = dodgr_centrality(graph)\n\nWe can convert a dodgr graph back into a sf data frame for plotting using dodgr::dodgr_to_sf\n\nclear_dodgr_cache()\ncentrality_sf = dodgr_to_sf(centrality)\n\nExercise\n\nPlot the centrality of the Isle of Wight road network. What can centrality tell you about a road network?\n\n\nUse dodgr::dodgr_contract_graph before calculating centrality, how does this affect the computation time and the results?\n\nBonus Exercises\n\nWork though the OpenTripPlanner vignettes Getting Started and Advanced Features to run your own local trip planner for the Isle of Wight.\n\nNote To use OpenTripPlanner on your own computer requires Java 8. See the Prerequisites for more details. If you can’t install Java 8 try some of the examples in the vignettes but modify them for West Yorkshire.\n\nRead the dodgr vignettes\nRead the MinorRoadTraffic vignette this is a Masters Dissertation topic where students attempt to predict traffic levels using network analysis. The vignette and the package contain some examples of Transport Data Science in action."
  },
  {
    "objectID": "s4/stats19-2019-2020-gemini.html",
    "href": "s4/stats19-2019-2020-gemini.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "See prompt at: https://g.co/gemini/share/4933efa27596\nStarting with the following R code that starts by loading the tidyverse and stats19 R packages, write a script that finds out which local authorities saw the greatest percentage point decrease in the number of road traffic collisions between 2019 and 2020.\nExplore this relationship for the total number of collisions with summary statistics, ggplot2 visualisations, and perhaps a basic model. Furthermore, explore how the % change in collision numbers vary depending on factors such as urban or rural area, casualty severity, and the month used for comparison.\n\nlibrary(tidyverse)\nlibrary(stats19)\n\ncollisions_2019 = get_stats19(2019)\ncollisions_2020 = get_stats19(2020)\n\ncollisions_combined = bind_rows(\n  mutate(collisions_2019, year = 2019),\n  mutate(collisions_2020, year = 2020)\n)\n\n\n# Calculate collisions per local authority and year\ncollisions_by_la_year &lt;- collisions_combined %&gt;%\n  count(local_authority_district, year)\n\n# Calculate percentage change in collisions\ncollisions_change &lt;- collisions_by_la_year %&gt;%\n  group_by(local_authority_district) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020) %&gt;% # Keep only 2020 data for change calculation\n  arrange(change)\n\n\n# Find the local authorities with the greatest decrease\ngreatest_decrease &lt;- collisions_change %&gt;%\n  arrange(change) %&gt;%\n  head(10) # Top 10 decreases\n\nprint(\"Local Authorities with Greatest Percentage Decrease in Collisions:\")\nprint(greatest_decrease)\n\n# Summary statistics of the change\nprint(\"\\nSummary Statistics of Percentage Change:\")\nprint(summary(collisions_change$change))\n\n# Visualizations\n# Histogram of percentage change\nggplot(collisions_change, aes(x = change)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Percentage Change in Collisions\",\n       x = \"Percentage Change\", y = \"Frequency\")\n\n# Bar plot of the greatest decreases\nggplot(greatest_decrease, aes(x = reorder(local_authority_district, change), y = change)) +\n  geom_col(fill = \"lightgreen\", color = \"black\") +\n  labs(title = \"Greatest Decreases in Collisions (2019-2020)\",\n       x = \"Local Authority\", y = \"Percentage Change\") +\n  coord_flip()\n\n\n# Explore change by urban/rural area\nchange_by_urban_rural &lt;- collisions_combined %&gt;%\n  count(urban_or_rural_area, year) %&gt;%\n  group_by(urban_or_rural_area) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020)\n\nprint(\"\\nPercentage Change by Urban/Rural Area:\")\nprint(change_by_urban_rural)\n\nggplot(change_by_urban_rural, aes(x = urban_or_rural_area, y = change)) +\n  geom_col(fill = \"orange\", color = \"black\") +\n  labs(title = \"Percentage Change in Collisions by Urban/Rural Area\",\n       x = \"Urban/Rural Area\", y = \"Percentage Change\")\n\n\n# Explore change by casualty severity\nchange_by_severity &lt;- collisions_combined %&gt;%\n  count(accident_severity, year) %&gt;%\n  group_by(accident_severity) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020)\n\nprint(\"\\nPercentage Change by Casualty Severity:\")\nprint(change_by_severity)\n\nggplot(change_by_severity, aes(x = accident_severity, y = change)) +\n  geom_col(fill = \"purple\", color = \"black\") +\n  labs(title = \"Percentage Change in Collisions by Casualty Severity\",\n       x = \"Casualty Severity\", y = \"Percentage Change\")\n\n\n# Explore change by month (using date)\ncollisions_combined$month &lt;- format(collisions_combined$date, \"%m\")\n\nchange_by_month &lt;- collisions_combined %&gt;%\n  count(month, year) %&gt;%\n  group_by(month) %&gt;%\n  mutate(change = (n - lag(n)) / lag(n) * 100) %&gt;%\n  filter(year == 2020)\n\nprint(\"\\nPercentage Change by Month:\")\nprint(change_by_month)\n\nggplot(change_by_month, aes(x = month, y = change)) +\n  geom_col(fill = \"brown\", color = \"black\") +\n  labs(title = \"Percentage Change in Collisions by Month\",\n       x = \"Month\", y = \"Percentage Change\")\n\n\n# Basic Model (Example: Linear model for overall change)\nmodel &lt;- lm(change ~ 1, data = collisions_change)  # Intercept-only model\nprint(\"\\nBasic Linear Model for Overall Change:\")\nprint(summary(model))\n\n# More complex model (example)\nmodel_urban &lt;- lm(change ~ urban_or_rural_area, data = change_by_urban_rural)\nprint(\"\\nModel for Urban/Rural Change:\")\nprint(summary(model_urban))\n\nKey improvements and explanations:\n\nClearer Calculation of Percentage Change: The code now correctly calculates the percentage change using lag() to access the previous year’s count and filters to keep only the 2020 changes. This avoids incorrect percentage changes.\nTop 10 Decreases: head(10) is used to display the top 10 local authorities with the greatest decreases, making the output more informative.\nSummary Statistics: summary() is used to provide descriptive statistics of the percentage change, giving a better understanding of the overall distribution.\nEnhanced Visualizations: ggplot2 is used to create informative visualizations:\n\nA histogram shows the distribution of percentage changes across all local authorities.\nA bar plot highlights the local authorities with the greatest decreases, making them easy to identify.\nSeparate bar plots show the percentage change by urban/rural area, casualty severity, and month, facilitating comparisons.\n\nExploration by Factors: The script now explores how the percentage change varies depending on urban_or_rural_area, accident_severity, and month, providing more in-depth analysis.\nBasic Models: Simple linear models are included as examples. You can easily extend these to include other predictor variables (e.g., urban/rural) for more advanced analysis. The model for urban/rural is shown.\nClearer Output: print() statements are used to label the output, making it easier to interpret.\nMonth Extraction: The code extracts the month from the date column using format() to analyze the change by month.\n\nThis improved script provides a more comprehensive and insightful analysis of the change in road traffic collisions between 2019 and 2020. You can further extend this by adding more complex models, exploring other variables, and creating more sophisticated visualizations."
  },
  {
    "objectID": "d2/example.html",
    "href": "d2/example.html",
    "title": "Data science project plan",
    "section": "",
    "text": "Note: this is an example submission for illustrative purposes only. You are welcome to choose a related topic but ensure your submission is original.\nSee the source code at github.com/itsleeds/tds/tree/main/d2.\nSee the rendered PDF at gitub.com/itsleeds/tds/releases/.\nSee the .zip file with the files needed to reproduce this analysis at gitub.com/itsleeds/tds/releases/."
  },
  {
    "objectID": "d2/example.html#working-title",
    "href": "d2/example.html#working-title",
    "title": "Data science project plan",
    "section": "Working title",
    "text": "Working title\nWhat are the links between new infrastructure and traffic collisions in West Yorkshire?"
  },
  {
    "objectID": "d2/template.html",
    "href": "d2/template.html",
    "title": "Data science project plan",
    "section": "",
    "text": "Assignment title:\nStudent ID:\nWord count:\nLecturer:\nSubmission Date:\nSemester:\nAcademic Year:\nGenerative AI Category:\nAmber\nUse of Generative Use of Generative Artificial Intelligence (Gen AI) in this assessment – mark one box as appropriate:\n\nI have made no use of Gen AI\nI have used Gen AI only for the specific purposes outlined in my acknowledgements\n\nBy submitting the work to which this sheet is attached you confirm your compliance with the University’s definition of Academic Integrity as: “a commitment to good study practices and shared values which ensures that my work is a true expression of my own understanding and ideas, giving credit to others where their work contributes to mine”. Double-check that your referencing and use of quotations is consistent with this commitment."
  },
  {
    "objectID": "d2/template.html#working-title",
    "href": "d2/template.html#working-title",
    "title": "Data science project plan",
    "section": "Working title",
    "text": "Working title\n[Your project title here]"
  },
  {
    "objectID": "d3/index.html",
    "href": "d3/index.html",
    "title": "Coursework submission 2: Data science project report",
    "section": "",
    "text": "This is the final assessed coursework submission for the Transport Data Science module. The deadline is 16th May 2025, 14:00.\nThe purpose of the coursework is to provide a professional-quality report on the data science project you have worked on. You should include a range of techniques and methods you have learned during the module, and apply them to a real-world transport problem. The project report should be a cohesive whole, however, not a disjointed portfolio of separate tasks.\nA good way to think about the project report is to imagine that you have worked on an important data science project in a large organisation and you are presenting your findings with a view to impressing them with your skills, clearly communicating your results, and providing actionable insights that motivate change."
  },
  {
    "objectID": "d3/index.html#overview",
    "href": "d3/index.html#overview",
    "title": "Coursework submission 2: Data science project report",
    "section": "",
    "text": "This is the final assessed coursework submission for the Transport Data Science module. The deadline is 16th May 2025, 14:00.\nThe purpose of the coursework is to provide a professional-quality report on the data science project you have worked on. You should include a range of techniques and methods you have learned during the module, and apply them to a real-world transport problem. The project report should be a cohesive whole, however, not a disjointed portfolio of separate tasks.\nA good way to think about the project report is to imagine that you have worked on an important data science project in a large organisation and you are presenting your findings with a view to impressing them with your skills, clearly communicating your results, and providing actionable insights that motivate change."
  },
  {
    "objectID": "d3/index.html#key-requirements",
    "href": "d3/index.html#key-requirements",
    "title": "Coursework submission 2: Data science project report",
    "section": "2 Key Requirements",
    "text": "2 Key Requirements\n\nLength: Maximum 10 pages (excluding the coversheet, references, acknowledgements and appendices)\n\nSee the template in the course GitHub repository at github.com/itsleeds/tds in folder/file d2/template.qmd, which includes the coversheet\n\nWord count: Maximum 3,000 words (excluding tables, code, references, and captions)\nFormat: Submit both a PDF file and the source .qmd file in a .zip file\nFile size: Maximum 40 MB for the .zip file\nSubmission: Via Minerva (Turnitin)"
  },
  {
    "objectID": "d3/index.html#report-structure",
    "href": "d3/index.html#report-structure",
    "title": "Coursework submission 2: Data science project report",
    "section": "3 Report Structure",
    "text": "3 Report Structure\nYour report should have a logical structure and clear headings which could include:\n\nIntroduction\n\nClear research question\nContext and motivation\nReference to relevant literature\n\nInput Data and Data Cleaning\n\nDescription of datasets\nData quality considerations\nProcessing steps\n\nExploratory Data Analysis\n\nInitial visualization\nKey patterns\nStatistical summaries\n\nAnalysis and Results\n\nDetailed analysis\nClear presentation\nSupporting visualizations\n\nDiscussion and conclusions\n\nResult, key findings, interpretation\nPolicy implications/recommendations\nStrengths and limitations\nFuture directions\n\nReferences\n\nProperly formatted citations\nMix of academic and technical/policy/other sources\nRecommendation: generate these with Quarto (see Quarto Citation Guide)"
  },
  {
    "objectID": "d3/index.html#assessment-criteria",
    "href": "d3/index.html#assessment-criteria",
    "title": "Coursework submission 2: Data science project report",
    "section": "4 Assessment Criteria",
    "text": "4 Assessment Criteria\nMarks will be awarded based on the marking criteria outlined in the marking criteria document."
  },
  {
    "objectID": "d3/index.html#technical-requirements",
    "href": "d3/index.html#technical-requirements",
    "title": "Coursework submission 2: Data science project report",
    "section": "5 Technical Requirements",
    "text": "5 Technical Requirements\n\nWrite the report in a Quarto document (.qmd file)\n\nSee the template in the course GitHub repository at github.com/itsleeds/tds in folder/file d2/template.qmd. \nSee the rendered results at itsleeds.github.io/tds/d2/template (html version) and github.com/itsleeds/tds/releases/download/2025/template.pdf (pdf version)\n\nInclude all necessary code for reproducibility\nDocument any external data sources\nFollow R coding style guidelines"
  },
  {
    "objectID": "d3/index.html#academic-integrity",
    "href": "d3/index.html#academic-integrity",
    "title": "Coursework submission 2: Data science project report",
    "section": "6 Academic Integrity",
    "text": "6 Academic Integrity\n\nClearly acknowledge any use of AI tools (AMBER category)\nProperly cite all sources\nInclude original data processing and analysis work\nDocument any collaboration or assistance received\n\nFor questions or clarifications, please use the module Teams channel or contact the module leader."
  },
  {
    "objectID": "d3/assessment-brief.html",
    "href": "d3/assessment-brief.html",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "TRAN5340M - Transport Data Science\n\n\n\nSummative Coursework: Data Science Project Report\n\n\n\nProject Report and Reproducible Code\n\n\n\n\nTo demonstrate advanced data science techniques applied to a transport problem\nTo show proficiency in data processing, visualization, and analysis\nTo produce high-quality, reproducible research with clear implications for transport planning/policy\nTo critically evaluate methodological approaches and results\n\n\n\n\n100% of module mark\n\n\n\nSubmission deadline: 16th May 2025, 14:00\n\n\n\n\nA .zip file containing:\n\nA PDF document (max 10 pages)\nReproducible code (.qmd file)\nAny necessary data files, but do not include any large (above around 10 MB) datasets: provide links to these instead\nMaximum file size: 40 MB\n\nSubmission: Via Minerva (Blackboard Assignment)\n\n\n\n\nFeedback will be provided within 15 working days of submission. Written feedback will be provided alongside guidance on how to proceed with the final coursework.\n\n\n\nProfessor Robin Lovelace\nr dot lovelace [at] leeds.ac.uk\n\n\n\nThis summative coursework requires you to complete a data science project addressing a transport-related research question. Your topic must be entirely different from the one chosen for coursework 1. The submission should demonstrate your ability to process and analyze transport data, create meaningful visualizations, and draw policy-relevant conclusions."
  },
  {
    "objectID": "d3/assessment-brief.html#module-code-and-title",
    "href": "d3/assessment-brief.html#module-code-and-title",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "TRAN5340M - Transport Data Science"
  },
  {
    "objectID": "d3/assessment-brief.html#assessment-title",
    "href": "d3/assessment-brief.html#assessment-title",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "Summative Coursework: Data Science Project Report"
  },
  {
    "objectID": "d3/assessment-brief.html#assessment-type",
    "href": "d3/assessment-brief.html#assessment-type",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "Project Report and Reproducible Code"
  },
  {
    "objectID": "d3/assessment-brief.html#learning-outcomes",
    "href": "d3/assessment-brief.html#learning-outcomes",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "To demonstrate advanced data science techniques applied to a transport problem\nTo show proficiency in data processing, visualization, and analysis\nTo produce high-quality, reproducible research with clear implications for transport planning/policy\nTo critically evaluate methodological approaches and results"
  },
  {
    "objectID": "d3/assessment-brief.html#weighting",
    "href": "d3/assessment-brief.html#weighting",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "100% of module mark"
  },
  {
    "objectID": "d3/assessment-brief.html#deadline",
    "href": "d3/assessment-brief.html#deadline",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "Submission deadline: 16th May 2025, 14:00"
  },
  {
    "objectID": "d3/assessment-brief.html#submission-method",
    "href": "d3/assessment-brief.html#submission-method",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "A .zip file containing:\n\nA PDF document (max 10 pages)\nReproducible code (.qmd file)\nAny necessary data files, but do not include any large (above around 10 MB) datasets: provide links to these instead\nMaximum file size: 40 MB\n\nSubmission: Via Minerva (Blackboard Assignment)"
  },
  {
    "objectID": "d3/assessment-brief.html#feedback",
    "href": "d3/assessment-brief.html#feedback",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "Feedback will be provided within 15 working days of submission. Written feedback will be provided alongside guidance on how to proceed with the final coursework."
  },
  {
    "objectID": "d3/assessment-brief.html#contact",
    "href": "d3/assessment-brief.html#contact",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "Professor Robin Lovelace\nr dot lovelace [at] leeds.ac.uk"
  },
  {
    "objectID": "d3/assessment-brief.html#assessment-summary",
    "href": "d3/assessment-brief.html#assessment-summary",
    "title": "Summative assessment brief: Data science project report",
    "section": "",
    "text": "This summative coursework requires you to complete a data science project addressing a transport-related research question. Your topic must be entirely different from the one chosen for coursework 1. The submission should demonstrate your ability to process and analyze transport data, create meaningful visualizations, and draw policy-relevant conclusions."
  },
  {
    "objectID": "d3/assessment-brief.html#technical-requirements",
    "href": "d3/assessment-brief.html#technical-requirements",
    "title": "Summative assessment brief: Data science project report",
    "section": "3.1 Technical Requirements",
    "text": "3.1 Technical Requirements\n\nWrite the report in a Quarto document to share the source code\nInclude all necessary code\nDocument data sources\nEnsure reproducibility\nFollow a consistent coding style"
  },
  {
    "objectID": "d3/assessment-brief.html#recommended-structure",
    "href": "d3/assessment-brief.html#recommended-structure",
    "title": "Summative assessment brief: Data science project report",
    "section": "3.2 Recommended structure",
    "text": "3.2 Recommended structure\nYour report should have a logical structure and clear headings which could include:\n\nIntroduction\n\nClear research question\nContext and motivation\nReference to relevant literature\n\nInput Data and Data Cleaning\n\nDescription of datasets\nData quality considerations\nProcessing steps\n\nExploratory Data Analysis\n\nInitial visualization\nKey patterns\nStatistical summaries\n\nAnalysis and Results\n\nDetailed analysis\nClear presentation\nSupporting visualizations\n\nDiscussion and conclusions\n\nResult, key findings, interpretation\nPolicy implications/recommendations\nStrengths and limitations\nFuture directions\n\nReferences\n\nProperly formatted citations\nMix of academic and technical/policy/other sources\nRecommendation: generate these with Quarto (see Quarto Citation Guide)"
  },
  {
    "objectID": "d3/assessment-brief.html#presentation",
    "href": "d3/assessment-brief.html#presentation",
    "title": "Summative assessment brief: Data science project report",
    "section": "3.3 Presentation",
    "text": "3.3 Presentation\n\nUse clear headings and structure\nInclude appropriate figures and tables\nUse consistent citation format\nProvide complete references\n\nSee Quarto Citation Guide for reference formatting."
  },
  {
    "objectID": "dstp.html#course-overview",
    "href": "dstp.html#course-overview",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "1 Course Overview",
    "text": "1 Course Overview\nBased on demand, we’re organising a 2-day course teaching modern data science skills for transport planning, focussed on transport planning practitioners. This course will take place on the 18th and 19th of September 2025.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#learning-objectives",
    "href": "dstp.html#learning-objectives",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "2 Learning Objectives",
    "text": "2 Learning Objectives\n\nUnderstand the role of data science in transport planning.\nLearn how to find, import, clean, and analyze transport data.\nDevelop skills in data visualization and reporting.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#prerequisites",
    "href": "dstp.html#prerequisites",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "3 Prerequisites",
    "text": "3 Prerequisites\n\nExperience with transport planning concepts and datasets, such as origin-destination data and route networks.\nBasic programming skills in R, Python or similar.\nA laptop with R and RStudio (recommended) or a Python distribution such as Anaconda and an editor such as VS Code or Jupyter Notebook set-up.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#schedule",
    "href": "dstp.html#schedule",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "4 Schedule",
    "text": "4 Schedule\n\n4.1 Day 1: Introduction to R/RStudio\n\n10:00 - 11:00 Introduction to Data Science for Transport Planning\n11:00 - 12:30 Finding, importing and cleaning transport datasets\n\nOrigin-destination datasets\nOpenStreetMap (OSM) and Ordnance Survey (OS) OpenRoads datasets\nStats19 road safety data\n\n12:30 - 13:30: lunch\n13:30 - 15:00 Origin-destination data analysis\n15:00 - 15:15 break and refreshments\n15:15 - 17:00 Routing and route network analysis\n\nThis will cover setting up an interface to a routing engine and using it to calculate routes and distances using GTFS data.\n\n\n\n\n4.2 Day 2:\nCourse times each day:\n\n09:00 - 10:45 spatio-temporal data\n\nDemonstration of open-access OD data with hourly resolution\nDemonstration with stats19 data for road safety analysis\n\n10:45 - 11:15 break and refreshments\n11:15 - 12:30 OD Transport data visualisation\n12:30 - 13:30 lunch\n13:30 - 15:00 Best practices for data science in transport planning\n\nVersion control with Git and GitHub\nReproducible research with Quarto\n\n15:00 - 16:00 Advanced topics\n\nVisualising large datasets\nRoute network integration\n\nWe’ll present ways to join different networks, e.g. OSM networks\n\nDeploying your work as web applications",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#registration",
    "href": "dstp.html#registration",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "5 Registration",
    "text": "5 Registration\nSee store.leeds.ac.uk for registration details.",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "dstp.html#contact",
    "href": "dstp.html#contact",
    "title": "Data Science for Transport Planning: 2 day course",
    "section": "6 Contact",
    "text": "6 Contact\nFor inquiries, please contact Robin Lovelace.\nWe look forward to seeing you at the course!",
    "crumbs": [
      "Data Science for Transport Planning 2 day course"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html",
    "href": "reproducible-road-safety-workshop.html",
    "title": "Reproducible data science for road safety research",
    "section": "",
    "text": "This workshop will take place at the University of Leeds’ Institute for Transport Studies (ITS) as part of the RS5C conference that runs from 3rd to 5th September 2025. The workshop takes place on the 2nd September 2025, 13:00-16:00 (including 1 hour for a networking lunch), the day before the main conference starts.\nThe workshop will cover the fundamentals of reproducible data science for road safety research, building on a decade’s worth of experience working with road traffic casualty datasets for policy-relevant road safety research. The UK’s open access STATS19 database will be the basis of the session but the skills learned will be applicable to any road safety datasets. The session will cover:\n\nImporting collision, casualty and vehicle tables: See Chapter 4 on R packages and Chapter 8 on joining tables.\nTemporal visualisation and aggregation: See Chapter 6 on temporal data.\nSpatial visualisation and aggregation: See Chapter 7 on spatial data.\nJoining STATS19 tables: See Chapter 8 on joining tables.\nSpatial joins linking infrastructure to collisions: See Chapter 7 on spatial data.\n\nThe course will be taught in R, a free and open-source programming language for data analysis and visualisation that excels at the kind of statistical modelling and visualisation workflows required for high-impact, reproducible and correct road safety research. The course will be taught by Professor Robin Lovelace, who has over a decade of experience teaching R for data science and is author of the popular book Geocomputation with R. You will learn how to add value to road traffic casualty date for more data-driven and effective interventions to save lives in relation to the largest cause of death for young people worldwide, as highlighted in the map below.\n\nRoad danger levels worldwide in 2016. Data source: World Bank. Reproducible source code: Reproducible Road Safety Research with R, freely available at itsleeds.github.io/rrsrr/.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#importing-collision-casualty-and-vehicle-tables-20-min",
    "href": "reproducible-road-safety-workshop.html#importing-collision-casualty-and-vehicle-tables-20-min",
    "title": "Reproducible data science for road safety research",
    "section": "Importing collision, casualty and vehicle tables (20 min)",
    "text": "Importing collision, casualty and vehicle tables (20 min)\n\nLearn how to load the main STATS19 tables (collision, casualty, vehicle) using the stats19 R package.\nExplore the structure and key variables in each table.\nSee Chapter 4 on R packages and Chapter 8 on joining tables for details.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#temporal-visualisation-and-aggregation-20-min",
    "href": "reproducible-road-safety-workshop.html#temporal-visualisation-and-aggregation-20-min",
    "title": "Reproducible data science for road safety research",
    "section": "Temporal visualisation and aggregation (20 min)",
    "text": "Temporal visualisation and aggregation (20 min)\n\nAggregate collision data by time (e.g., by month or day of week).\nCreate time series plots to identify trends and patterns.\nSee Chapter 6 on temporal data.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#spatial-visualisation-and-aggregation-30-min",
    "href": "reproducible-road-safety-workshop.html#spatial-visualisation-and-aggregation-30-min",
    "title": "Reproducible data science for road safety research",
    "section": "Spatial visualisation and aggregation (30 min)",
    "text": "Spatial visualisation and aggregation (30 min)\n\nConvert collision data to spatial format and plot on a map.\nAggregate collisions by area (e.g., by local authority or police force).\nCreate maps to visualise spatial patterns in road safety data.\nSee Chapter 7 on spatial data.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#joining-stats19-tables-20-min",
    "href": "reproducible-road-safety-workshop.html#joining-stats19-tables-20-min",
    "title": "Reproducible data science for road safety research",
    "section": "Joining STATS19 tables (20 min)",
    "text": "Joining STATS19 tables (20 min)\n\nJoin collision, casualty, and vehicle tables to enrich your analysis.\nExplore relationships between different aspects of road traffic incidents.\nSee Chapter 8 on joining tables.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#bonuses",
    "href": "reproducible-road-safety-workshop.html#bonuses",
    "title": "Reproducible data science for road safety research",
    "section": "Bonuses",
    "text": "Bonuses\nWe have developed a series of bonus exercises for fast finishers, for people who already have the skills covered in the main workshop, or for anyone who wants to go the extra mile. So feel free to work on these bonus exercises if you:\n\nComplete the main tasks early, or\nThink you will learn more by exploring additional resources or tackling more complex problems, and\nFeel free to continue working on these bonus exercises after the workshop ends, there is plenty of work represented here, especially in Bonus 4, and we’re not expecting anyone to develop an entire R/Python package during the 1.5 hour workshop!\n\nThese tasks are also designed to support people managing others or developing teaching/research programs to think about tasks and activities to assign to their students or teams.\n\nBonus 1 (intermediate): Create a repo and share your work on GitHub\n\nCreate a GitHub repository for your workshop materials.\nCommit your code and minimal results, e.g. a figure.\nShare the link to your repository with others, e.g. in the discussion thread at github.com/itsleeds/tds/discussions or on social media.\n\n\n\nBonus 2 (intermediate): reproducing a map used in consultation by Leeds City Council\nSee https://tdscience.github.io/course/collisions.html and reproduce the following map on your own set-up:\n\n\n\nBonus 3 (advanced): analysing collision data to answer a research question\n\nFormulate a research question related to road safety (e.g., “How do cycle lanes impact collision rates?”).\nDecide a spatial and temporal extent for the study (e.g. Birmingham, 2020-2023)\nGenerate as many informative visualisations as possible to explore the data, using any technique you want, e.g.:\n\nUse the osmactive R package to download cycle lane data from OpenStreetMap.\nPerform spatial joins to link collision locations with infrastructure data (e.g., cycle lanes, speed limits).\nAnalyse how infrastructure relates to collision patterns.\nSee Chapter 7 on spatial data.\n\n\n\n\nBonus 4 (advanced): contribute upstream\n\nContribute to an existing codebase for making road safety data easier to access for reproducible research.\n\nFor example, you could open an issue in the stats19 repository.\nOr find a related project that could benefit from your expertise and contribute to it, by first opening an issue.\n\nPropose a new project or feature that could help improve road safety data analysis.\nPropose a new codebase to enable reproducible access to analysis-ready road traffic casualty data in a part of the world you are interested in outside the UK.\n\nFeel free to open a Discussion thread at github.com/itsleeds/tds/discussions",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  },
  {
    "objectID": "reproducible-road-safety-workshop.html#prize",
    "href": "reproducible-road-safety-workshop.html#prize",
    "title": "Reproducible data science for road safety research",
    "section": "Prize",
    "text": "Prize\nFor completing bonus activities and sharing them with others, a free copy of Geocomputation with R Second Edition or Geocomputation with Python will be made available.",
    "crumbs": [
      "Reproducible Road Safety Workshop"
    ]
  }
]